[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Python for Clinical Study Reports and Submission",
    "section": "",
    "text": "Welcome\nWelcome to Python for Clinical Study Reports and Submission. Clinical study reports (CSR) are crucial components in clinical trial development. A CSR is an “integrated” full scientific report of an individual clinical trial.\nThe ICH E3: Structure and Content of Clinical Study Reports offers comprehensive instructions to sponsors on the creation of a CSR. This book is a clear and straightforward guide on using Python to streamline the process of preparing CSRs. Additionally, it provides detailed guidance on the submission process to regulatory agencies. Whether you are a beginner or an experienced developer, this book is an indispensable asset in your clinical reporting toolkit.\nThis is a work-in-progress draft.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#events",
    "href": "index.html#events",
    "title": "Python for Clinical Study Reports and Submission",
    "section": "Events",
    "text": "Events\n\n\n\nVenue\nType\nDate\nMaterials\n\n\n\n\nR/Pharma Conference\nWorkshop\n2025-11-07\nSlides",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "In this book\nThis book is designed for people who are interested in using Python for clinical development. Each part of the book makes certain assumptions about the readers’ background:",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#in-this-book",
    "href": "preface.html#in-this-book",
    "title": "Preface",
    "section": "",
    "text": "Part 1, titled “Environment and toolchain” and “Reporting packages”, provides general information on setting up Python development environments for clinical reporting.\nPart 2, titled “Delivering TLFs in CSR”, provides general information and examples on creating tables, listings, and figures using Python. It assumes that readers are individual contributors to a clinical project with prior experience in Python. Familiarity with data manipulation using Polars is expected. Recommended references for this part include Python Polars: The Definitive Guide, theand the rtflite documentation.\nPart 3, titled “Clinical trial project”, provides general information and examples on managing a clinical trial A&R project using Python. It assumes that readers are project leads who have experience in Python package development.\nPart 4, titled “eCTD submission package”, provides general information on preparing submission packages related to the CSR in the electronic Common Technical Document (eCTD) format using Python. It assumes that readers are project leads of clinical projects who possess experience in Python package development and regulatory submission processes.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#philosophy",
    "href": "preface.html#philosophy",
    "title": "Preface",
    "section": "Philosophy",
    "text": "Philosophy\nWe share the same philosophy described in the introduction of the R Packages book (Wickham and Bryan 2023), which we quote below:\n\n“Anything that can be automated, should be automated.”\n“Do as little as possible by hand. Do as much as possible with functions.”",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#authors-and-contributors",
    "href": "preface.html#authors-and-contributors",
    "title": "Preface",
    "section": "Authors and contributors",
    "text": "Authors and contributors\nThis document is a collaborative effort maintained by a community. As you read through it, you also have the opportunity to contribute and enhance its quality. Your input and involvement play a vital role in shaping the excellence of this document.\n\nAuthors: made significant contributions to at least one chapter, constituting the majority of the content.\nYilong Zhang, Nan Xiao,\n\n\n\n\n\nWickham, Hadley, and Jennifer Bryan. 2023. R Packages. O’Reilly Media, Inc.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "env-dev.html",
    "href": "env-dev.html",
    "title": "1  Python developer setup",
    "section": "",
    "text": "1.1 Development environments\nFor this book, you have several options for your development environment. Choose the one that best fits your current setup and constraints.",
    "crumbs": [
      "Environment and toolchain",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python developer setup</span>"
    ]
  },
  {
    "objectID": "env-dev.html#development-environments",
    "href": "env-dev.html#development-environments",
    "title": "1  Python developer setup",
    "section": "",
    "text": "1.1.1 GitHub Codespaces\nGitHub Codespaces provides a cloud-based development environment with everything pre-configured. This is the easiest option if you don’t have a local Python setup.\nWe will provide a dev container configuration that includes:\n\nPython with uv pre-installed.\nQuarto pre-installed for document rendering.\nAll necessary VS Code extensions.\nConsistent environment across all readers, useable in the web browser.\n\nTo use a pre-configured Codespaces for this book, use the “Open in GitHub Codespaces” button below.\n\nClick “Create new Codespace”, you will see a notification “Setting up remote connection: Building codespace…” After two or three minutes, your Codespace will be ready with everything set up.\n\n\n\n\n\n\nNote\n\n\n\nCodespaces currently offers 120 hours of free compute time per month for personal accounts. This is more than sufficient for this book.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemember to stop your Codespace after this workshop to avoid unnecessary usage. We will remind you at the end of this book.\n\n\n\n\n1.1.2 Positron\nPositron is Posit’s next-generation data science IDE, built on Code OSS (the open source core of VS Code), with specific improvements for R and Python development.\nKey features for Python work:\n\nNative notebook support.\nInteractive variable explorer.\nIntegrated plot viewer.\nBuilt-in data viewer for DataFrames.\n\n\n\nPositron uses Open VSX instead of the Microsoft VS Code marketplace. Most essential Python extensions are available, but the selection is more limited.\nDownload Positron from https://positron.posit.co/.\n\n\n1.1.3 VS Code\nVisual Studio Code remains the most popular choice for Python development. It offers a rich ecosystem of extensions and tools.\nEssential extensions for this book:\n\nPython: Core Python language support.\nPylance: Fast, feature-rich Python language server.\nRuff: Lightning-fast linting and formatting.\nEven Better TOML: Syntax highlighting for TOML files (pyproject.toml).\nQuarto: Authoring support for Quarto documents.",
    "crumbs": [
      "Environment and toolchain",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python developer setup</span>"
    ]
  },
  {
    "objectID": "env-dev.html#vs-code-settings",
    "href": "env-dev.html#vs-code-settings",
    "title": "1  Python developer setup",
    "section": "1.2 VS Code settings",
    "text": "1.2 VS Code settings\n\n1.2.1 Unicode highlighting\nPython allows Unicode characters in strings and identifiers. AI coding tools might also generate code with non-ASCII characters. For regulatory work, you should highlight non-ASCII characters to find these hidden issues early and avoid problems in submissions.\nVia Settings UI:\n\nOpen Command Palette (Cmd/Ctrl + Shift + P)\nSearch for “Preferences: Open Settings (UI)”\nSearch for “Unicode Highlight”\nEnable “Non Basic ASCII” for both trusted and untrusted workspaces\n\nVia Settings JSON:\nOpen Command Palette with Cmd/Ctrl + Shift + P, select “Preferences: Open User Settings (JSON)”, then add:\n\"editor.unicodeHighlight.nonBasicASCII\": true\nThis highlights characters like curly quotes, em dashes, and other non-ASCII characters that could cause issues in eCTD submission packages.",
    "crumbs": [
      "Environment and toolchain",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python developer setup</span>"
    ]
  },
  {
    "objectID": "env-dev.html#terminal-setup",
    "href": "env-dev.html#terminal-setup",
    "title": "1  Python developer setup",
    "section": "1.3 Terminal setup",
    "text": "1.3 Terminal setup\nFor local development, you will interact with uv and Quarto through the terminal.\n\n1.3.1 Shell\nAny modern shell works well:\n\nmacOS/Linux: zsh (default on macOS), bash\nWindows: PowerShell, Windows Terminal\n\n\n\n1.3.2 Terminal emulator\nIf you are on macOS and want a faster terminal experience, consider Ghostty. It is written in Zig for exceptional performance.",
    "crumbs": [
      "Environment and toolchain",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python developer setup</span>"
    ]
  },
  {
    "objectID": "env-dev.html#ai-coding-assistants",
    "href": "env-dev.html#ai-coding-assistants",
    "title": "1  Python developer setup",
    "section": "1.4 AI coding assistants",
    "text": "1.4 AI coding assistants\nModern agentic AI coding tools can accelerate statistical and clinical coding tasks, especially for popular programming languages like Python. We encourage you to use them, for example:\n\nCodex (command-line interface, VS Code extension)\nClaude Code (command-line interface)\nCursor (AI-first editor)\nGitHub Copilot (VS Code extension)\n\n\n1.4.1 Effective use of AI tools\nTo use AI assistants effectively for programming, you need:\nProduct manager mindset: Know exactly what you want to build. In clinical reporting, this means understanding the table shell, statistical method, and regulatory requirements.\nSoftware architect mindset: Evaluate model outputs critically. Can you spot issues with data transformations? Do the statistical computations match the statistical analysis plan? Is the output format submission-ready?\n\n\n\n\n\n\nWarning\n\n\n\nAI tools are assistants, not replacements for domain expertise. Always verify outputs against statistical analysis plans and regulatory guidance.",
    "crumbs": [
      "Environment and toolchain",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python developer setup</span>"
    ]
  },
  {
    "objectID": "env-dev.html#whats-next",
    "href": "env-dev.html#whats-next",
    "title": "1  Python developer setup",
    "section": "1.5 What’s next",
    "text": "1.5 What’s next\nWith your development environment configured, you are ready to learn about uv, the modern project management tool for Python.\nIn the next chapter, we will cover:\n\nCreating and managing Python projects.\nPinning Python versions.\nInstalling dependencies.\nUnderstanding the modern Python packaging ecosystem.",
    "crumbs": [
      "Environment and toolchain",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python developer setup</span>"
    ]
  },
  {
    "objectID": "env-uv.html",
    "href": "env-uv.html",
    "title": "2  Python projects with uv",
    "section": "",
    "text": "2.1 Why virtual environments\nIn Python, virtual environments are not optional. They are essential for any serious project work.\nUnlike R’s renv (which primarily helps with reproducibility), Python virtual environments serve a fundamental purpose: isolating project dependencies from the system Python.\nHere is why this matters:",
    "crumbs": [
      "Environment and toolchain",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python projects with uv</span>"
    ]
  },
  {
    "objectID": "env-uv.html#why-virtual-environments",
    "href": "env-uv.html#why-virtual-environments",
    "title": "2  Python projects with uv",
    "section": "",
    "text": "Different projects need different package versions.\nSystem Python library should never be modified directly.\nDependency conflicts are common and destructive.\nReproducibility requires exact version control.\n\n\n\n\n\n\n\nWarning\n\n\n\nInstalling packages globally with pip install without a virtual environment will cause conflicts and break system tools. Always use virtual environments. To install Python packages as global command-line tools, use pipx.",
    "crumbs": [
      "Environment and toolchain",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python projects with uv</span>"
    ]
  },
  {
    "objectID": "env-uv.html#what-is-uv",
    "href": "env-uv.html#what-is-uv",
    "title": "2  Python projects with uv",
    "section": "2.2 What is uv",
    "text": "2.2 What is uv\nuv is a modern Python package and project manager written in Rust. It replaces and improves upon a scattered toolchain:\n\npip (package installation)\nvenv (virtual environment creation)\npyenv (Python version management)\npip-tools (dependency locking)\nsetuptools (package building)\n\nBenefits of uv:\n\nFast: 10-100x faster than pip due to Rust implementation.\nComplete: Manages Python versions, dependencies, and builds.\nModern: Uses pyproject.toml as the single source of truth.\nReliable: Automatic dependency resolution and lock files.\n\n\n\nIn R terms, uv combines functionality from renv, devtools, usethis, and pak into a single, cohesive tool.",
    "crumbs": [
      "Environment and toolchain",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python projects with uv</span>"
    ]
  },
  {
    "objectID": "env-uv.html#python-packaging-standards",
    "href": "env-uv.html#python-packaging-standards",
    "title": "2  Python projects with uv",
    "section": "2.3 Python packaging standards",
    "text": "2.3 Python packaging standards\nPython has standardized on pyproject.toml as the configuration file for all projects. This is similar to R’s DESCRIPTION file but uses TOML format.\nThe Official Python packaging guide is available at https://packaging.python.org/.\nKey concepts:\n\npyproject.toml defines project metadata and dependencies.\nuv.lock records exact versions (like renv.lock).\nBuild backends (like hatchling) create distributable packages.",
    "crumbs": [
      "Environment and toolchain",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python projects with uv</span>"
    ]
  },
  {
    "objectID": "env-uv.html#installing-uv",
    "href": "env-uv.html#installing-uv",
    "title": "2  Python projects with uv",
    "section": "2.4 Installing uv",
    "text": "2.4 Installing uv\nFollow the official installation guide.\nmacOS and Linux:\ncurl -LsSf https://astral.sh/uv/install.sh | sh\nWindows:\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\nVia Homebrew (macOS):\nbrew install uv\nVerify installation:\nuv --version",
    "crumbs": [
      "Environment and toolchain",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python projects with uv</span>"
    ]
  },
  {
    "objectID": "env-uv.html#updating-uv",
    "href": "env-uv.html#updating-uv",
    "title": "2  Python projects with uv",
    "section": "2.5 Updating uv",
    "text": "2.5 Updating uv\nuv can update itself:\nuv self update\nRegular updates are important because uv frequently adds support for new Python versions and features.\n\n\n\n\n\n\nNote\n\n\n\nuv uses Python distributions from the python-build-standalone project. These are optimized, portable Python builds that work consistently across platforms.",
    "crumbs": [
      "Environment and toolchain",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python projects with uv</span>"
    ]
  },
  {
    "objectID": "env-uv.html#initialize-a-project",
    "href": "env-uv.html#initialize-a-project",
    "title": "2  Python projects with uv",
    "section": "2.6 Initialize a project",
    "text": "2.6 Initialize a project\n\n\n\n\n\n\nWarningFor GitHub Codespaces users\n\n\n\nIf you are using GitHub Codespaces, the pycsr project folder is opened by default. Before creating a new practice project, close this folder via File &gt; Close Folder so your shell returns to the home directory. This prevents uv from getting confused about which uv.lock file to write when you “initialize a new project within an existing project”.\n\n\nCreate a new Python project:\nuv init pycsr-example\ncd pycsr-example\nThis creates a basic structure:\npycsr-example/\n├── .python-version    # Pinned Python version\n├── pyproject.toml     # Project metadata and dependencies\n├── README.md          # Project documentation\n└── src/\n    └── pycsr_example/\n        └── __init__.py\n\n\nNotice the directory name uses hyphens (pycsr-example) while the package name uses underscores (pycsr_example). This is Python convention.\n\n2.6.1 Project structure\nThe pyproject.toml file contains project configuration:\n[project]\nname = \"pycsr-example\"\nversion = \"0.1.0\"\ndescription = \"Example clinical study report project\"\ndependencies = []\n\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\nKey sections:\n\n[project]: Package metadata.\n[project.dependencies]: Hard, runtime dependencies.\n[dependency-groups.dev]: Development dependencies.\n[build-system]: How to build the package.",
    "crumbs": [
      "Environment and toolchain",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python projects with uv</span>"
    ]
  },
  {
    "objectID": "env-uv.html#pin-python-version",
    "href": "env-uv.html#pin-python-version",
    "title": "2  Python projects with uv",
    "section": "2.7 Pin Python version",
    "text": "2.7 Pin Python version\nSpecify the exact Python version for your project:\nuv python pin 3.13.9\nThis updates .python-version file so everyone uses the same Python version when they restore the environment.\n\n\n\n\n\n\nImportant\n\n\n\nUse the full MAJOR.MINOR.PATCH version (for example, 3.13.9) rather than just MAJOR.MINOR (for example, 3.13). This prevents drift as new patch versions are released.\n\n\nWhy pin the exact version:\n\nPatch releases can introduce subtle behavior changes.\nReproducibility requires exact version matching.\nRegulatory submissions should document the exact Python version.\n\nCheck which Python versions are available:\nuv python list\nInstall a specific Python version if needed:\nuv python install 3.13.9",
    "crumbs": [
      "Environment and toolchain",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python projects with uv</span>"
    ]
  },
  {
    "objectID": "env-uv.html#managing-dependencies",
    "href": "env-uv.html#managing-dependencies",
    "title": "2  Python projects with uv",
    "section": "2.8 Managing dependencies",
    "text": "2.8 Managing dependencies\n\n2.8.1 Adding dependencies\nAdd runtime dependencies:\nuv add polars plotnine rtflite\nAdd development-only dependencies:\nuv add --dev ruff pytest mypy\nThis updates pyproject.toml:\n[project]\ndependencies = [\n    \"polars&gt;=1.34.0\",\n    \"plotnine&gt;=0.15.0\",\n    \"rtflite&gt;=1.0.2\",\n]\n\n[dependency-groups.dev]\ndependencies = [\n    \"ruff&gt;=0.14.1\",\n    \"pytest&gt;=8.4.2\",\n    \"mypy&gt;=1.18.2\",\n]\n\n\n\n\n\n\nNote\n\n\n\nBy default, uv adds dependencies with &gt;= constraints. This allows updates within compatible versions. The lock file ensures exact versions are used.\n\n\n\n\n2.8.2 Removing dependencies\nRemove a package:\nuv remove pandas\nThis removes the package from both pyproject.toml and the environment.",
    "crumbs": [
      "Environment and toolchain",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python projects with uv</span>"
    ]
  },
  {
    "objectID": "env-uv.html#lock-files-and-syncing",
    "href": "env-uv.html#lock-files-and-syncing",
    "title": "2  Python projects with uv",
    "section": "2.9 Lock files and syncing",
    "text": "2.9 Lock files and syncing\n\n2.9.1 Creating and updating the lock file\nGenerate or update the lock file:\nuv sync\nThis creates uv.lock, which records:\n\nExact version of every package.\nAll transitive dependencies.\nPackage hashes for verification.\n\nThe lock file ensures reproducibility across different machines and over time.\n\n\n2.9.2 Upgrading dependencies\nTo update packages while respecting constraints in pyproject.toml:\nuv lock --upgrade\nThen synchronize the environment:\nuv sync\nThis is similar to:\n\nR: renv::update() followed by renv::snapshot().\nNode.js: npm update followed by npm install.\n\n\n\nThe two-step process (lock & sync) gives you control: you can review lock file changes before updating your environment.",
    "crumbs": [
      "Environment and toolchain",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python projects with uv</span>"
    ]
  },
  {
    "objectID": "env-uv.html#running-commands",
    "href": "env-uv.html#running-commands",
    "title": "2  Python projects with uv",
    "section": "2.10 Running commands",
    "text": "2.10 Running commands\nYou have two options for running commands in your project environment.\n\n2.10.1 Option 1: Activate the virtual environment\nsource .venv/bin/activate  # macOS/Linux\n# or\n.venv\\Scripts\\activate     # Windows\nThen run commands directly:\npython -m pycsr_example\npytest\nruff check\nDeactivate when done:\ndeactivate\n\n\n2.10.2 Option 2: Use uv run\nRun commands without activation:\nuv run python -m pycsr_example\nuv run pytest\nuv run ruff check\n\n\n\n\n\n\nTip\n\n\n\nuv run is convenient for one-off commands and CI/CD scripts. For interactive work, activating the environment is often more ergonomic.\n\n\n\n\n2.10.3 uv run and uvx\nuvx runs tools in isolated, temporary environments:\nuvx ruff check .\nuvx black --check .\nUse uvx when:\n\nRunning tools you don’t want to install in the project.\nTrying packages without adding them as dependencies.\nRunning scripts that declare their own dependencies.\n\nUse uv run when:\n\nRunning project code.\nRunning tests.\nUsing project dependencies.\n\nSee using tools in uv for details.",
    "crumbs": [
      "Environment and toolchain",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python projects with uv</span>"
    ]
  },
  {
    "objectID": "env-uv.html#building-and-publishing",
    "href": "env-uv.html#building-and-publishing",
    "title": "2  Python projects with uv",
    "section": "2.11 Building and publishing",
    "text": "2.11 Building and publishing\nFor creating distributable packages, you need a build backend. The simplest option is hatchling.\nAdd to pyproject.toml:\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n2.11.1 Build wheel\nCreate distribution files:\nuv build\nThis creates: - dist/pycsr_example-0.1.0.tar.gz (source distribution) - dist/pycsr_example-0.1.0-py3-none-any.whl (wheel)\n\n\n2.11.2 Publish to PyPI\nPublish to the Python Package Index:\nuv publish\n\n\n\n\n\n\nNote\n\n\n\nBuilding and publishing are not typically needed for internal clinical reporting projects. However, if you develop reusable tools like table generation packages, open sourcing in a GitHub repository and publishing on PyPI will make them more visible.",
    "crumbs": [
      "Environment and toolchain",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python projects with uv</span>"
    ]
  },
  {
    "objectID": "env-uv.html#exercise",
    "href": "env-uv.html#exercise",
    "title": "2  Python projects with uv",
    "section": "2.12 Exercise",
    "text": "2.12 Exercise\nCreate a small project to practice uv commands:\n\nInitialize a new project called csr-practice.\nPin Python to version 3.13.9 (or latest available).\nAdd polars as a dependency.\nAdd pytest as a development dependency.\nExamine the generated pyproject.toml and uv.lock files.\nRun Python using uv run python --version.\n\n\n\nView solution\n\n# Initialize project\nuv init csr-practice\ncd csr-practice\n\n# Pin Python version\nuv python pin 3.13.9\n\n# Add dependencies\nuv add polars\nuv add --dev pytest\n\n# View configuration\ncat pyproject.toml\n\n# Check lock file\ncat uv.lock\n\n# Run Python\nuv run python --version\nYour pyproject.toml should look similar to:\n[project]\nname = \"csr-practice\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\ndependencies = [\n    \"polars&gt;=1.18.0\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pytest&gt;=8.3.4\",\n]\n\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"",
    "crumbs": [
      "Environment and toolchain",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python projects with uv</span>"
    ]
  },
  {
    "objectID": "env-uv.html#whats-next",
    "href": "env-uv.html#whats-next",
    "title": "2  Python projects with uv",
    "section": "2.13 What’s next",
    "text": "2.13 What’s next\nNow that you understand uv basics, the next chapter covers the Python package toolchain:\n\nFormatting and linting with Ruff.\nType checking with mypy.\nTesting with pytest.\nDocumentation generation.\nDevelopment workflows for clinical reporting.",
    "crumbs": [
      "Environment and toolchain",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python projects with uv</span>"
    ]
  },
  {
    "objectID": "env-pkg.html",
    "href": "env-pkg.html",
    "title": "3  Python package toolchain",
    "section": "",
    "text": "3.1 The modern Python toolchain\nIn R, packages like devtools, usethis, styler, lintr, and testthat provide development infrastructure. Python’s ecosystem distributes these functions across specialized tools.\nFor clinical reporting projects, we recommend:\nAll tools are installed as development dependencies and configured through pyproject.toml.",
    "crumbs": [
      "Environment and toolchain",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python package toolchain</span>"
    ]
  },
  {
    "objectID": "env-pkg.html#the-modern-python-toolchain",
    "href": "env-pkg.html#the-modern-python-toolchain",
    "title": "3  Python package toolchain",
    "section": "",
    "text": "uv: Package and environment management.\nRuff: Code formatting and linting.\nmypy: Static type checking.\npytest: Unit testing framework.\nquartodoc: Documentation and reporting.\n\n\n\nFor R users, think of this as: uv = renv + pak + devtools, Ruff = styler + lintr, pytest = testthat, mypy = (no direct R equivalent).",
    "crumbs": [
      "Environment and toolchain",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python package toolchain</span>"
    ]
  },
  {
    "objectID": "env-pkg.html#ruff-formatting-and-linting",
    "href": "env-pkg.html#ruff-formatting-and-linting",
    "title": "3  Python package toolchain",
    "section": "3.2 Ruff: Formatting and linting",
    "text": "3.2 Ruff: Formatting and linting\nRuff is an super fast linter and formatter written in Rust. It replaces multiple legacy tools (Black, isort, Flake8, pyupgrade) with a single, consistent interface.\n\n3.2.1 Installation\nAdd Ruff as a development dependency:\nuv add --dev ruff\n\n\n3.2.2 Code formatting\nFormat your code:\nuv run ruff format\nOr using uvx:\nuvx ruff format\nRuff format:\n\nEnforces consistent style (like Black).\nSorts imports automatically.\nRemoves trailing whitespace.\nEnsures consistent line lengths.\n\n\n\n3.2.3 Linting\nCheck for linting issues:\nuv run ruff check\nFix auto-fixable issues:\nuv run ruff check --fix\nRuff detects:\n\nUnused imports and variables.\nUndefined names.\nStyle violations.\nCommon anti-patterns.\nSecurity issues.\n\n\n\n3.2.4 Configuration\nAdd Ruff configuration to pyproject.toml:\n[tool.ruff]\nline-length = 88\ntarget-version = \"py313\"\n\n[tool.ruff.format]\nquote-style = \"double\"\nindent-style = \"space\"\n\n[tool.ruff.lint]\nselect = [\n    \"E\",    # pycodestyle\n    \"F\",    # Pyflakes\n    \"UP\",   # pyupgrade\n    \"B\",    # flake8-bugbear\n    \"SIM\",  # flake8-simplify\n    \"I\",    # isort\n]\nignore = []\n\n\n\n\n\n\nNote\n\n\n\nLine length of 88 characters is the Python community standard. It balances readability with modern screen sizes.",
    "crumbs": [
      "Environment and toolchain",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python package toolchain</span>"
    ]
  },
  {
    "objectID": "env-pkg.html#type-checking-with-mypy",
    "href": "env-pkg.html#type-checking-with-mypy",
    "title": "3  Python package toolchain",
    "section": "3.3 Type checking with mypy",
    "text": "3.3 Type checking with mypy\nPython supports optional type annotations through PEP 484. Type annotations improve code clarity and catch errors before runtime.\n\n3.3.1 Why type checking matters\nFor clinical programming:\n\nCatch data transformation errors at development time.\nDocument expected DataFrame structures.\nImprove IDE autocomplete and refactoring.\nReduce runtime errors in production.\n\n\n\n3.3.2 Installation\nAdd mypy as a development dependency:\nuv add --dev mypy\n\n\n3.3.3 Basic usage\nCheck types in your code:\nuv run mypy .\n\n\n3.3.4 Type annotation example\nWithout types:\ndef calculate_bmi(weight, height):\n    return weight / (height ** 2)\nWith types:\ndef calculate_bmi(weight: float, height: float) -&gt; float:\n    \"\"\"Calculate BMI from weight (kg) and height (m).\"\"\"\n    return weight / (height ** 2)\nThe type checker verifies:\n\nArguments are the correct type.\nReturn value matches the declared type.\nOperations are valid for the types used.\n\n\n\n3.3.5 Configuration\nAdd mypy settings to pyproject.toml:\n[tool.mypy]\npython_version = \"3.13\"\nwarn_return_any = true\nwarn_unused_configs = true\ndisallow_untyped_defs = false\ndisallow_incomplete_defs = true\ncheck_untyped_defs = true\nno_implicit_optional = true\n\n\nStart with lenient settings (disallow_untyped_defs = false) and progressively tighten as you add type annotations to your codebase.\n\n\n3.3.6 Type stubs for libraries\nSome libraries don’t include type information. Install type stubs when available:\nuv add --dev types-tabulate\n\n\n\n\n\n\nNote\n\n\n\nPopular data science libraries like polars include built-in type annotations. Older libraries like pandas require separate stub packages (pandas-stubs).",
    "crumbs": [
      "Environment and toolchain",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python package toolchain</span>"
    ]
  },
  {
    "objectID": "env-pkg.html#testing-with-pytest",
    "href": "env-pkg.html#testing-with-pytest",
    "title": "3  Python package toolchain",
    "section": "3.4 Testing with pytest",
    "text": "3.4 Testing with pytest\npytest is Python’s de facto standard testing framework. It’s more powerful and ergonomic than the built-in unittest module.\n\n3.4.1 Installation\nAdd pytest and coverage tools:\nuv add --dev pytest pytest-cov\n\n\n3.4.2 Writing tests\nCreate a tests/ directory:\npycsr-example/\n├── src/\n│   └── pycsr_example/\n│       └── __init__.py\n└── tests/\n    └── test_calculations.py\nWrite a simple test in tests/test_calculations.py:\nfrom pycsr_example.calculations import calculate_bmi\nimport pytest\n\ndef test_calculate_bmi():\n    # Normal BMI calculation\n    assert calculate_bmi(70, 1.75) == pytest.approx(22.857142857142858)\n\ndef test_calculate_bmi_underweight():\n    # BMI &lt; 18.5 indicates underweight\n    assert calculate_bmi(50, 1.75) &lt; 18.5\n\n\n3.4.3 Running tests\nRun all tests:\nuv run pytest\nRun with verbose output:\nuv run pytest -v\nRun specific test file:\nuv run pytest tests/test_calculations.py\n\n\n3.4.4 Code coverage\nGenerate coverage report:\nuv run pytest --cov=pycsr_example --cov-report=term\nGenerate HTML coverage report:\nuv run pytest --cov=pycsr_example --cov-report=html\nThis creates htmlcov/index.html showing which lines are tested.\n\n\n\n\n\n\nImportant\n\n\n\nFor regulatory submissions, high test coverage demonstrates code quality. Aim for &gt;80% coverage for critical data transformation and statistical computation functions.\n\n\n\n\n3.4.5 pytest configuration\nAdd pytest settings to pyproject.toml:\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\npython_files = [\"test_*.py\"]\npython_functions = [\"test_*\"]\naddopts = [\n    \"--strict-markers\",\n    \"--strict-config\",\n    \"-ra\",\n]",
    "crumbs": [
      "Environment and toolchain",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python package toolchain</span>"
    ]
  },
  {
    "objectID": "env-pkg.html#documentation-generation",
    "href": "env-pkg.html#documentation-generation",
    "title": "3  Python package toolchain",
    "section": "3.5 Documentation generation",
    "text": "3.5 Documentation generation\nFor clinical reporting projects, documentation serves two purposes:\n\nCode documentation: Function and module documentation.\nReport generation: Analysis reports and TLFs.\n\n\n3.5.1 Quarto for reports\nWe use Quarto for creating reproducible analysis documents:\n# Install Quarto separately (not via uv)\n# See: https://quarto.org/docs/get-started/\nQuarto documents (.qmd files) combine:\n\nMarkdown text.\nPython code cells.\nGenerated outputs (tables, listings, figures).\n\nThis book itself is written in Quarto.\n\n\n3.5.2 quartodoc for API documentation\nFor packages that need API documentation (similar to R’s pkgdown), use quartodoc:\nuv add --dev quartodoc\nquartodoc generates documentation from docstrings and integrates with Quarto for full website generation.\n\n\nFor analysis projects (rather than reusable packages), Quarto alone is usually sufficient. Use quartodoc when building analysis packages for team to collaborate on.",
    "crumbs": [
      "Environment and toolchain",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python package toolchain</span>"
    ]
  },
  {
    "objectID": "env-pkg.html#development-workflow",
    "href": "env-pkg.html#development-workflow",
    "title": "3  Python package toolchain",
    "section": "3.6 Development workflow",
    "text": "3.6 Development workflow\nPutting it all together, a typical development cycle looks like:\n\nFormat code: uv run ruff format\nCheck linting: uv run ruff check --fix\nVerify types: uv run mypy .\nRun tests: uv run pytest --cov=pycsr_example\nGenerate reports: quarto render\n\n\n3.6.1 Pre-commit automation\nYou can automate these checks using Git hooks (not covered in this book), but manual execution provides better learning and control during development.",
    "crumbs": [
      "Environment and toolchain",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python package toolchain</span>"
    ]
  },
  {
    "objectID": "env-pkg.html#clinical-project-structure-guidelines",
    "href": "env-pkg.html#clinical-project-structure-guidelines",
    "title": "3  Python package toolchain",
    "section": "3.7 Clinical project structure guidelines",
    "text": "3.7 Clinical project structure guidelines\nIn case you need clinical reporting projects using both R and Python:\nSeparate R and Python directories:\nproject/\n├── r-package/          # R package for R-based analyses\n│   ├── DESCRIPTION\n│   ├── R/\n│   └── tests/\n├── python-package/     # Python package for Python-based analyses\n│   ├── pyproject.toml\n│   ├── src/\n│   └── tests/\n├── data/               # Shared input data (SDTM, ADaM)\n└── output/             # Shared output (TLFs, reports)\nWhy separate?\nAs John Carmack noted: “It’s almost always a mistake to mix languages in a single project.”\nReasons:\n\nDifferent build systems.\nDifferent dependency management.\nDifferent testing frameworks.\nDifferent IDE configurations.\n\nShared resources:\n\nInput datasets (SDTM, ADaM) can be in a common data/ directory.\nOutput deliverables can go to a common output/ directory.\nDocumentation can reference both implementations.\n\n\n\n\n\n\n\nNote\n\n\n\nFor this book, we focus exclusively on Python. Mixed R/Python workflows are beyond scope but follow the same principles.",
    "crumbs": [
      "Environment and toolchain",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python package toolchain</span>"
    ]
  },
  {
    "objectID": "env-pkg.html#exercise",
    "href": "env-pkg.html#exercise",
    "title": "3  Python package toolchain",
    "section": "3.8 Exercise",
    "text": "3.8 Exercise\nSet up a complete development environment:\n\nCreate a new project with uv init dev-practice.\nAdd development dependencies: ruff, mypy, pytest, pytest-cov.\nCreate a simple function in src/dev_practice/stats.py:\ndef mean(values: list[float]) -&gt; float:\n    return sum(values) / len(values)\nWrite a test in tests/test_stats.py.\nRun Ruff format and check.\nRun mypy type checking.\nRun pytest with coverage.\n\n\n\nView solution\n\n# Create project\nuv init dev-practice\ncd dev-practice\n\n# Add dev dependencies\nuv add --dev ruff mypy pytest pytest-cov\n\n# Create stats module\nmkdir -p src/dev_practice\ncat &gt; src/dev_practice/stats.py &lt;&lt; 'EOF'\ndef mean(values: list[float]) -&gt; float:\n    \"\"\"Calculate the arithmetic mean of a list of numbers.\"\"\"\n    if not values:\n        raise ValueError(\"Cannot calculate mean of empty list\")\n    return sum(values) / len(values)\nEOF\n\n# Create test file\nmkdir -p tests\ncat &gt; tests/test_stats.py &lt;&lt; 'EOF'\nimport pytest\nfrom dev_practice.stats import mean\n\ndef test_mean_basic():\n    assert mean([1.0, 2.0, 3.0]) == 2.0\n\ndef test_mean_single_value():\n    assert mean([5.0]) == 5.0\n\ndef test_mean_empty_raises():\n    with pytest.raises(ValueError):\n        mean([])\nEOF\n\n# Run checks\nuv run ruff format .\nuv run ruff check .\nuv run mypy src/\nuv run pytest --cov=dev_practice --cov-report=term\nExpected output from pytest:\n============================= test session starts ==============================\ncollected 3 items\n\ntests/test_stats.py ...                                                  [100%]\n\n---------- coverage: platform darwin, python 3.13.9-final-0 ----------\nName                        Stmts   Miss  Cover\n-----------------------------------------------\nsrc/dev_practice/__init__.py    0      0   100%\nsrc/dev_practice/stats.py       4      0   100%\n-----------------------------------------------\nTOTAL                           4      0   100%\n\n============================== 3 passed in 0.05s ===============================",
    "crumbs": [
      "Environment and toolchain",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python package toolchain</span>"
    ]
  },
  {
    "objectID": "env-pkg.html#example-repositories",
    "href": "env-pkg.html#example-repositories",
    "title": "3  Python package toolchain",
    "section": "3.9 Example repositories",
    "text": "3.9 Example repositories\nDemo project repositories have been created:\n\nPython package example: [Link to be added]\neCTD package example: [Link to be added]\n\nWith the knowledge from this chapter, you can understand how these projects are organized and develop similar professional Python packages for clinical reporting.",
    "crumbs": [
      "Environment and toolchain",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python package toolchain</span>"
    ]
  },
  {
    "objectID": "env-pkg.html#whats-next",
    "href": "env-pkg.html#whats-next",
    "title": "3  Python package toolchain",
    "section": "3.10 What’s next",
    "text": "3.10 What’s next\nYou now have a complete Python development environment with:\n\nuv for project and dependency management.\nRuff for code quality.\nmypy for type safety.\npytest for testing.\nQuarto for documentation.\n\nNext part will introduce how to create real clinical study reports, demonstrating TLF generation with polars and rtflite.",
    "crumbs": [
      "Environment and toolchain",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python package toolchain</span>"
    ]
  },
  {
    "objectID": "tlf-overview.html",
    "href": "tlf-overview.html",
    "title": "4  TLF overview",
    "section": "",
    "text": "4.1 Overview\nTables, listings, and figures (TLFs) are essential components of clinical study reports (CSRs) and regulatory submissions. Following ICH E3 guidance, TLFs provide standardized summaries of clinical trial data that support regulatory decision-making.\nThis chapter provides an overview of creating TLFs using Python, focusing on the tools and workflows demonstrated throughout this book.",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>TLF overview</span>"
    ]
  },
  {
    "objectID": "tlf-overview.html#background",
    "href": "tlf-overview.html#background",
    "title": "4  TLF overview",
    "section": "4.2 Background",
    "text": "4.2 Background\nSubmitting clinical trial results to regulatory agencies is a crucial aspect of clinical development. The Electronic Common Technical Document (eCTD) has emerged as the global standard format for regulatory submissions. For instance, the United States Food and Drug Administration (US FDA) mandates the use of eCTD for new drug applications and biologics license applications.\nA CSR provides comprehensive information about the methods and results of an individual clinical study. To support the statistical analysis, numerous tables, listings, and figures are included within the main text and appendices. The creation of a CSR is a collaborative effort that involves various professionals such as clinicians, medical writers, statisticians, and statistical programmers.\nWithin an organization, these professionals typically collaborate to define, develop, validate, and deliver the necessary TLFs for a CSR. These TLFs serve to summarize the efficacy and/or safety of the pharmaceutical product under study. In the pharmaceutical industry, Microsoft Word is widely utilized for CSR preparation. As a result, the deliverables from statisticians and statistical programmers are commonly provided in formats such as .rtf, .doc, .docx to align with industry standards and requirements.\n\n\n\n\n\n\nNote\n\n\n\nEach organization may define specific TLF format requirements that differ from the examples in this book. It is advisable to consult and adhere to the guidelines and specifications set by your respective organization when preparing TLFs for submission.\n\n\nBy following the ICH E3 guidance, most TLFs in a CSR are located at:\n\nSection 10: Study participants\nSection 11: Efficacy evaluation\nSection 12: Safety evaluation\nSection 14: Tables, listings, and figures referenced but not included in the text\nSection 16: Appendices",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>TLF overview</span>"
    ]
  },
  {
    "objectID": "tlf-overview.html#datasets",
    "href": "tlf-overview.html#datasets",
    "title": "4  TLF overview",
    "section": "4.3 Datasets",
    "text": "4.3 Datasets\nThe dataset structure follows CDISC Analysis Data Model (ADaM).\nIn this project, we use publicly available CDISC pilot study data, which is accessible through the CDISC GitHub repository.\nWe have converted these datasets from the .xpt format to the .parquet format for ease of use and compatibility with Python tools. The dataset structure adheres to the CDISC Analysis Data Model (ADaM) standard.",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>TLF overview</span>"
    ]
  },
  {
    "objectID": "tlf-overview.html#tools",
    "href": "tlf-overview.html#tools",
    "title": "4  TLF overview",
    "section": "4.4 Tools",
    "text": "4.4 Tools\nTo exemplify the generation of TLFs in RTF format, we rely on the functionality provided by two Python packages:\n\nPolars: Preparation of datasets in a format suitable for reporting purposes. Polars offers a comprehensive suite of tools and functions for data manipulation and transformation, ensuring that the data is structured appropriately.\nrtflite: Creation of RTF files. The rtflite package offers functions specifically designed for generating RTF files, allowing us to produce TLFs in the desired format.",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>TLF overview</span>"
    ]
  },
  {
    "objectID": "tlf-overview.html#polars",
    "href": "tlf-overview.html#polars",
    "title": "4  TLF overview",
    "section": "4.5 Polars",
    "text": "4.5 Polars\nPolars is an open-source library for data manipulation implemented in Rust with Python bindings. It offers exceptional performance while maintaining a user-friendly interface for interactive data analysis.\nKey advantages of Polars include:\n\nPerformance: 10-100x faster than pandas for most operations due to Rust implementation\nMemory efficiency: Lazy evaluation and columnar storage reduce memory usage\nFamiliar syntax: Similar to tidyverse-style pipelines, making it accessible to R users\nType safety: Strong typing system that catches errors early in development\n\nThe creators of Polars have provided exceptional documentation and tutorials that serve as valuable resources for learning and mastering the functionalities of the library.\nFurthermore, several books are available that serve as introductions to Polars:\n\nPython Polars: The Definitive Guide\n\n\n\n\n\n\n\nNote\n\n\n\nIn this book, we assume that the reader has some experience with data manipulation concepts. This prior knowledge enables a more efficient and focused exploration of the clinical reporting concepts presented throughout the book.\n\n\nTo illustrate the basic usage of Polars, let’s work with a sample ADSL dataset. This dataset contains subject-level information from a clinical trial, which will serve as a practical example for generating summaries using Polars.\n\nimport polars as pl\n\n\n\npolars.config.Config\n\n\n\n# Read clinical data\nadsl = pl.read_parquet(\"data/adsl.parquet\")\n\n# Select columns\nadsl = adsl.select([\"USUBJID\", \"TRT01A\", \"AGE\", \"SEX\"])\n\n# Basic data exploration\nadsl\n\n\nshape: (254, 4)\n\n\n\nUSUBJID\nTRT01A\nAGE\nSEX\n\n\nstr\nstr\nf64\nstr\n\n\n\n\n\"01-701-1015\"\n\"Placebo\"\n63.0\n\"Female\"\n\n\n\"01-701-1023\"\n\"Placebo\"\n64.0\n\"Male\"\n\n\n\"01-701-1028\"\n\"Xanomeline High Dose\"\n71.0\n\"Male\"\n\n\n…\n…\n…\n…\n\n\n\"01-718-1371\"\n\"Xanomeline High Dose\"\n69.0\n\"Female\"\n\n\n\"01-718-1427\"\n\"Xanomeline High Dose\"\n74.0\n\"Female\"\n\n\n\n\n\n\nKey Polars operations for clinical reporting include:\n\n4.5.1 I/O\nPolars supports multiple data formats for input and output (see the I/O guide). For clinical development, we recommend the .parquet format because tools in Python, R, and Julia can read and write it without conversion. The example below loads subject-level ADSL data with Polars.\n\nimport polars as pl\n\n\nadsl = pl.read_parquet(\"data/adsl.parquet\")\nadsl = adsl.select(\"STUDYID\", \"USUBJID\", \"TRT01A\", \"AGE\", \"SEX\") # select columns\nadsl\n\n\nshape: (254, 5)\n\n\n\nSTUDYID\nUSUBJID\nTRT01A\nAGE\nSEX\n\n\nstr\nstr\nstr\nf64\nstr\n\n\n\n\n\"CDISCPILOT01\"\n\"01-701-1015\"\n\"Placebo\"\n63.0\n\"Female\"\n\n\n\"CDISCPILOT01\"\n\"01-701-1023\"\n\"Placebo\"\n64.0\n\"Male\"\n\n\n\"CDISCPILOT01\"\n\"01-701-1028\"\n\"Xanomeline High Dose\"\n71.0\n\"Male\"\n\n\n…\n…\n…\n…\n…\n\n\n\"CDISCPILOT01\"\n\"01-718-1371\"\n\"Xanomeline High Dose\"\n69.0\n\"Female\"\n\n\n\"CDISCPILOT01\"\n\"01-718-1427\"\n\"Xanomeline High Dose\"\n74.0\n\"Female\"\n\n\n\n\n\n\n\n\n4.5.2 Filtering\nFiltering in Polars uses the .filter() method with column expressions. Below are examples applied to the ADSL data.\n\n# Filter female subjects\nadsl.filter(pl.col(\"SEX\") == \"Female\")\n\n\nshape: (143, 5)\n\n\n\nSTUDYID\nUSUBJID\nTRT01A\nAGE\nSEX\n\n\nstr\nstr\nstr\nf64\nstr\n\n\n\n\n\"CDISCPILOT01\"\n\"01-701-1015\"\n\"Placebo\"\n63.0\n\"Female\"\n\n\n\"CDISCPILOT01\"\n\"01-701-1034\"\n\"Xanomeline High Dose\"\n77.0\n\"Female\"\n\n\n\"CDISCPILOT01\"\n\"01-701-1047\"\n\"Placebo\"\n85.0\n\"Female\"\n\n\n…\n…\n…\n…\n…\n\n\n\"CDISCPILOT01\"\n\"01-718-1371\"\n\"Xanomeline High Dose\"\n69.0\n\"Female\"\n\n\n\"CDISCPILOT01\"\n\"01-718-1427\"\n\"Xanomeline High Dose\"\n74.0\n\"Female\"\n\n\n\n\n\n\n\n# Filter subjects with Age &gt;= 65\nadsl.filter(pl.col(\"AGE\") &gt;= 65)\n\n\nshape: (221, 5)\n\n\n\nSTUDYID\nUSUBJID\nTRT01A\nAGE\nSEX\n\n\nstr\nstr\nstr\nf64\nstr\n\n\n\n\n\"CDISCPILOT01\"\n\"01-701-1028\"\n\"Xanomeline High Dose\"\n71.0\n\"Male\"\n\n\n\"CDISCPILOT01\"\n\"01-701-1033\"\n\"Xanomeline Low Dose\"\n74.0\n\"Male\"\n\n\n\"CDISCPILOT01\"\n\"01-701-1034\"\n\"Xanomeline High Dose\"\n77.0\n\"Female\"\n\n\n…\n…\n…\n…\n…\n\n\n\"CDISCPILOT01\"\n\"01-718-1371\"\n\"Xanomeline High Dose\"\n69.0\n\"Female\"\n\n\n\"CDISCPILOT01\"\n\"01-718-1427\"\n\"Xanomeline High Dose\"\n74.0\n\"Female\"\n\n\n\n\n\n\n\n\n4.5.3 Deriving\nDeriving new variables is common in clinical data analysis for creating age groups, BMI categories, or treatment flags. Polars uses .with_columns() to add new columns while keeping existing ones.\n\n# Create age groups\nadsl.with_columns([\n    pl.when(pl.col(\"AGE\") &lt; 65)\n      .then(pl.lit(\"&lt;65\"))\n      .otherwise(pl.lit(\"&gt;=65\"))\n      .alias(\"AGECAT\")\n])\n\n\nshape: (254, 6)\n\n\n\nSTUDYID\nUSUBJID\nTRT01A\nAGE\nSEX\nAGECAT\n\n\nstr\nstr\nstr\nf64\nstr\nstr\n\n\n\n\n\"CDISCPILOT01\"\n\"01-701-1015\"\n\"Placebo\"\n63.0\n\"Female\"\n\"&lt;65\"\n\n\n\"CDISCPILOT01\"\n\"01-701-1023\"\n\"Placebo\"\n64.0\n\"Male\"\n\"&lt;65\"\n\n\n\"CDISCPILOT01\"\n\"01-701-1028\"\n\"Xanomeline High Dose\"\n71.0\n\"Male\"\n\"&gt;=65\"\n\n\n…\n…\n…\n…\n…\n…\n\n\n\"CDISCPILOT01\"\n\"01-718-1371\"\n\"Xanomeline High Dose\"\n69.0\n\"Female\"\n\"&gt;=65\"\n\n\n\"CDISCPILOT01\"\n\"01-718-1427\"\n\"Xanomeline High Dose\"\n74.0\n\"Female\"\n\"&gt;=65\"\n\n\n\n\n\n\n\n\n4.5.4 Grouping\nGrouping operations are fundamental for creating summary statistics in clinical reports. Polars uses group_by() followed by aggregation functions to compute counts, means, and other statistics by categorical variables like treatment groups.\nThe .count() method provides a quick way to get subject counts by group.\n\n# Count by treatment group\nadsl.group_by(\"TRT01A\").len().sort(\"TRT01A\")\n\n\nshape: (3, 2)\n\n\n\nTRT01A\nlen\n\n\nstr\nu32\n\n\n\n\n\"Placebo\"\n86\n\n\n\"Xanomeline High Dose\"\n84\n\n\n\"Xanomeline Low Dose\"\n84\n\n\n\n\n\n\nYou can also use .agg() with multiple aggregation functions:\n\n# Age statistics by treatment group\nadsl.group_by(\"TRT01A\").agg([\n    pl.col(\"AGE\").mean().round(1).alias(\"mean_age\"),\n    pl.col(\"AGE\").std().round(2).alias(\"sd_age\")\n]).sort(\"TRT01A\")\n\n\nshape: (3, 3)\n\n\n\nTRT01A\nmean_age\nsd_age\n\n\nstr\nf64\nf64\n\n\n\n\n\"Placebo\"\n75.2\n8.59\n\n\n\"Xanomeline High Dose\"\n74.4\n7.89\n\n\n\"Xanomeline Low Dose\"\n75.7\n8.29\n\n\n\n\n\n\n\n\n4.5.5 Joining\nJoining datasets is essential for combining subject-level data (ADSL) with event-level data (e.g. ADAE, ADLB). Polars supports various join types including inner, left, and full joins.\nHere is a toy example that splits ADSL and joins it back by USUBJID.\n\n# Create a simple demographics subset\ndemo = adsl.select(\"USUBJID\", \"AGE\", \"SEX\").head(3)\n\n# Create treatment info subset\ntrt = adsl.select(\"USUBJID\", \"TRT01A\").head(3)\n\n# Left join to combine datasets\ndemo.join(trt, on=\"USUBJID\", how=\"left\")\n\n\nshape: (3, 4)\n\n\n\nUSUBJID\nAGE\nSEX\nTRT01A\n\n\nstr\nf64\nstr\nstr\n\n\n\n\n\"01-701-1015\"\n63.0\n\"Female\"\n\"Placebo\"\n\n\n\"01-701-1023\"\n64.0\n\"Male\"\n\"Placebo\"\n\n\n\"01-701-1028\"\n71.0\n\"Male\"\n\"Xanomeline High Dose\"\n\n\n\n\n\n\n\n\n4.5.6 Pivoting\nPivoting transforms data from long to wide format, commonly needed for creating tables. Use .pivot() to reshape grouped data into columns.\n\n# Create summary by treatment and sex\n(\n    adsl\n        .group_by([\"TRT01A\", \"SEX\"])\n        .agg(pl.len().alias(\"n\"))\n        .pivot(\n            values=\"n\",\n            index=\"SEX\",\n            on=\"TRT01A\"\n        )\n)\n\n\nshape: (2, 4)\n\n\n\nSEX\nXanomeline High Dose\nXanomeline Low Dose\nPlacebo\n\n\nstr\nu32\nu32\nu32\n\n\n\n\n\"Male\"\n44\n34\n33\n\n\n\"Female\"\n40\n50\n53\n\n\n\n\n\n\nHaving covered the essential Polars operations for data manipulation, we now turn to the second component of our clinical reporting workflow: formatting and presenting the processed data in regulatory-compliant RTF format.",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>TLF overview</span>"
    ]
  },
  {
    "objectID": "tlf-overview.html#rtflite",
    "href": "tlf-overview.html#rtflite",
    "title": "4  TLF overview",
    "section": "4.6 rtflite",
    "text": "4.6 rtflite\n\nimport rtflite as rtf \n\nrtflite is a Python package for creating production-ready tables and figures in RTF format. While Polars handles the data processing and statistical calculations, rtflite focuses exclusively on the presentation layer. The package is designed to:\n\nProvide simple Python classes that map to table elements (title, headers, body, footnotes) for intuitive table construction.\nOffer a canonical Python API with a clear, composable interface.\nFocus exclusively on table formatting and layout, leaving data manipulation to dataframe libraries like polars or pandas.\nMinimize external dependencies for maximum portability and reliability.\n\nCreating an RTF table involves three steps:\n\nDesign the desired table layout and structure.\nConfigure the appropriate rtflite components.\nGenerate and save the RTF document.\n\nThis guide introduces rtflite’s core components and demonstrates how to turn dataframes into Tables, Listings, and Figures (TLFs) for clinical reporting.\n\n4.6.1 Data: adverse events\nTo explore the RTF generation capabilities in rtflite, we will use the dataset data/adae.parquet. This dataset contains adverse event (AE) information from a clinical trial.\nBelow are the meanings of relevant variables:\n\nUSUBJID: Unique Subject Identifier\nTRTA: Actual Treatment\nAEDECOD: Dictionary-Derived Term\n\n\n# Load adverse events data\ndf = pl.read_parquet(\"data/adae.parquet\")\n\ndf.select([\"USUBJID\", \"TRTA\", \"AEDECOD\"])\n\n\nshape: (1_191, 3)\n\n\n\nUSUBJID\nTRTA\nAEDECOD\n\n\nstr\nstr\nstr\n\n\n\n\n\"01-701-1015\"\n\"Placebo\"\n\"APPLICATION SITE ERYTHEMA\"\n\n\n\"01-701-1015\"\n\"Placebo\"\n\"APPLICATION SITE PRURITUS\"\n\n\n\"01-701-1015\"\n\"Placebo\"\n\"DIARRHOEA\"\n\n\n…\n…\n…\n\n\n\"01-718-1427\"\n\"Xanomeline High Dose\"\n\"DECREASED APPETITE\"\n\n\n\"01-718-1427\"\n\"Xanomeline High Dose\"\n\"NAUSEA\"\n\n\n\n\n\n\n\n\n4.6.2 Table-ready data\nIn this AE example, we provide the number of subjects with each type of AE by treatment group.\n\ntbl = (\n    df.group_by([\"TRTA\", \"AEDECOD\"])\n    .agg(pl.len().alias(\"n\"))\n    .sort(\"TRTA\")\n    .pivot(values=\"n\", index=\"AEDECOD\", on=\"TRTA\")\n    .fill_null(0)\n    .sort(\"AEDECOD\")  # Sort by adverse event name to match R output\n)\n\ntbl\n\n\nshape: (242, 4)\n\n\n\nAEDECOD\nPlacebo\nXanomeline High Dose\nXanomeline Low Dose\n\n\nstr\nu32\nu32\nu32\n\n\n\n\n\"ABDOMINAL DISCOMFORT\"\n0\n1\n0\n\n\n\"ABDOMINAL PAIN\"\n1\n2\n3\n\n\n\"ACROCHORDON EXCISION\"\n0\n1\n0\n\n\n…\n…\n…\n…\n\n\n\"WOUND\"\n0\n0\n2\n\n\n\"WOUND HAEMORRHAGE\"\n0\n1\n0\n\n\n\n\n\n\n\n\n4.6.3 Table component classes\nrtflite provides dedicated classes for each table component. Commonly used classes include:\n\nRTFPage: RTF page information (orientation, margins, pagination).\nRTFPageHeader: Page headers with page numbering (compatible with r2rtf).\nRTFPageFooter: Page footers for attribution and notices.\nRTFTitle: RTF title information.\nRTFColumnHeader: RTF column header information.\nRTFBody: RTF table body information.\nRTFFootnote: RTF footnote information.\nRTFSource: RTF data source information.\n\nThese component classes work together to build complete RTF documents. A full list of all classes and their parameters can be found in the API reference.\n\n\n4.6.4 Simple example\nA minimal example below illustrates how to combine components to create an RTF table.\n\nRTFBody() defines table body layout.\nRTFDocument() transfers table layout information into RTF syntax.\nwrite_rtf() saves encoded RTF into a .rtf file.\n\n\n\nrtf/tlf_overview1.rtf\n\n\n\n\nPosixPath('pdf/tlf_overview1.pdf')\n\n\n\n\n\n4.6.5 Column width\nIf we want to adjust the width of each column to provide more space to the first column, this can be achieved by updating col_rel_width in RTFBody.\nThe input of col_rel_width is a list with the same length as the number of columns. This argument defines the relative length of each column within a pre-defined total column width.\nIn this example, the defined relative width is 3:2:2:2. Only the ratio of col_rel_width is used. Therefore it is equivalent to use col_rel_width = [6,4,4,4] or col_rel_width = [1.5,1,1,1].\n\n\nrtf/tlf_overview2.rtf\n\n\n\n\nPosixPath('pdf/tlf_overview2.pdf')\n\n\n\n\n\n4.6.6 Column headers\nIn RTFColumnHeader, the text argument provides the column header content as a list of strings.\n\n\nrtf/tlf_overview3.rtf\n\n\n\n\nPosixPath('pdf/tlf_overview3.pdf')\n\n\n\nWe also allow column headers to be displayed in multiple lines. If an empty column name is needed for a column, you can insert an empty string. For example, [\"name 1\", \"\", \"name 3\"].\nIn RTFColumnHeader, the col_rel_width can be used to align column headers with different numbers of columns.\nBy using RTFColumnHeader with col_rel_width, one can customize complex column headers. If there are multiple pages, the column header will repeat on each page by default.\n\n\nrtf/tlf_overview4.rtf\n\n\n\n\nPosixPath('pdf/tlf_overview4.pdf')\n\n\n\n\n\n4.6.7 Titles, footnotes, and data source\nRTF documents can include additional components to provide context and documentation:\n\nRTFTitle: Add document titles and subtitles\nRTFFootnote: Add explanatory footnotes\nRTFSource: Add data source attribution\n\n\n\nrtf/tlf_overview5.rtf\n\n\n\n\nPosixPath('pdf/tlf_overview5.pdf')\n\n\n\nNote the use of \\\\line in column headers to create line breaks within cells.\n\n\n4.6.8 Text formatting and alignment\nrtflite supports various text formatting options:\n\nText formatting: Bold (b), italic (i), underline (u), strikethrough (s)\nText alignment: Left (l), center (c), right (r), justify (j)\nFont properties: Font size, font family\n\n\n\nrtf/tlf_overview6.rtf\n\n\n\n\nPosixPath('pdf/tlf_overview6.pdf')\n\n\n\n\n\n4.6.9 Border customization\nTable borders can be customized extensively:\n\nBorder styles: single, double, thick, dotted, dashed\nBorder sides: border_top, border_bottom, border_left, border_right\nPage borders: border_first, border_last for first/last rows across pages\n\n\n\nrtf/tlf_overview7.rtf\n\n\n\n\nPosixPath('pdf/tlf_overview7.pdf')",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>TLF overview</span>"
    ]
  },
  {
    "objectID": "tlf-overview.html#next-steps",
    "href": "tlf-overview.html#next-steps",
    "title": "4  TLF overview",
    "section": "4.7 Next Steps",
    "text": "4.7 Next Steps\nHaving covered the fundamental concepts and tools for creating clinical TLFs with Python, readers can explore specific implementations based on their requirements:\nEach chapter provides step-by-step tutorials with reproducible code examples that can be adapted for specific clinical reporting requirements.",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>TLF overview</span>"
    ]
  },
  {
    "objectID": "tlf-disposition.html",
    "href": "tlf-disposition.html",
    "title": "5  Disposition of participants",
    "section": "",
    "text": "5.1 Overview\nClinical trials needs to track how participants flow through a study from enrollment to completion. Following ICH E3 guidance, regulatory submissions require a disposition table in Section 10.1 that summarizes:\nThis tutorial shows you how to create a regulatory-compliant disposition table using Python’s rtflite package.\nimport polars as pl # Manipulate data\nimport rtflite as rtf # Reporting in RTF format\npolars.config.Config",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Disposition of participants</span>"
    ]
  },
  {
    "objectID": "tlf-disposition.html#overview",
    "href": "tlf-disposition.html#overview",
    "title": "5  Disposition of participants",
    "section": "",
    "text": "Enrolled: Total participants who entered the study\nCompleted: Participants who finished the study protocol\nDiscontinued: Participants who left early and their reasons",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Disposition of participants</span>"
    ]
  },
  {
    "objectID": "tlf-disposition.html#step-1-load-data",
    "href": "tlf-disposition.html#step-1-load-data",
    "title": "5  Disposition of participants",
    "section": "5.2 Step 1: Load Data",
    "text": "5.2 Step 1: Load Data\nWe start by loading the Subject-level Analysis Dataset (ADSL), which contains all participant information needed for our disposition table.\nThe ADSL dataset stores participant-level information including treatment assignments and study completion status. We’re using the parquet format for data storage.\n\nadsl = pl.read_parquet(\"data/adsl.parquet\")\n\nLet’s examine the key variables we’ll use to build our disposition table:\n\nUSUBJID: Unique identifier for each participant\nTRT01P: Treatment name (text)\nTRT01PN: Treatment group (numeric code)\nDISCONFL: Flag indicating if participant discontinued (Y/N)\nDCREASCD: Specific reason for discontinuation\n\n\nadsl.select([\"USUBJID\", \"TRT01P\", \"TRT01PN\", \"DISCONFL\", \"DCREASCD\"])\n\n\nshape: (254, 5)\n\n\n\nUSUBJID\nTRT01P\nTRT01PN\nDISCONFL\nDCREASCD\n\n\nstr\nstr\ni64\nstr\nstr\n\n\n\n\n\"01-701-1015\"\n\"Placebo\"\n0\n\"\"\n\"Completed\"\n\n\n\"01-701-1023\"\n\"Placebo\"\n0\n\"Y\"\n\"Adverse Event\"\n\n\n\"01-701-1028\"\n\"Xanomeline High Dose\"\n81\n\"\"\n\"Completed\"\n\n\n…\n…\n…\n…\n…\n\n\n\"01-718-1371\"\n\"Xanomeline High Dose\"\n81\n\"Y\"\n\"Adverse Event\"\n\n\n\"01-718-1427\"\n\"Xanomeline High Dose\"\n81\n\"Y\"\n\"Lack of Efficacy\"",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Disposition of participants</span>"
    ]
  },
  {
    "objectID": "tlf-disposition.html#step-2-count-total-participants",
    "href": "tlf-disposition.html#step-2-count-total-participants",
    "title": "5  Disposition of participants",
    "section": "5.3 Step 2: Count Total Participants",
    "text": "5.3 Step 2: Count Total Participants\nFirst, we count how many participants were enrolled in each treatment group.\nWe group participants by treatment arm and count them using .group_by() and .agg(). The .pivot() operation reshapes our data from long format (rows for each treatment) to wide format (columns for each treatment), which matches the standard disposition table layout.\n\nn_rand = (\n    adsl\n    .group_by(\"TRT01PN\")\n    .agg(n = pl.len())\n    .with_columns([\n        pl.lit(\"Participants in population\").alias(\"row\"),\n        pl.lit(None, dtype=pl.Float64).alias(\"pct\") # Placeholder for percentage (not applicable for totals)\n    ])\n    .pivot(\n        index=\"row\",\n        on=\"TRT01PN\",\n        values=[\"n\", \"pct\"],\n        sort_columns=True\n    )\n)\n\nn_rand\n\n\nshape: (1, 7)\n\n\n\nrow\nn_0\nn_54\nn_81\npct_0\npct_54\npct_81\n\n\nstr\nu32\nu32\nu32\nf64\nf64\nf64\n\n\n\n\n\"Participants in population\"\n86\n84\n84\nnull\nnull\nnull",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Disposition of participants</span>"
    ]
  },
  {
    "objectID": "tlf-disposition.html#step-3-count-completed-participants",
    "href": "tlf-disposition.html#step-3-count-completed-participants",
    "title": "5  Disposition of participants",
    "section": "5.4 Step 3: Count Completed Participants",
    "text": "5.4 Step 3: Count Completed Participants\nNext, we identify participants who successfully completed the study and calculate what percentage they represent of each treatment group.\nWe filter for participants where DCREASCD == \"Completed\", then calculate both counts and percentages. The .join() operation brings in the total count for each treatment group so we can compute percentages.\n\nn_complete = (\n    adsl\n    .filter(pl.col(\"DCREASCD\") == \"Completed\")\n    .group_by(\"TRT01PN\")\n    .agg(n = pl.len())\n    .join(\n        adsl.group_by(\"TRT01PN\").agg(total = pl.len()),\n        on=\"TRT01PN\"\n    )\n    .with_columns([\n        pl.lit(\"Completed\").alias(\"row\"),\n        (100.0 * pl.col(\"n\") / pl.col(\"total\")).round(1).alias(\"pct\")\n    ])\n    .pivot(\n        index=\"row\",\n        on=\"TRT01PN\",\n        values=[\"n\", \"pct\"],\n        sort_columns=True\n    )\n)\n\nn_complete\n\n\nshape: (1, 7)\n\n\n\nrow\nn_0\nn_54\nn_81\npct_0\npct_54\npct_81\n\n\nstr\nu32\nu32\nu32\nf64\nf64\nf64\n\n\n\n\n\"Completed\"\n58\n25\n27\n67.4\n29.8\n32.1",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Disposition of participants</span>"
    ]
  },
  {
    "objectID": "tlf-disposition.html#step-4-count-discontinued-participants",
    "href": "tlf-disposition.html#step-4-count-discontinued-participants",
    "title": "5  Disposition of participants",
    "section": "5.5 Step 4: Count Discontinued Participants",
    "text": "5.5 Step 4: Count Discontinued Participants\nNow we count participants who left the study early, regardless of their specific reason.\nWe filter for participants where the discontinuation flag DISCONFL == \"Y\", then follow the same pattern of counting and calculating percentages within each treatment group.\n\nn_disc = (\n    adsl\n    .filter(pl.col(\"DISCONFL\") == \"Y\")\n    .group_by(\"TRT01PN\")\n    .agg(n = pl.len())\n    .join(\n        adsl.group_by(\"TRT01PN\").agg(total = pl.len()),\n        on=\"TRT01PN\"\n    )\n    .with_columns([\n        pl.lit(\"Discontinued\").alias(\"row\"),\n        (100.0 * pl.col(\"n\") / pl.col(\"total\")).round(1).alias(\"pct\")\n    ])\n    .pivot(\n        index=\"row\",\n        on=\"TRT01PN\",\n        values=[\"n\", \"pct\"],\n        sort_columns=True\n    )\n)\n\nn_disc\n\n\nshape: (1, 7)\n\n\n\nrow\nn_0\nn_54\nn_81\npct_0\npct_54\npct_81\n\n\nstr\nu32\nu32\nu32\nf64\nf64\nf64\n\n\n\n\n\"Discontinued\"\n28\n59\n57\n32.6\n70.2\n67.9",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Disposition of participants</span>"
    ]
  },
  {
    "objectID": "tlf-disposition.html#step-5-break-down-discontinuation-reasons",
    "href": "tlf-disposition.html#step-5-break-down-discontinuation-reasons",
    "title": "5  Disposition of participants",
    "section": "5.6 Step 5: Break Down Discontinuation Reasons",
    "text": "5.6 Step 5: Break Down Discontinuation Reasons\nFor regulatory reporting, we need to show the specific reasons why participants discontinued.\nWe filter out completed participants, then group by both treatment and discontinuation reason. The indentation (four spaces) in the row labels helps show these are subcategories under “Discontinued”. We also use .fill_null(0) to handle cases where certain discontinuation reasons don’t occur in all treatment groups.\n\nn_reason = (\n    adsl\n    .filter(pl.col(\"DCREASCD\") != \"Completed\")\n    .group_by([\"TRT01PN\", \"DCREASCD\"])\n    .agg(n = pl.len())\n    .join(\n        adsl.group_by(\"TRT01PN\").agg(total = pl.len()),\n        on=\"TRT01PN\"\n    )\n    .with_columns([\n        pl.concat_str([pl.lit(\"    \"), pl.col(\"DCREASCD\")]).alias(\"row\"),\n        (100.0 * pl.col(\"n\") / pl.col(\"total\")).round(1).alias(\"pct\")\n    ])\n    .pivot(\n        index=\"row\",\n        on=\"TRT01PN\",\n        values=[\"n\", \"pct\"],\n        sort_columns=True\n    )\n    .with_columns([\n        pl.col([\"n_0\", \"n_54\", \"n_81\"]).fill_null(0),\n        pl.col([\"pct_0\", \"pct_54\", \"pct_81\"]).fill_null(0.0)\n    ])\n    .sort(\"row\")\n)\n\nn_reason\n\n\nshape: (9, 7)\n\n\n\nrow\nn_0\nn_54\nn_81\npct_0\npct_54\npct_81\n\n\nstr\nu32\nu32\nu32\nf64\nf64\nf64\n\n\n\n\n\"    Adverse Event\"\n8\n44\n40\n9.3\n52.4\n47.6\n\n\n\"    Death\"\n2\n1\n0\n2.3\n1.2\n0.0\n\n\n\"    I/E Not Met\"\n1\n0\n2\n1.2\n0.0\n2.4\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n\"    Sponsor Decision\"\n2\n2\n3\n2.3\n2.4\n3.6\n\n\n\"    Withdrew Consent\"\n9\n10\n8\n10.5\n11.9\n9.5",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Disposition of participants</span>"
    ]
  },
  {
    "objectID": "tlf-disposition.html#step-6-combine-all-results",
    "href": "tlf-disposition.html#step-6-combine-all-results",
    "title": "5  Disposition of participants",
    "section": "5.7 Step 6: Combine All Results",
    "text": "5.7 Step 6: Combine All Results\nNow we stack all our individual summaries together to create the complete disposition table.\nUsing pl.concat(), we combine the enrollment counts, completion counts, discontinuation counts, and detailed discontinuation reasons into a single table that flows logically from top to bottom.\n\ntbl_disp = pl.concat([\n    n_rand,\n    n_complete,\n    n_disc,\n    n_reason\n])\n\ntbl_disp\n\n\nshape: (12, 7)\n\n\n\nrow\nn_0\nn_54\nn_81\npct_0\npct_54\npct_81\n\n\nstr\nu32\nu32\nu32\nf64\nf64\nf64\n\n\n\n\n\"Participants in population\"\n86\n84\n84\nnull\nnull\nnull\n\n\n\"Completed\"\n58\n25\n27\n67.4\n29.8\n32.1\n\n\n\"Discontinued\"\n28\n59\n57\n32.6\n70.2\n67.9\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n\"    Sponsor Decision\"\n2\n2\n3\n2.3\n2.4\n3.6\n\n\n\"    Withdrew Consent\"\n9\n10\n8\n10.5\n11.9\n9.5",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Disposition of participants</span>"
    ]
  },
  {
    "objectID": "tlf-disposition.html#step-7-generate-publication-ready-output",
    "href": "tlf-disposition.html#step-7-generate-publication-ready-output",
    "title": "5  Disposition of participants",
    "section": "5.8 Step 7: Generate Publication-Ready Output",
    "text": "5.8 Step 7: Generate Publication-Ready Output\nFinally, we format our table in RTF format using the rtflite package.\nThe RTFDocument class handles the complex formatting required for clinical reports, including proper column headers, borders, and spacing. The resulting RTF file can be directly included in regulatory submissions or converted to PDF for review.\n\ndoc_disp = rtf.RTFDocument(\n    df=tbl_disp.select(\"row\", \"n_0\", \"pct_0\", \"n_54\", \"pct_54\", \"n_81\", \"pct_81\"),\n    rtf_title=rtf.RTFTitle(text=[\"Disposition of Participants\"]),\n    rtf_column_header=[\n        rtf.RTFColumnHeader(\n            text=[\"\", \"Placebo\", \"Xanomeline Low Dose\", \"Xanomeline High Dose\"],\n            col_rel_width=[3] + [2] * 3,\n            text_justification=[\"l\"] + [\"c\"] * 3,\n        ),\n        rtf.RTFColumnHeader(\n            text=[\"\", \"n\", \"(%)\", \"n\", \"(%)\", \"n\", \"(%)\"],\n            col_rel_width=[3] + [1] * 6,\n            text_justification=[\"l\"] + [\"c\"] * 6,\n            border_top=[\"\"] + [\"single\"] * 6,\n            border_left=[\"single\"] + [\"single\", \"\"] * 3\n        )\n    ],\n    rtf_body=rtf.RTFBody(\n        col_rel_width=[3] + [1] * 6,\n        text_justification=[\"l\"] + [\"c\"] * 6,\n        border_left=[\"single\"] + [\"single\", \"\"] * 3\n    ),\n    rtf_source=rtf.RTFSource(text=[\"Source: ADSL dataset\"]) # Required source attribution\n)\n\ndoc_disp.write_rtf(\"rtf/tlf_disposition.rtf\")  # Save as RTF for submission\n\nrtf/tlf_disposition.rtf\n\n\n\n\nPosixPath('pdf/tlf_disposition.pdf')",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Disposition of participants</span>"
    ]
  },
  {
    "objectID": "tlf-population.html",
    "href": "tlf-population.html",
    "title": "6  Study population",
    "section": "",
    "text": "6.1 Overview\nClinical trials define multiple analysis populations based on different inclusion criteria. Following ICH E3 guidance, regulatory submissions must clearly document the number of participants in each analysis population to support the validity of statistical analyses.\nThe key analysis populations typically include:\nThis tutorial shows you how to create a population summary table using Python’s rtflite package.\nimport polars as pl # Data manipulation\nimport rtflite as rtf # RTF reporting\npolars.config.Config",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Study population</span>"
    ]
  },
  {
    "objectID": "tlf-population.html#overview",
    "href": "tlf-population.html#overview",
    "title": "6  Study population",
    "section": "",
    "text": "All Randomized: Total participants who entered the study\nIntent-to-Treat (ITT): Participants included in the primary efficacy analysis\nEfficacy Population: Participants who meet specific criteria for efficacy evaluation\nSafety Population: Participants who received at least one dose of study treatment",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Study population</span>"
    ]
  },
  {
    "objectID": "tlf-population.html#step-1-load-data",
    "href": "tlf-population.html#step-1-load-data",
    "title": "6  Study population",
    "section": "6.2 Step 1: Load Data",
    "text": "6.2 Step 1: Load Data\nWe start by loading the Subject-level Analysis Dataset (ADSL), which contains population flags for each participant.\n\nadsl = pl.read_parquet(\"data/adsl.parquet\")\n\nLet’s examine the key population flag variables we’ll use:\n\nUSUBJID: Unique participant identifier\nTRT01P: Planned treatment group\nITTFL: Intent-to-treat population flag (Y/N)\nEFFFL: Efficacy population flag (Y/N)\nSAFFL: Safety population flag (Y/N)\n\n\nadsl.select([\"USUBJID\", \"TRT01P\", \"ITTFL\", \"EFFFL\", \"SAFFL\"])\n\n\nshape: (254, 5)\n\n\n\nUSUBJID\nTRT01P\nITTFL\nEFFFL\nSAFFL\n\n\nstr\nstr\nstr\nstr\nstr\n\n\n\n\n\"01-701-1015\"\n\"Placebo\"\n\"Y\"\n\"Y\"\n\"Y\"\n\n\n\"01-701-1023\"\n\"Placebo\"\n\"Y\"\n\"Y\"\n\"Y\"\n\n\n\"01-701-1028\"\n\"Xanomeline High Dose\"\n\"Y\"\n\"Y\"\n\"Y\"\n\n\n…\n…\n…\n…\n…\n\n\n\"01-718-1371\"\n\"Xanomeline High Dose\"\n\"Y\"\n\"Y\"\n\"Y\"\n\n\n\"01-718-1427\"\n\"Xanomeline High Dose\"\n\"Y\"\n\"Y\"\n\"Y\"",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Study population</span>"
    ]
  },
  {
    "objectID": "tlf-population.html#step-2-calculate-treatment-group-totals",
    "href": "tlf-population.html#step-2-calculate-treatment-group-totals",
    "title": "6  Study population",
    "section": "6.3 Step 2: Calculate Treatment Group Totals",
    "text": "6.3 Step 2: Calculate Treatment Group Totals\nFirst, we calculate the total number of randomized participants in each treatment group, which will serve as the denominator for percentage calculations.\n\ntotals = adsl.group_by(\"TRT01P\").agg(\n    total = pl.len()\n)\n\ntotals\n\n\nshape: (3, 2)\n\n\n\nTRT01P\ntotal\n\n\nstr\nu32\n\n\n\n\n\"Xanomeline Low Dose\"\n84\n\n\n\"Xanomeline High Dose\"\n84\n\n\n\"Placebo\"\n86",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Study population</span>"
    ]
  },
  {
    "objectID": "tlf-population.html#step-3-define-helper-function",
    "href": "tlf-population.html#step-3-define-helper-function",
    "title": "6  Study population",
    "section": "6.4 Step 3: Define Helper Function",
    "text": "6.4 Step 3: Define Helper Function\nWe create a reusable function to count participants by treatment group for any population subset.\n\ndef count_by_treatment(data, population_name):\n    \"\"\"Count participants by treatment group and add population label\"\"\"\n    return data.group_by(\"TRT01P\").agg(\n        n = pl.len()\n    ).with_columns(\n        population = pl.lit(population_name)\n    )",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Study population</span>"
    ]
  },
  {
    "objectID": "tlf-population.html#step-4-count-each-population",
    "href": "tlf-population.html#step-4-count-each-population",
    "title": "6  Study population",
    "section": "6.5 Step 4: Count Each Population",
    "text": "6.5 Step 4: Count Each Population\nNow we calculate participant counts for each analysis population.\n\n6.5.1 All Randomized Participants\n\npop_all = count_by_treatment(\n    data=adsl,\n    population_name=\"Participants in population\"\n)\n\npop_all\n\n\nshape: (3, 3)\n\n\n\nTRT01P\nn\npopulation\n\n\nstr\nu32\nstr\n\n\n\n\n\"Placebo\"\n86\n\"Participants in population\"\n\n\n\"Xanomeline High Dose\"\n84\n\"Participants in population\"\n\n\n\"Xanomeline Low Dose\"\n84\n\"Participants in population\"\n\n\n\n\n\n\n\n\n6.5.2 Intent-to-Treat Population\n\nadsl_itt = adsl.filter(pl.col(\"ITTFL\") == \"Y\")\npop_itt = count_by_treatment(\n    data=adsl_itt,\n    population_name=\"Participants included in ITT population\"\n)\n\npop_itt\n\n\nshape: (3, 3)\n\n\n\nTRT01P\nn\npopulation\n\n\nstr\nu32\nstr\n\n\n\n\n\"Placebo\"\n86\n\"Participants included in ITT p…\n\n\n\"Xanomeline High Dose\"\n84\n\"Participants included in ITT p…\n\n\n\"Xanomeline Low Dose\"\n84\n\"Participants included in ITT p…\n\n\n\n\n\n\n\n\n6.5.3 Efficacy Population\n\nadsl_eff = adsl.filter(pl.col(\"EFFFL\") == \"Y\")\npop_eff = count_by_treatment(\n    data=adsl_eff,\n    population_name=\"Participants included in efficacy population\"\n)\n\npop_eff\n\n\nshape: (3, 3)\n\n\n\nTRT01P\nn\npopulation\n\n\nstr\nu32\nstr\n\n\n\n\n\"Placebo\"\n79\n\"Participants included in effic…\n\n\n\"Xanomeline Low Dose\"\n81\n\"Participants included in effic…\n\n\n\"Xanomeline High Dose\"\n74\n\"Participants included in effic…\n\n\n\n\n\n\n\n\n6.5.4 Safety Population\n\nadsl_saf = adsl.filter(pl.col(\"SAFFL\") == \"Y\")\npop_saf = count_by_treatment(\n    data=adsl_saf,\n    population_name=\"Participants included in safety population\"\n)\n\npop_saf\n\n\nshape: (3, 3)\n\n\n\nTRT01P\nn\npopulation\n\n\nstr\nu32\nstr\n\n\n\n\n\"Placebo\"\n86\n\"Participants included in safet…\n\n\n\"Xanomeline Low Dose\"\n84\n\"Participants included in safet…\n\n\n\"Xanomeline High Dose\"\n84\n\"Participants included in safet…",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Study population</span>"
    ]
  },
  {
    "objectID": "tlf-population.html#step-5-combine-all-populations",
    "href": "tlf-population.html#step-5-combine-all-populations",
    "title": "6  Study population",
    "section": "6.6 Step 5: Combine All Populations",
    "text": "6.6 Step 5: Combine All Populations\nWe stack all population counts together into a single dataset.\n\nall_populations = pl.concat([\n    pop_all,\n    pop_itt,\n    pop_eff,\n    pop_saf\n])\n\nall_populations\n\n\nshape: (12, 3)\n\n\n\nTRT01P\nn\npopulation\n\n\nstr\nu32\nstr\n\n\n\n\n\"Placebo\"\n86\n\"Participants in population\"\n\n\n\"Xanomeline High Dose\"\n84\n\"Participants in population\"\n\n\n\"Xanomeline Low Dose\"\n84\n\"Participants in population\"\n\n\n…\n…\n…\n\n\n\"Xanomeline Low Dose\"\n84\n\"Participants included in safet…\n\n\n\"Xanomeline High Dose\"\n84\n\"Participants included in safet…",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Study population</span>"
    ]
  },
  {
    "objectID": "tlf-population.html#step-6-calculate-percentages",
    "href": "tlf-population.html#step-6-calculate-percentages",
    "title": "6  Study population",
    "section": "6.7 Step 6: Calculate Percentages",
    "text": "6.7 Step 6: Calculate Percentages\nWe join with the total counts and calculate what percentage each population represents of the total randomized participants.\n\nstats_with_pct = all_populations.join(\n    totals,\n    on=\"TRT01P\"\n).with_columns(\n    pct = (100.0 * pl.col(\"n\") / pl.col(\"total\")).round(1)\n)\n\nstats_with_pct\n\n\nshape: (12, 5)\n\n\n\nTRT01P\nn\npopulation\ntotal\npct\n\n\nstr\nu32\nstr\nu32\nf64\n\n\n\n\n\"Placebo\"\n86\n\"Participants in population\"\n86\n100.0\n\n\n\"Xanomeline High Dose\"\n84\n\"Participants in population\"\n84\n100.0\n\n\n\"Xanomeline Low Dose\"\n84\n\"Participants in population\"\n84\n100.0\n\n\n…\n…\n…\n…\n…\n\n\n\"Xanomeline Low Dose\"\n84\n\"Participants included in safet…\n84\n100.0\n\n\n\"Xanomeline High Dose\"\n84\n\"Participants included in safet…\n84\n100.0",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Study population</span>"
    ]
  },
  {
    "objectID": "tlf-population.html#step-7-format-display-values",
    "href": "tlf-population.html#step-7-format-display-values",
    "title": "6  Study population",
    "section": "6.8 Step 7: Format Display Values",
    "text": "6.8 Step 7: Format Display Values\nFor the final table, we format the display text. The total randomized count shows just “N”, while subset populations show “N (%)”.\n\nformatted_stats = stats_with_pct.with_columns(\n    display = pl.when(pl.col(\"population\") == \"Participants in population\")\n        .then(pl.col(\"n\").cast(str)) \n        .otherwise(\n            pl.concat_str([ \n                pl.col(\"n\").cast(str),\n                pl.lit(\" (\"),\n                pl.col(\"pct\").round(1).cast(str),\n                pl.lit(\")\")\n            ])\n        )\n)\n\nformatted_stats\n\n\nshape: (12, 6)\n\n\n\nTRT01P\nn\npopulation\ntotal\npct\ndisplay\n\n\nstr\nu32\nstr\nu32\nf64\nstr\n\n\n\n\n\"Placebo\"\n86\n\"Participants in population\"\n86\n100.0\n\"86\"\n\n\n\"Xanomeline High Dose\"\n84\n\"Participants in population\"\n84\n100.0\n\"84\"\n\n\n\"Xanomeline Low Dose\"\n84\n\"Participants in population\"\n84\n100.0\n\"84\"\n\n\n…\n…\n…\n…\n…\n…\n\n\n\"Xanomeline Low Dose\"\n84\n\"Participants included in safet…\n84\n100.0\n\"84 (100.0)\"\n\n\n\"Xanomeline High Dose\"\n84\n\"Participants included in safet…\n84\n100.0\n\"84 (100.0)\"",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Study population</span>"
    ]
  },
  {
    "objectID": "tlf-population.html#step-8-create-final-table",
    "href": "tlf-population.html#step-8-create-final-table",
    "title": "6  Study population",
    "section": "6.9 Step 8: Create Final Table",
    "text": "6.9 Step 8: Create Final Table\nWe reshape the data from long format (rows for each treatment-population combination) to wide format (columns for each treatment group).\n\ndf_overview = formatted_stats.pivot(\n    values=\"display\",\n    index=\"population\",\n    on=\"TRT01P\",\n    maintain_order=True\n).select(\n    [\"population\", \"Placebo\", \"Xanomeline Low Dose\", \"Xanomeline High Dose\"]\n)\n\ndf_overview\n\n\nshape: (4, 4)\n\n\n\npopulation\nPlacebo\nXanomeline Low Dose\nXanomeline High Dose\n\n\nstr\nstr\nstr\nstr\n\n\n\n\n\"Participants in population\"\n\"86\"\n\"84\"\n\"84\"\n\n\n\"Participants included in ITT p…\n\"86 (100.0)\"\n\"84 (100.0)\"\n\"84 (100.0)\"\n\n\n\"Participants included in effic…\n\"79 (91.9)\"\n\"81 (96.4)\"\n\"74 (88.1)\"\n\n\n\"Participants included in safet…\n\"86 (100.0)\"\n\"84 (100.0)\"\n\"84 (100.0)\"",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Study population</span>"
    ]
  },
  {
    "objectID": "tlf-population.html#step-9-generate-publication-ready-output",
    "href": "tlf-population.html#step-9-generate-publication-ready-output",
    "title": "6  Study population",
    "section": "6.10 Step 9: Generate Publication-Ready Output",
    "text": "6.10 Step 9: Generate Publication-Ready Output\nFinally, we format the population table for regulatory submission using the rtflite package.\n\ndoc_overview = rtf.RTFDocument(\n    df=df_overview,\n    rtf_title=rtf.RTFTitle(\n        text=[\"Analysis Population\", \"All Participants Randomized\"]\n    ),\n    rtf_column_header=rtf.RTFColumnHeader(\n        text=[\"\", \"Placebo\\nn (%)\", \"Xanomeline Low Dose\\nn (%)\", \"Xanomeline High Dose\\nn (%)\"],\n        col_rel_width=[4, 2, 2, 2],\n        text_justification=[\"l\", \"c\", \"c\", \"c\"],\n    ),\n    rtf_body=rtf.RTFBody(\n        col_rel_width=[4, 2, 2, 2],\n        text_justification=[\"l\", \"c\", \"c\", \"c\"],\n    ),\n    rtf_source=rtf.RTFSource(text=[\"Source: ADSL dataset\"])\n)\n\ndoc_overview.write_rtf(\"rtf/tlf_population.rtf\")\n\nrtf/tlf_population.rtf\n\n\n\n\nPosixPath('pdf/tlf_population.pdf')",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Study population</span>"
    ]
  },
  {
    "objectID": "tlf-baseline.html",
    "href": "tlf-baseline.html",
    "title": "7  Baseline characteristics",
    "section": "",
    "text": "7.1 Overview\nBaseline characteristics tables summarize the demographic and clinical characteristics of study participants at enrollment. Following ICH E3 guidance, these tables are essential for understanding the study population and assessing comparability between treatment groups.\nThis tutorial shows you how to create a baseline characteristics table using Python’s rtflite package.\nimport polars as pl # Data manipulation\nimport rtflite as rtf # RTF reporting\npolars.config.Config",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Baseline characteristics</span>"
    ]
  },
  {
    "objectID": "tlf-baseline.html#step-1-load-data",
    "href": "tlf-baseline.html#step-1-load-data",
    "title": "7  Baseline characteristics",
    "section": "7.2 Step 1: Load Data",
    "text": "7.2 Step 1: Load Data\nWe start by loading the Subject-level Analysis Dataset (ADSL) and filtering to the safety population.\n\nadsl = (\n    pl.read_parquet(\"data/adsl.parquet\")\n    .select([\"USUBJID\", \"TRT01P\", \"AGE\", \"SEX\", \"RACE\"])\n)\n\nadsl\n\n\nshape: (254, 5)\n\n\n\nUSUBJID\nTRT01P\nAGE\nSEX\nRACE\n\n\nstr\nstr\nf64\nstr\nstr\n\n\n\n\n\"01-701-1015\"\n\"Placebo\"\n63.0\n\"Female\"\n\"White\"\n\n\n\"01-701-1023\"\n\"Placebo\"\n64.0\n\"Male\"\n\"White\"\n\n\n\"01-701-1028\"\n\"Xanomeline High Dose\"\n71.0\n\"Male\"\n\"White\"\n\n\n…\n…\n…\n…\n…\n\n\n\"01-718-1371\"\n\"Xanomeline High Dose\"\n69.0\n\"Female\"\n\"White\"\n\n\n\"01-718-1427\"\n\"Xanomeline High Dose\"\n74.0\n\"Female\"\n\"Black Or African American\"",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Baseline characteristics</span>"
    ]
  },
  {
    "objectID": "tlf-baseline.html#step-2-calculate-summary-statistics",
    "href": "tlf-baseline.html#step-2-calculate-summary-statistics",
    "title": "7  Baseline characteristics",
    "section": "7.3 Step 2: Calculate Summary Statistics",
    "text": "7.3 Step 2: Calculate Summary Statistics\nWe’ll create separate functions to handle continuous and categorical variables.\n\n7.3.1 Continuous Variables (Age)\nFor continuous variables, we calculate mean (SD) and median [min, max].\n\ndef summarize_continuous(df, var):\n    \"\"\"Calculate summary statistics for continuous variables\"\"\"\n    return df.group_by(\"TRT01P\").agg([\n        pl.col(var).mean().round(1).alias(\"mean\"),\n        pl.col(var).std().round(2).alias(\"sd\"),\n        pl.col(var).median().alias(\"median\"),\n        pl.col(var).min().alias(\"min\"),\n        pl.col(var).max().alias(\"max\"),\n        pl.len().alias(\"n\")\n    ])\n\nage_stats = summarize_continuous(adsl, \"AGE\")\nage_stats\n\n\nshape: (3, 7)\n\n\n\nTRT01P\nmean\nsd\nmedian\nmin\nmax\nn\n\n\nstr\nf64\nf64\nf64\nf64\nf64\nu32\n\n\n\n\n\"Xanomeline Low Dose\"\n75.7\n8.29\n77.5\n51.0\n88.0\n84\n\n\n\"Xanomeline High Dose\"\n74.4\n7.89\n76.0\n56.0\n88.0\n84\n\n\n\"Placebo\"\n75.2\n8.59\n76.0\n52.0\n89.0\n86\n\n\n\n\n\n\n\n\n7.3.2 Categorical Variables (Sex, Race)\nFor categorical variables, we calculate counts and percentages.\n\ndef summarize_categorical(df, var):\n    \"\"\"Calculate counts and percentages for categorical variables\"\"\"\n    # Get counts by treatment and category\n    counts = df.group_by([\"TRT01P\", var]).len()\n\n    # Get treatment totals for percentage calculations\n    totals = df.group_by(\"TRT01P\").len().rename({\"len\": \"total\"})\n\n    # Calculate percentages\n    result = counts.join(totals, on=\"TRT01P\").with_columns([\n        (100.0 * pl.col(\"len\") / pl.col(\"total\")).round(1).alias(\"pct\")\n    ])\n\n    return result\n\nsex_stats = summarize_categorical(adsl, \"SEX\")\nsex_stats\n\n\nshape: (6, 5)\n\n\n\nTRT01P\nSEX\nlen\ntotal\npct\n\n\nstr\nstr\nu32\nu32\nf64\n\n\n\n\n\"Placebo\"\n\"Female\"\n53\n86\n61.6\n\n\n\"Placebo\"\n\"Male\"\n33\n86\n38.4\n\n\n\"Xanomeline High Dose\"\n\"Female\"\n40\n84\n47.6\n\n\n…\n…\n…\n…\n…\n\n\n\"Xanomeline Low Dose\"\n\"Male\"\n34\n84\n40.5\n\n\n\"Xanomeline Low Dose\"\n\"Female\"\n50\n84\n59.5\n\n\n\n\n\n\n\nrace_stats = summarize_categorical(adsl, \"RACE\")\nrace_stats\n\n\nshape: (7, 5)\n\n\n\nTRT01P\nRACE\nlen\ntotal\npct\n\n\nstr\nstr\nu32\nu32\nf64\n\n\n\n\n\"Xanomeline Low Dose\"\n\"Black Or African American\"\n6\n84\n7.1\n\n\n\"Xanomeline Low Dose\"\n\"White\"\n78\n84\n92.9\n\n\n\"Xanomeline High Dose\"\n\"White\"\n74\n84\n88.1\n\n\n…\n…\n…\n…\n…\n\n\n\"Xanomeline High Dose\"\n\"Black Or African American\"\n9\n84\n10.7\n\n\n\"Placebo\"\n\"White\"\n78\n86\n90.7",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Baseline characteristics</span>"
    ]
  },
  {
    "objectID": "tlf-baseline.html#step-3-format-results",
    "href": "tlf-baseline.html#step-3-format-results",
    "title": "7  Baseline characteristics",
    "section": "7.4 Step 3: Format Results",
    "text": "7.4 Step 3: Format Results\nNow we format the statistics into the standard baseline table format.\n\n7.4.1 Format Age Statistics\n\n# Format age as \"Mean (SD)\" and \"Median [Min, Max]\"\nage_formatted = age_stats.with_columns([\n    pl.format(\"{} ({})\", pl.col(\"mean\"), pl.col(\"sd\")).alias(\"mean_sd\"),\n    pl.format(\"{} [{}, {}]\", pl.col(\"median\"), pl.col(\"min\"), pl.col(\"max\")).alias(\"median_range\")\n]).select([\"TRT01P\", \"mean_sd\", \"median_range\"])\n\nage_formatted\n\n\nshape: (3, 3)\n\n\n\nTRT01P\nmean_sd\nmedian_range\n\n\nstr\nstr\nstr\n\n\n\n\n\"Xanomeline Low Dose\"\n\"75.7 (8.29)\"\n\"77.5 [51.0, 88.0]\"\n\n\n\"Xanomeline High Dose\"\n\"74.4 (7.89)\"\n\"76.0 [56.0, 88.0]\"\n\n\n\"Placebo\"\n\"75.2 (8.59)\"\n\"76.0 [52.0, 89.0]\"\n\n\n\n\n\n\n\n\n7.4.2 Format Categorical Statistics\n\n# Format categorical as \"n (%)\"\nsex_formatted = sex_stats.with_columns(\n    pl.format(\"{} ({}%)\", pl.col(\"len\"), pl.col(\"pct\")).alias(\"n_pct\")\n).select([\"TRT01P\", \"SEX\", \"n_pct\"])\n\nrace_formatted = race_stats.with_columns(\n    pl.format(\"{} ({}%)\", pl.col(\"len\"), pl.col(\"pct\")).alias(\"n_pct\")\n).select([\"TRT01P\", \"RACE\", \"n_pct\"])\n\nsex_formatted\n\n\nshape: (6, 3)\n\n\n\nTRT01P\nSEX\nn_pct\n\n\nstr\nstr\nstr\n\n\n\n\n\"Placebo\"\n\"Female\"\n\"53 (61.6%)\"\n\n\n\"Placebo\"\n\"Male\"\n\"33 (38.4%)\"\n\n\n\"Xanomeline High Dose\"\n\"Female\"\n\"40 (47.6%)\"\n\n\n…\n…\n…\n\n\n\"Xanomeline Low Dose\"\n\"Male\"\n\"34 (40.5%)\"\n\n\n\"Xanomeline Low Dose\"\n\"Female\"\n\"50 (59.5%)\"",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Baseline characteristics</span>"
    ]
  },
  {
    "objectID": "tlf-baseline.html#step-4-create-table-structure",
    "href": "tlf-baseline.html#step-4-create-table-structure",
    "title": "7  Baseline characteristics",
    "section": "7.5 Step 4: Create Table Structure",
    "text": "7.5 Step 4: Create Table Structure\nWe’ll build the table row by row following the standard baseline table format.\n\n# Helper function to get value for a treatment group\ndef get_value(df, treatment):\n    \"\"\"Get value for a specific treatment group or return default\"\"\"\n    result = df.filter(pl.col(\"TRT01P\") == treatment)\n    return result[result.columns[-1]][0] if result.height &gt; 0 else \"0 (0.0%)\"\n\n# Build the baseline table structure\ntable_rows = []\n\n# Age section\ntable_rows.append([\"Age (years)\", \"\", \"\", \"\"])\n\n# Age Mean (SD) row\nage_mean_row = [\"  Mean (SD)\"] + [\n    get_value(age_formatted.select([\"TRT01P\", \"mean_sd\"]), trt).replace(\"0 (0.0%)\", \"\")\n    for trt in [\"Placebo\", \"Xanomeline Low Dose\", \"Xanomeline High Dose\"]\n]\ntable_rows.append(age_mean_row)\n\n# Age Median [Min, Max] row\nage_median_row = [\"  Median [Min, Max]\"] + [\n    get_value(age_formatted.select([\"TRT01P\", \"median_range\"]), trt).replace(\"0 (0.0%)\", \"\")\n    for trt in [\"Placebo\", \"Xanomeline Low Dose\", \"Xanomeline High Dose\"]\n]\ntable_rows.append(age_median_row)\n\n# Sex section\ntable_rows.append([\"Sex\", \"\", \"\", \"\"])\n\nfor sex_cat in [\"Female\", \"Male\"]:\n    sex_data = sex_formatted.filter(pl.col(\"SEX\") == sex_cat)\n    sex_row = [f\"  {sex_cat}\"] + [\n        get_value(sex_data, trt)\n        for trt in [\"Placebo\", \"Xanomeline Low Dose\", \"Xanomeline High Dose\"]\n    ]\n    table_rows.append(sex_row)\n\n# Race section\ntable_rows.append([\"Race\", \"\", \"\", \"\"])\n\nfor race_cat in [\"White\", \"Black Or African American\", \"American Indian Or Alaska Native\"]:\n    race_data = race_formatted.filter(pl.col(\"RACE\") == race_cat)\n    race_row = [f\"  {race_cat}\"] + [\n        get_value(race_data, trt)\n        for trt in [\"Placebo\", \"Xanomeline Low Dose\", \"Xanomeline High Dose\"]\n    ]\n    table_rows.append(race_row)\n\n# Create DataFrame from table rows\nbaseline_table = pl.DataFrame(\n    table_rows,\n    schema=[\"Characteristic\", \"Placebo\", \"Xanomeline Low Dose\", \"Xanomeline High Dose\"],\n    orient=\"row\"\n)\n\nbaseline_table\n\n\nshape: (10, 4)\n\n\n\nCharacteristic\nPlacebo\nXanomeline Low Dose\nXanomeline High Dose\n\n\nstr\nstr\nstr\nstr\n\n\n\n\n\"Age (years)\"\n\"\"\n\"\"\n\"\"\n\n\n\"  Mean (SD)\"\n\"75.2 (8.59)\"\n\"75.7 (8.29)\"\n\"74.4 (7.89)\"\n\n\n\"  Median [Min, Max]\"\n\"76.0 [52.0, 89.0]\"\n\"77.5 [51.0, 88.0]\"\n\"76.0 [56.0, 88.0]\"\n\n\n…\n…\n…\n…\n\n\n\"  Black Or African American\"\n\"8 (9.3%)\"\n\"6 (7.1%)\"\n\"9 (10.7%)\"\n\n\n\"  American Indian Or Alaska Na…\n\"0 (0.0%)\"\n\"0 (0.0%)\"\n\"1 (1.2%)\"",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Baseline characteristics</span>"
    ]
  },
  {
    "objectID": "tlf-baseline.html#step-5-generate-publication-ready-output",
    "href": "tlf-baseline.html#step-5-generate-publication-ready-output",
    "title": "7  Baseline characteristics",
    "section": "7.6 Step 5: Generate Publication-Ready Output",
    "text": "7.6 Step 5: Generate Publication-Ready Output\nFinally, we format the baseline table for regulatory submission using the rtflite package.\n\n# Get treatment group sizes for column headers\ntreatment_n = adsl.group_by(\"TRT01P\").len().sort(\"TRT01P\")\nn_placebo = treatment_n.filter(pl.col(\"TRT01P\") == \"Placebo\")[\"len\"][0]\nn_low = treatment_n.filter(pl.col(\"TRT01P\") == \"Xanomeline Low Dose\")[\"len\"][0]\nn_high = treatment_n.filter(pl.col(\"TRT01P\") == \"Xanomeline High Dose\")[\"len\"][0]\n\ndoc_baseline = rtf.RTFDocument(\n    df=baseline_table,\n    rtf_title=rtf.RTFTitle(\n        text=[\n            \"Baseline Characteristics of Participants\", \n            \"(All Participants Randomized)\"\n        ]\n    ),\n    rtf_column_header=rtf.RTFColumnHeader(\n        text=[\n            \"Characteristic\",\n            f\"Placebo\\n(N={n_placebo})\",\n            f\"Xanomeline Low Dose\\n(N={n_low})\",\n            f\"Xanomeline High Dose\\n(N={n_high})\"\n        ],\n        text_justification=[\"l\", \"c\", \"c\", \"c\"],\n        col_rel_width=[3, 2, 2, 2]\n    ),\n    rtf_body=rtf.RTFBody(\n        text_justification=[\"l\", \"c\", \"c\", \"c\"],\n        col_rel_width=[3, 2, 2, 2]\n    ),\n    rtf_source=rtf.RTFSource(text=[\"Source: ADSL dataset\"])\n)\n\ndoc_baseline.write_rtf(\"rtf/tlf_baseline.rtf\") # Save as RTF for submission\n\nrtf/tlf_baseline.rtf\n\n\n\n\nPosixPath('pdf/tlf_baseline.pdf')",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Baseline characteristics</span>"
    ]
  },
  {
    "objectID": "tlf-ae-summary.html",
    "href": "tlf-ae-summary.html",
    "title": "8  Adverse events summary",
    "section": "",
    "text": "8.1 Overview\nAdverse events (AE) summary tables are critical safety assessments required in clinical study reports. Following ICH E3 guidance, these tables summarize the overall safety profile by showing the number and percentage of participants experiencing various categories of adverse events across treatment groups.\nKey categories typically include:\nThis tutorial shows you how to create an AE summary table using Python’s rtflite package.\nimport polars as pl\nimport rtflite as rtf\npolars.config.Config",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Adverse events summary</span>"
    ]
  },
  {
    "objectID": "tlf-ae-summary.html#overview",
    "href": "tlf-ae-summary.html#overview",
    "title": "8  Adverse events summary",
    "section": "",
    "text": "Any adverse event: Total participants with at least one AE\nDrug-related events: Events potentially related to study treatment\nSerious adverse events: Events meeting regulatory criteria for seriousness\nDeaths: Fatal outcomes\nDiscontinuations: Participants who stopped treatment due to AEs",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Adverse events summary</span>"
    ]
  },
  {
    "objectID": "tlf-ae-summary.html#step-1-load-data",
    "href": "tlf-ae-summary.html#step-1-load-data",
    "title": "8  Adverse events summary",
    "section": "8.2 Step 1: Load Data",
    "text": "8.2 Step 1: Load Data\nWe need two datasets for AE analysis: the subject-level dataset (ADSL) and the adverse events dataset (ADAE).\n\n# Load datasets\nadsl = pl.read_parquet(\"data/adsl.parquet\")\nadae = pl.read_parquet(\"data/adae.parquet\")\n\n# Display key variables from ADSL\nadsl.select([\"USUBJID\", \"TRT01A\", \"SAFFL\"])\n\n\nshape: (254, 3)\n\n\n\nUSUBJID\nTRT01A\nSAFFL\n\n\nstr\nstr\nstr\n\n\n\n\n\"01-701-1015\"\n\"Placebo\"\n\"Y\"\n\n\n\"01-701-1023\"\n\"Placebo\"\n\"Y\"\n\n\n\"01-701-1028\"\n\"Xanomeline High Dose\"\n\"Y\"\n\n\n…\n…\n…\n\n\n\"01-718-1371\"\n\"Xanomeline High Dose\"\n\"Y\"\n\n\n\"01-718-1427\"\n\"Xanomeline High Dose\"\n\"Y\"\n\n\n\n\n\n\n\n# Display key variables from ADAE\nadae.select([\"USUBJID\", \"AEREL\", \"AESER\", \"AEOUT\", \"AEACN\"])\n\n\nshape: (1_191, 5)\n\n\n\nUSUBJID\nAEREL\nAESER\nAEOUT\nAEACN\n\n\nstr\nstr\nstr\nstr\nstr\n\n\n\n\n\"01-701-1015\"\n\"PROBABLE\"\n\"N\"\n\"NOT RECOVERED/NOT RESOLVED\"\n\"\"\n\n\n\"01-701-1015\"\n\"PROBABLE\"\n\"N\"\n\"NOT RECOVERED/NOT RESOLVED\"\n\"\"\n\n\n\"01-701-1015\"\n\"REMOTE\"\n\"N\"\n\"RECOVERED/RESOLVED\"\n\"\"\n\n\n…\n…\n…\n…\n…\n\n\n\"01-718-1427\"\n\"POSSIBLE\"\n\"N\"\n\"RECOVERED/RESOLVED\"\n\"\"\n\n\n\"01-718-1427\"\n\"POSSIBLE\"\n\"N\"\n\"RECOVERED/RESOLVED\"\n\"\"\n\n\n\n\n\n\nKey ADAE variables used in this analysis:\n\nUSUBJID: Unique subject identifier to link with ADSL\nAEREL: Relationship of adverse event to study drug (e.g., “RELATED”, “POSSIBLE”, “PROBABLE”, “DEFINITE”, “NOT RELATED”)\nAESER: Serious adverse event flag (“Y” = serious, “N” = not serious)\nAEOUT: Outcome of adverse event (e.g., “RECOVERED”, “RECOVERING”, “NOT RECOVERED”, “FATAL”)\nAEACN: Action taken with study treatment (e.g., “DOSE NOT CHANGED”, “DRUG WITHDRAWN”, “DOSE REDUCED”)",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Adverse events summary</span>"
    ]
  },
  {
    "objectID": "tlf-ae-summary.html#step-2-filter-safety-population",
    "href": "tlf-ae-summary.html#step-2-filter-safety-population",
    "title": "8  Adverse events summary",
    "section": "8.3 Step 2: Filter Safety Population",
    "text": "8.3 Step 2: Filter Safety Population\nFor safety analyses, we focus on participants who received at least one dose of study treatment.\n\n# Filter to safety population\nadsl_safety = adsl.filter(pl.col(\"SAFFL\") == \"Y\").select([\"USUBJID\", \"TRT01A\"])\n\n# Get treatment counts for denominators\npop_counts = adsl_safety.group_by(\"TRT01A\").agg(\n    N = pl.len()\n).sort(\"TRT01A\")\n\n# Preserve the treatment level order for downstream joins\ntreatment_levels = pop_counts.select([\"TRT01A\"])\n\n# Safety population by treatment\npop_counts\n\n\nshape: (3, 2)\n\n\n\nTRT01A\nN\n\n\nstr\nu32\n\n\n\n\n\"Placebo\"\n86\n\n\n\"Xanomeline High Dose\"\n84\n\n\n\"Xanomeline Low Dose\"\n84\n\n\n\n\n\n\n\n# Join treatment information to AE data\nadae_safety = adae.join(adsl_safety, on=\"USUBJID\")\n\n# Total AE records in safety population\nadae_safety.height\n\n1191",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Adverse events summary</span>"
    ]
  },
  {
    "objectID": "tlf-ae-summary.html#step-3-define-ae-categories",
    "href": "tlf-ae-summary.html#step-3-define-ae-categories",
    "title": "8  Adverse events summary",
    "section": "8.4 Step 3: Define AE Categories",
    "text": "8.4 Step 3: Define AE Categories\nWe’ll calculate participant counts for standard AE categories used in regulatory submissions.\n\ndef count_participants(df, condition=None):\n    \"\"\"\n    Count unique participants meeting a condition\n\n    Args:\n        df: DataFrame with adverse events\n        condition: polars expression for filtering (None = count all)\n\n    Returns:\n        DataFrame with counts by treatment\n    \"\"\"\n    if condition is not None:\n        df = df.filter(condition)\n\n    counts = df.group_by(\"TRT01A\").agg(\n        n = pl.col(\"USUBJID\").n_unique()\n    )\n\n    return treatment_levels.join(counts, on=\"TRT01A\", how=\"left\").with_columns(\n        pl.col(\"n\").fill_null(0)\n    )\n\n# Calculate each category\ncategories = []\n\n# 1. Participants in population (no filtering)\npop_row = pop_counts.with_columns(\n    category = pl.lit(\"Participants in population\")\n).rename({\"N\": \"n\"})\ncategories.append(pop_row)\n\n# 2. With any adverse event\nany_ae = count_participants(adae_safety).with_columns(\n    category = pl.lit(\"With any adverse event\")\n)\ncategories.append(any_ae)\n\n\n# 3. With drug-related adverse event\ndrug_related = count_participants(\n    adae_safety,\n    pl.col(\"AEREL\").is_in([\"POSSIBLE\", \"PROBABLE\", \"DEFINITE\", \"RELATED\"])\n).with_columns(\n    category = pl.lit(\"With drug-related adverse event\")\n)\ncategories.append(drug_related)\n\n# 4. With serious adverse event\nserious = count_participants(\n    adae_safety,\n    pl.col(\"AESER\") == \"Y\"\n).with_columns(\n    category = pl.lit(\"With serious adverse event\")\n)\ncategories.append(serious)\n\n\n# 5. With serious drug-related adverse event\nserious_drug_related = count_participants(\n    adae_safety,\n    (pl.col(\"AESER\") == \"Y\") &\n    pl.col(\"AEREL\").is_in([\"POSSIBLE\", \"PROBABLE\", \"DEFINITE\", \"RELATED\"])\n).with_columns(\n    category = pl.lit(\"With serious drug-related adverse event\")\n)\ncategories.append(serious_drug_related)\n\n# 6. Who died\ndeaths = count_participants(\n    adae_safety,\n    pl.col(\"AEOUT\") == \"FATAL\"\n).with_columns(\n    category = pl.lit(\"Who died\")\n)\ncategories.append(deaths)\n\n# 7. Discontinued due to adverse event\ndiscontinued = count_participants(\n    adae_safety,\n    pl.col(\"AEACN\") == \"DRUG WITHDRAWN\"\n).with_columns(\n    category = pl.lit(\"Discontinued due to adverse event\")\n)\ncategories.append(discontinued)",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Adverse events summary</span>"
    ]
  },
  {
    "objectID": "tlf-ae-summary.html#step-4-combine-and-calculate-percentages",
    "href": "tlf-ae-summary.html#step-4-combine-and-calculate-percentages",
    "title": "8  Adverse events summary",
    "section": "8.5 Step 4: Combine and Calculate Percentages",
    "text": "8.5 Step 4: Combine and Calculate Percentages\nNow we combine all categories and calculate percentages based on the safety population.\n\n# Combine all categories\nae_summary = pl.concat(categories, how=\"diagonal\")\n\n# Add population totals and calculate percentages\nae_summary = ae_summary.join(\n    pop_counts.select([\"TRT01A\", \"N\"]),\n    on=\"TRT01A\",\n    how=\"left\"\n).with_columns([\n    # Fill missing counts with 0\n    pl.col(\"n\").fill_null(0),\n    # Calculate percentage\n    pl.when(pl.col(\"category\") == \"Participants in population\")\n        .then(None)  # No percentage for population row\n        .otherwise((100.0 * pl.col(\"n\") / pl.col(\"N\")).round(1))\n        .alias(\"pct\")\n])\n\nae_summary.sort([\"category\", \"TRT01A\"])\n\n\nshape: (21, 5)\n\n\n\nTRT01A\nn\ncategory\nN\npct\n\n\nstr\nu32\nstr\nu32\nf64\n\n\n\n\n\"Placebo\"\n0\n\"Discontinued due to adverse ev…\n86\n0.0\n\n\n\"Xanomeline High Dose\"\n0\n\"Discontinued due to adverse ev…\n84\n0.0\n\n\n\"Xanomeline Low Dose\"\n0\n\"Discontinued due to adverse ev…\n84\n0.0\n\n\n…\n…\n…\n…\n…\n\n\n\"Xanomeline High Dose\"\n1\n\"With serious drug-related adve…\n84\n1.2\n\n\n\"Xanomeline Low Dose\"\n1\n\"With serious drug-related adve…\n84\n1.2",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Adverse events summary</span>"
    ]
  },
  {
    "objectID": "tlf-ae-summary.html#step-5-format-for-display",
    "href": "tlf-ae-summary.html#step-5-format-for-display",
    "title": "8  Adverse events summary",
    "section": "8.6 Step 5: Format for Display",
    "text": "8.6 Step 5: Format for Display\nWe’ll format the counts and percentages for the final table display.\n\n# Format display values\nae_formatted = ae_summary.with_columns([\n    # Show counts as strings, including zeros\n    pl.col(\"n\").cast(str).alias(\"n_display\"),\n    # Format percentages with parentheses; blank out population row\n    pl.when(pl.col(\"category\") == \"Participants in population\")\n      .then(pl.lit(\"\"))\n      .otherwise(\n          pl.format(\"({})\", pl.col(\"pct\").fill_null(0).round(1).cast(str))\n      )\n      .alias(\"pct_display\")\n])\n\nae_formatted.select([\"category\", \"TRT01A\", \"n_display\", \"pct_display\"])\n\n\nshape: (21, 4)\n\n\n\ncategory\nTRT01A\nn_display\npct_display\n\n\nstr\nstr\nstr\nstr\n\n\n\n\n\"Participants in population\"\n\"Placebo\"\n\"86\"\n\"\"\n\n\n\"Participants in population\"\n\"Xanomeline High Dose\"\n\"84\"\n\"\"\n\n\n\"Participants in population\"\n\"Xanomeline Low Dose\"\n\"84\"\n\"\"\n\n\n…\n…\n…\n…\n\n\n\"Discontinued due to adverse ev…\n\"Xanomeline High Dose\"\n\"0\"\n\"(0.0)\"\n\n\n\"Discontinued due to adverse ev…\n\"Xanomeline Low Dose\"\n\"0\"\n\"(0.0)\"",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Adverse events summary</span>"
    ]
  },
  {
    "objectID": "tlf-ae-summary.html#step-6-create-final-table-structure",
    "href": "tlf-ae-summary.html#step-6-create-final-table-structure",
    "title": "8  Adverse events summary",
    "section": "8.7 Step 6: Create Final Table Structure",
    "text": "8.7 Step 6: Create Final Table Structure\nWe reshape the data to create the final table with treatments as columns.\n\n# Define category order for consistent display\ncategory_order = [\n    \"Participants in population\",\n    \"With any adverse event\",\n    \"With drug-related adverse event\",\n    \"With serious adverse event\",\n    \"With serious drug-related adverse event\",\n    \"Who died\",\n    \"Discontinued due to adverse event\"\n]\n\n# Pivot to wide format\nae_wide = ae_formatted.pivot(\n    values=[\"n_display\", \"pct_display\"],\n    index=\"category\",\n    on=\"TRT01A\",\n    maintain_order=True\n)\n\n# Reorder columns for each treatment group\ntreatments = [\"Placebo\", \"Xanomeline Low Dose\", \"Xanomeline High Dose\"]\ncolumn_order = [\"category\"]\nfor trt in treatments:\n    column_order.extend([f\"n_display_{trt}\", f\"pct_display_{trt}\"])\n\n# Create final table with proper column order\nfinal_table = ae_wide.select(column_order).sort(\n    pl.col(\"category\").cast(pl.Enum(category_order))\n)\n\nfinal_table\n\n\nshape: (7, 7)\n\n\n\ncategory\nn_display_Placebo\npct_display_Placebo\nn_display_Xanomeline Low Dose\npct_display_Xanomeline Low Dose\nn_display_Xanomeline High Dose\npct_display_Xanomeline High Dose\n\n\nstr\nstr\nstr\nstr\nstr\nstr\nstr\n\n\n\n\n\"Participants in population\"\n\"86\"\n\"\"\n\"84\"\n\"\"\n\"84\"\n\"\"\n\n\n\"With any adverse event\"\n\"69\"\n\"(80.2)\"\n\"77\"\n\"(91.7)\"\n\"79\"\n\"(94.0)\"\n\n\n\"With drug-related adverse even…\n\"44\"\n\"(51.2)\"\n\"73\"\n\"(86.9)\"\n\"70\"\n\"(83.3)\"\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Who died\"\n\"2\"\n\"(2.3)\"\n\"1\"\n\"(1.2)\"\n\"0\"\n\"(0.0)\"\n\n\n\"Discontinued due to adverse ev…\n\"0\"\n\"(0.0)\"\n\"0\"\n\"(0.0)\"\n\"0\"\n\"(0.0)\"",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Adverse events summary</span>"
    ]
  },
  {
    "objectID": "tlf-ae-summary.html#step-7-generate-publication-ready-output",
    "href": "tlf-ae-summary.html#step-7-generate-publication-ready-output",
    "title": "8  Adverse events summary",
    "section": "8.8 Step 7: Generate Publication-Ready Output",
    "text": "8.8 Step 7: Generate Publication-Ready Output\nFinally, we format the AE summary table for regulatory submission using the rtflite package.\n\n# Get population sizes for column headers\nn_placebo = pop_counts.filter(pl.col(\"TRT01A\") == \"Placebo\")[\"N\"][0]\nn_low = pop_counts.filter(pl.col(\"TRT01A\") == \"Xanomeline Low Dose\")[\"N\"][0]\nn_high = pop_counts.filter(pl.col(\"TRT01A\") == \"Xanomeline High Dose\")[\"N\"][0]\n\ndoc_ae_summary = rtf.RTFDocument(\n    df=final_table.rename({\"category\": \"\"}),\n    rtf_title=rtf.RTFTitle(\n        text=[\n            \"Analysis of Adverse Event Summary\",\n            \"(Safety Analysis Population)\"\n        ]\n    ),\n    rtf_column_header=[\n        rtf.RTFColumnHeader(\n            text = [\n                \"\",\n                \"Placebo\",\n                \"Xanomeline Low Dose\",\n                \"Xanomeline High Dose\"\n            ],\n            col_rel_width=[4, 2, 2, 2],\n            text_justification=[\"l\", \"c\", \"c\", \"c\"],\n        ),\n        rtf.RTFColumnHeader(\n            text=[\n                \"\",          # Empty for first column\n                \"n\", \"(%)\",  # Placebo columns\n                \"n\", \"(%)\",  # Low Dose columns\n                \"n\", \"(%)\"   # High Dose columns\n            ],\n            col_rel_width=[4] + [1] * 6,\n            text_justification=[\"l\"] + [\"c\"] * 6,\n            border_left = [\"single\"] + [\"single\", \"\"] * 3,\n            border_top = [\"\"] + [\"single\"] * 6\n        )\n    ],\n    rtf_body=rtf.RTFBody(\n        col_rel_width=[4] + [1] * 6,\n        text_justification=[\"l\"] + [\"c\"] * 6,\n        border_left = [\"single\"] + [\"single\", \"\"] * 3\n    ),\n    rtf_footnote=rtf.RTFFootnote(\n        text=[\n            \"Every subject is counted a single time for each applicable row and column.\"\n        ]\n    ),\n    rtf_source=rtf.RTFSource(\n        text=[\"Source: ADSL and ADAE datasets\"]\n    )\n)\n\ndoc_ae_summary.write_rtf(\"rtf/tlf_ae_summary.rtf\")\n\nrtf/tlf_ae_summary.rtf\n\n\n\n\nPosixPath('pdf/tlf_ae_summary.pdf')",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Adverse events summary</span>"
    ]
  },
  {
    "objectID": "tlf-ae-specific.html",
    "href": "tlf-ae-specific.html",
    "title": "9  Specific adverse events",
    "section": "",
    "text": "9.1 Overview\nSpecific adverse events tables provide detailed safety information organized by System Organ Class (SOC) and Preferred Term (PT) following the Medical Dictionary for Regulatory Activities (MedDRA) hierarchy. Following ICH E3 guidance, these tables are essential components of clinical study reports that present participant-level adverse event data across treatment groups.\nKey features of specific AE tables include:\nThis tutorial demonstrates how to create a regulatory-compliant specific adverse events table using Python’s rtflite package.",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Specific adverse events</span>"
    ]
  },
  {
    "objectID": "tlf-ae-specific.html#overview",
    "href": "tlf-ae-specific.html#overview",
    "title": "9  Specific adverse events",
    "section": "",
    "text": "Hierarchical structure: SOC categories with nested specific AE terms\nParticipant counts: Number of participants experiencing each AE type\nTreatment comparison: Side-by-side counts across treatment groups\nMedDRA compliance: Standardized medical terminology for regulatory submissions",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Specific adverse events</span>"
    ]
  },
  {
    "objectID": "tlf-ae-specific.html#setup",
    "href": "tlf-ae-specific.html#setup",
    "title": "9  Specific adverse events",
    "section": "9.2 Setup",
    "text": "9.2 Setup\n\nimport polars as pl\nimport rtflite as rtf\n\n\n\npolars.config.Config\n\n\n\nadsl = pl.read_parquet(\"data/adsl.parquet\")\nadae = pl.read_parquet(\"data/adae.parquet\")\ntreatments = [\"Placebo\", \"Xanomeline Low Dose\", \"Xanomeline High Dose\"]",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Specific adverse events</span>"
    ]
  },
  {
    "objectID": "tlf-ae-specific.html#step-1-load-and-explore-data",
    "href": "tlf-ae-specific.html#step-1-load-and-explore-data",
    "title": "9  Specific adverse events",
    "section": "9.3 Step 1: Load and Explore Data",
    "text": "9.3 Step 1: Load and Explore Data\nWe start by examining the adverse events data structure and understanding the MedDRA hierarchy.\n\n# Display key variables in ADAE dataset\nadae_vars = adae.select([\"USUBJID\", \"TRTA\", \"AEBODSYS\", \"AEDECOD\", \"AESEV\", \"AESER\"])\n# Key ADAE variables\nadae_vars\n\n\nshape: (1_191, 6)\n\n\n\nUSUBJID\nTRTA\nAEBODSYS\nAEDECOD\nAESEV\nAESER\n\n\nstr\nstr\nstr\nstr\nstr\nstr\n\n\n\n\n\"01-701-1015\"\n\"Placebo\"\n\"GENERAL DISORDERS AND ADMINIST…\n\"APPLICATION SITE ERYTHEMA\"\n\"MILD\"\n\"N\"\n\n\n\"01-701-1015\"\n\"Placebo\"\n\"GENERAL DISORDERS AND ADMINIST…\n\"APPLICATION SITE PRURITUS\"\n\"MILD\"\n\"N\"\n\n\n\"01-701-1015\"\n\"Placebo\"\n\"GASTROINTESTINAL DISORDERS\"\n\"DIARRHOEA\"\n\"MILD\"\n\"N\"\n\n\n…\n…\n…\n…\n…\n…\n\n\n\"01-718-1427\"\n\"Xanomeline High Dose\"\n\"METABOLISM AND NUTRITION DISOR…\n\"DECREASED APPETITE\"\n\"MODERATE\"\n\"N\"\n\n\n\"01-718-1427\"\n\"Xanomeline High Dose\"\n\"GASTROINTESTINAL DISORDERS\"\n\"NAUSEA\"\n\"MODERATE\"\n\"N\"\n\n\n\n\n\n\n\n# Examine the MedDRA hierarchy structure\n# System Organ Classes (SOCs) in the data\nsoc_summary = adae.group_by(\"AEBODSYS\").agg(\n    n_participants=pl.col(\"USUBJID\").n_unique(),\n    n_events=pl.len()\n).sort(\"n_participants\", descending=True)\nsoc_summary\n\n\nshape: (23, 3)\n\n\n\nAEBODSYS\nn_participants\nn_events\n\n\nstr\nu32\nu32\n\n\n\n\n\"GENERAL DISORDERS AND ADMINIST…\n108\n292\n\n\n\"SKIN AND SUBCUTANEOUS TISSUE D…\n105\n276\n\n\n\"NERVOUS SYSTEM DISORDERS\"\n59\n101\n\n\n…\n…\n…\n\n\n\"SOCIAL CIRCUMSTANCES\"\n1\n1\n\n\n\"HEPATOBILIARY DISORDERS\"\n1\n1",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Specific adverse events</span>"
    ]
  },
  {
    "objectID": "tlf-ae-specific.html#step-2-prepare-analysis-population",
    "href": "tlf-ae-specific.html#step-2-prepare-analysis-population",
    "title": "9  Specific adverse events",
    "section": "9.4 Step 2: Prepare Analysis Population",
    "text": "9.4 Step 2: Prepare Analysis Population\nFollowing regulatory standards, we focus on the safety analysis population.\n\n# Define safety population\nadsl_safety = adsl.filter(pl.col(\"SAFFL\") == \"Y\").select([\"USUBJID\", \"TRT01A\"])\n# Safety population size\nadsl_safety.height\n\n# Get safety population counts by treatment\npop_counts = adsl_safety.group_by(\"TRT01A\").agg(N=pl.len()).sort(\"TRT01A\")\n# Safety population by treatment\npop_counts\n\n\nshape: (3, 2)\n\n\n\nTRT01A\nN\n\n\nstr\nu32\n\n\n\n\n\"Placebo\"\n86\n\n\n\"Xanomeline High Dose\"\n84\n\n\n\"Xanomeline Low Dose\"\n84\n\n\n\n\n\n\n\n# Filter adverse events to safety population\nadae_safety = adae.join(adsl_safety, on=\"USUBJID\", how=\"inner\")\n# AE records in safety population\nadae_safety.height\n\n1191",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Specific adverse events</span>"
    ]
  },
  {
    "objectID": "tlf-ae-specific.html#step-3-data-preparation-and-standardization",
    "href": "tlf-ae-specific.html#step-3-data-preparation-and-standardization",
    "title": "9  Specific adverse events",
    "section": "9.5 Step 3: Data Preparation and Standardization",
    "text": "9.5 Step 3: Data Preparation and Standardization\nWe standardize the adverse event terms and prepare the hierarchical data structure.\n\n# Standardize AE term formatting for consistency\nae_counts = (\n    adae_safety\n    .with_columns([\n        pl.col(\"AEDECOD\").str.to_titlecase().alias(\"AEDECOD_STD\"),\n        pl.col(\"AEBODSYS\").str.to_titlecase().alias(\"AEBODSYS_STD\")\n    ])\n    .group_by([\"TRT01A\", \"AEBODSYS_STD\", \"AEDECOD_STD\"])\n    .agg(n=pl.col(\"USUBJID\").n_unique())\n    .sort([\"AEBODSYS_STD\", \"AEDECOD_STD\", \"TRT01A\"])\n)\n\n# Sample of prepared AE counts\nae_counts\n\n\nshape: (373, 4)\n\n\n\nTRT01A\nAEBODSYS_STD\nAEDECOD_STD\nn\n\n\nstr\nstr\nstr\nu32\n\n\n\n\n\"Placebo\"\n\"Cardiac Disorders\"\n\"Atrial Fibrillation\"\n1\n\n\n\"Xanomeline High Dose\"\n\"Cardiac Disorders\"\n\"Atrial Fibrillation\"\n3\n\n\n\"Xanomeline Low Dose\"\n\"Cardiac Disorders\"\n\"Atrial Fibrillation\"\n1\n\n\n…\n…\n…\n…\n\n\n\"Placebo\"\n\"Vascular Disorders\"\n\"Orthostatic Hypotension\"\n1\n\n\n\"Xanomeline High Dose\"\n\"Vascular Disorders\"\n\"Wound Haemorrhage\"\n1",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Specific adverse events</span>"
    ]
  },
  {
    "objectID": "tlf-ae-specific.html#step-4-build-hierarchical-table-structure",
    "href": "tlf-ae-specific.html#step-4-build-hierarchical-table-structure",
    "title": "9  Specific adverse events",
    "section": "9.6 Step 4: Build Hierarchical Table Structure",
    "text": "9.6 Step 4: Build Hierarchical Table Structure\nWe create a nested table structure with SOC headers and indented specific terms.\n\n# Initialize table with population counts\ntable_data = [\n    [\"Participants in population\"] + [\n        str(pop_counts.filter(pl.col(\"TRT01A\") == t)[\"N\"][0])\n        for t in treatments\n    ],\n    [\"\"] * 4  # Blank separator row\n]\n\n# Build hierarchical structure: SOC -&gt; Specific AE terms\nfor soc in ae_counts[\"AEBODSYS_STD\"].unique().sort():\n    # Add SOC header row (bold formatting will be applied later)\n    table_data.append([soc] + [\"\"] * 3)\n\n    # Get all AE terms within this SOC\n    soc_data = ae_counts.filter(pl.col(\"AEBODSYS_STD\") == soc)\n\n    # Add each specific AE term with counts\n    for ae_term in soc_data[\"AEDECOD_STD\"].unique().sort():\n        row = [f\"  {ae_term}\"]  # Indent specific terms\n\n        # Add counts for each treatment group\n        for trt in treatments:\n            count_data = soc_data.filter(\n                (pl.col(\"AEDECOD_STD\") == ae_term) &\n                (pl.col(\"TRT01A\") == trt)\n            )\n            count = count_data[\"n\"][0] if count_data.height &gt; 0 else 0\n            row.append(str(count))\n\n        table_data.append(row)\n\n# Convert to Polars DataFrame\ndf_ae_specific = pl.DataFrame(\n    table_data,\n    schema=[\"\"] + treatments,\n    orient=\"row\"\n)\n\n# Final table structure\ndf_ae_specific.shape\ndf_ae_specific\n\n\nshape: (267, 4)\n\n\n\ncolumn_0\nPlacebo\nXanomeline Low Dose\nXanomeline High Dose\n\n\nstr\nstr\nstr\nstr\n\n\n\n\n\"Participants in population\"\n\"86\"\n\"84\"\n\"84\"\n\n\n\"\"\n\"\"\n\"\"\n\"\"\n\n\n\"Cardiac Disorders\"\n\"\"\n\"\"\n\"\"\n\n\n…\n…\n…\n…\n\n\n\"  Orthostatic Hypotension\"\n\"1\"\n\"0\"\n\"0\"\n\n\n\"  Wound Haemorrhage\"\n\"0\"\n\"0\"\n\"1\"",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Specific adverse events</span>"
    ]
  },
  {
    "objectID": "tlf-ae-specific.html#step-5-create-regulatory-compliant-rtf-output",
    "href": "tlf-ae-specific.html#step-5-create-regulatory-compliant-rtf-output",
    "title": "9  Specific adverse events",
    "section": "9.7 Step 5: Create Regulatory-Compliant RTF Output",
    "text": "9.7 Step 5: Create Regulatory-Compliant RTF Output\nWe format the table following regulatory submission standards with proper hierarchy and formatting.\n\n# Create comprehensive RTF document\ndoc_ae_specific = rtf.RTFDocument(\n    df=df_ae_specific,\n    rtf_title=rtf.RTFTitle(\n        text=[\n            \"Adverse Events by System Organ Class and Preferred Term\",\n            \"(Safety Analysis Set)\"\n        ]\n    ),\n    rtf_column_header=rtf.RTFColumnHeader(\n        text=[\n            \"System Organ Class\\\\line   Preferred Term\",\n            f\"Placebo\\\\line (N={pop_counts.filter(pl.col('TRT01A') == 'Placebo')['N'][0]})\",\n            f\"Xanomeline Low Dose\\\\line (N={pop_counts.filter(pl.col('TRT01A') == 'Xanomeline Low Dose')['N'][0]})\",\n            f\"Xanomeline High Dose\\\\line (N={pop_counts.filter(pl.col('TRT01A') == 'Xanomeline High Dose')['N'][0]})\"\n        ],\n        col_rel_width=[4, 1.5, 1.5, 1.5],\n        text_justification=[\"l\", \"c\", \"c\", \"c\"],\n        text_format=\"b\",  # Bold headers\n        border_bottom=\"single\"\n    ),\n    rtf_body=rtf.RTFBody(\n        col_rel_width=[4, 1.5, 1.5, 1.5],\n        text_justification=[\"l\", \"c\", \"c\", \"c\"],\n        # Apply bold formatting to SOC headers (rows without indentation)\n        text_font_style=lambda df, i, j: \"b\" if j == 0 and not str(df[i, j]).startswith(\"  \") and str(df[i, j]) != \"\" else \"\"\n    ),\n    rtf_footnote=rtf.RTFFootnote(\n        text=[\n            \"MedDRA version 25.0.\",\n            \"Each participant is counted once within each preferred term and system organ class.\",\n            \"Participants with multiple events in the same preferred term are counted only once for that term.\"\n        ]\n    ),\n    rtf_source=rtf.RTFSource(\n        text=[\"Source: ADAE Analysis Dataset (Data cutoff: 01JAN2023)\"]\n    )\n)\n\n# Generate RTF file\ndoc_ae_specific.write_rtf(\"rtf/tlf_ae_specific.rtf\")\n# RTF file created: rtf/tlf_ae_specific.rtf\n\nrtf/tlf_ae_specific.rtf\n\n\n\n\nPosixPath('pdf/tlf_ae_specific.pdf')",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Specific adverse events</span>"
    ]
  },
  {
    "objectID": "tlf-efficacy-ancova.html",
    "href": "tlf-efficacy-ancova.html",
    "title": "10  ANCOVA efficacy analysis",
    "section": "",
    "text": "10.1 Overview\nAnalysis of Covariance (ANCOVA) is the primary statistical method for efficacy evaluation in clinical trials. Following ICH E9 guidance on statistical principles for clinical trials, ANCOVA provides a robust framework for comparing treatment effects while controlling for baseline covariates.\nKey features of ANCOVA efficacy analysis include:\nThis tutorial demonstrates how to create a comprehensive ANCOVA efficacy table for glucose change from baseline using Python’s statistical libraries and rtflite for regulatory-compliant formatting.",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ANCOVA efficacy analysis</span>"
    ]
  },
  {
    "objectID": "tlf-efficacy-ancova.html#overview",
    "href": "tlf-efficacy-ancova.html#overview",
    "title": "10  ANCOVA efficacy analysis",
    "section": "",
    "text": "Covariate adjustment: Controls for baseline values to reduce variability\nLeast squares means: Provides treatment effect estimates adjusted for covariates\nMissing data handling: Implements appropriate imputation strategies (e.g., LOCF, MMRM)\nPairwise comparisons: Tests specific treatment contrasts with confidence intervals\nRegulatory compliance: Follows statistical analysis plan specifications",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ANCOVA efficacy analysis</span>"
    ]
  },
  {
    "objectID": "tlf-efficacy-ancova.html#setup",
    "href": "tlf-efficacy-ancova.html#setup",
    "title": "10  ANCOVA efficacy analysis",
    "section": "10.2 Setup",
    "text": "10.2 Setup\n\nimport polars as pl\nimport rtflite as rtf\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\nfrom scipy import stats as scipy_stats\n\n\n\npolars.config.Config\n\n\n\nadsl = pl.read_parquet(\"data/adsl.parquet\")\nadlbc = pl.read_parquet(\"data/adlbc.parquet\")\ntreatments = [\"Placebo\", \"Xanomeline Low Dose\", \"Xanomeline High Dose\"]",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ANCOVA efficacy analysis</span>"
    ]
  },
  {
    "objectID": "tlf-efficacy-ancova.html#step-1-explore-laboratory-data-structure",
    "href": "tlf-efficacy-ancova.html#step-1-explore-laboratory-data-structure",
    "title": "10  ANCOVA efficacy analysis",
    "section": "10.3 Step 1: Explore Laboratory Data Structure",
    "text": "10.3 Step 1: Explore Laboratory Data Structure\nWe start by understanding the glucose data structure and endpoint definitions.\n\n# Display key laboratory variables\n# Key ADLBC variables for efficacy analysis\nlab_vars = adlbc.select([\"USUBJID\", \"PARAMCD\", \"PARAM\", \"AVISIT\", \"AVISITN\", \"AVAL\", \"BASE\", \"CHG\"])\nlab_vars\n\n\nshape: (74_264, 8)\n\n\n\nUSUBJID\nPARAMCD\nPARAM\nAVISIT\nAVISITN\nAVAL\nBASE\nCHG\n\n\nstr\nstr\nstr\nstr\nf64\nf64\nf64\nf64\n\n\n\n\n\"01-701-1015\"\n\"SODIUM\"\n\"Sodium (mmol/L)\"\n\"        Baseline\"\n0.0\n140.0\n140.0\nnull\n\n\n\"01-701-1015\"\n\"K\"\n\"Potassium (mmol/L)\"\n\"        Baseline\"\n0.0\n4.5\n4.5\nnull\n\n\n\"01-701-1015\"\n\"CL\"\n\"Chloride (mmol/L)\"\n\"        Baseline\"\n0.0\n106.0\n106.0\nnull\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"01-718-1427\"\n\"_CK\"\n\"Creatine Kinase (U/L) change f…\n\"          Week 8\"\n8.0\n-0.5\nnull\nnull\n\n\n\"01-718-1427\"\n\"_CK\"\n\"Creatine Kinase (U/L) change f…\n\"End of Treatment\"\n99.0\n-0.5\nnull\nnull\n\n\n\n\n\n\n\n# Focus on glucose parameter\ngluc_visits = adlbc.filter(pl.col(\"PARAMCD\") == \"GLUC\").select(\"AVISIT\", \"AVISITN\").unique().sort(\"AVISITN\")\n# Available glucose measurement visits\ngluc_visits\n\n\nshape: (12, 2)\n\n\n\nAVISIT\nAVISITN\n\n\nstr\nf64\n\n\n\n\n\"               .\"\nnull\n\n\n\"        Baseline\"\n0.0\n\n\n\"          Week 2\"\n2.0\n\n\n…\n…\n\n\n\"         Week 26\"\n26.0\n\n\n\"End of Treatment\"\n99.0",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ANCOVA efficacy analysis</span>"
    ]
  },
  {
    "objectID": "tlf-efficacy-ancova.html#step-2-define-analysis-population-and-endpoint",
    "href": "tlf-efficacy-ancova.html#step-2-define-analysis-population-and-endpoint",
    "title": "10  ANCOVA efficacy analysis",
    "section": "10.4 Step 2: Define Analysis Population and Endpoint",
    "text": "10.4 Step 2: Define Analysis Population and Endpoint\nFollowing the Statistical Analysis Plan, we focus on the efficacy population for the primary endpoint.\n\n# Clean data types and prepare datasets\nadlbc_clean = adlbc.with_columns([\n    pl.col(c).cast(str).str.strip_chars()\n    for c in [\"USUBJID\", \"PARAMCD\", \"AVISIT\", \"TRTP\"]\n])\n\n# Define efficacy population\nadsl_eff = adsl.filter(pl.col(\"EFFFL\") == \"Y\").select([\"USUBJID\"])\n# Efficacy population size\nadsl_eff.height\n\n# Filter laboratory data to efficacy population\nadlbc_eff = adlbc_clean.join(adsl_eff, on=\"USUBJID\", how=\"inner\")\n# Laboratory records in efficacy population\nadlbc_eff.height\n\n71882\n\n\n\n# Examine glucose data availability by visit and treatment\ngluc_availability = (\n    adlbc_eff.filter(pl.col(\"PARAMCD\") == \"GLUC\")\n    .group_by([\"TRTP\", \"AVISIT\"])\n    .agg(n_subjects=pl.col(\"USUBJID\").n_unique())\n    .sort([\"TRTP\", \"AVISIT\"])\n)\n# Glucose data availability by visit\ngluc_availability\n\n\nshape: (36, 3)\n\n\n\nTRTP\nAVISIT\nn_subjects\n\n\nstr\nstr\nu32\n\n\n\n\n\"Placebo\"\n\".\"\n13\n\n\n\"Placebo\"\n\"Baseline\"\n79\n\n\n\"Placebo\"\n\"End of Treatment\"\n79\n\n\n…\n…\n…\n\n\n\"Xanomeline Low Dose\"\n\"Week 6\"\n62\n\n\n\"Xanomeline Low Dose\"\n\"Week 8\"\n59",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ANCOVA efficacy analysis</span>"
    ]
  },
  {
    "objectID": "tlf-efficacy-ancova.html#step-3-implement-locf-imputation-strategy",
    "href": "tlf-efficacy-ancova.html#step-3-implement-locf-imputation-strategy",
    "title": "10  ANCOVA efficacy analysis",
    "section": "10.5 Step 3: Implement LOCF Imputation Strategy",
    "text": "10.5 Step 3: Implement LOCF Imputation Strategy\nWe apply Last Observation Carried Forward (LOCF) for missing Week 24 glucose values.\n\n# Prepare glucose data with LOCF for Week 24 endpoint\ngluc_data = (\n    adlbc_eff\n    .filter((pl.col(\"PARAMCD\") == \"GLUC\") & (pl.col(\"AVISITN\") &lt;= 24))\n    .sort([\"USUBJID\", \"AVISITN\"])\n    .group_by(\"USUBJID\")\n    .agg([\n        pl.col(\"TRTP\").first(),\n        pl.col(\"BASE\").first(),\n        pl.col(\"AVAL\").filter(pl.col(\"AVISITN\") == 0).first().alias(\"Baseline\"),\n        pl.col(\"AVAL\").last().alias(\"Week_24_LOCF\"),  # LOCF: last available value &lt;= Week 24\n        pl.col(\"AVISITN\").max().alias(\"Last_Visit\")   # Track actual last visit\n    ])\n    .filter(pl.col(\"Baseline\").is_not_null() & pl.col(\"Week_24_LOCF\").is_not_null())\n    .with_columns((pl.col(\"Week_24_LOCF\") - pl.col(\"Baseline\")).alias(\"CHG\"))\n)\n\n# Subjects with baseline and Week 24 (LOCF) glucose\ngluc_data.height\n# Sample of prepared analysis data\ngluc_data\n\n\nshape: (232, 7)\n\n\n\nUSUBJID\nTRTP\nBASE\nBaseline\nWeek_24_LOCF\nLast_Visit\nCHG\n\n\nstr\nstr\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"01-701-1015\"\n\"Placebo\"\n4.71835\n4.71835\n4.49631\n24.0\n-0.22204\n\n\n\"01-701-1023\"\n\"Placebo\"\n5.32896\n5.32896\n5.43998\n4.0\n0.11102\n\n\n\"01-701-1028\"\n\"Xanomeline High Dose\"\n4.77386\n4.77386\n5.43998\n24.0\n0.66612\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n\"01-718-1371\"\n\"Xanomeline High Dose\"\n6.27263\n6.27263\n5.10692\n12.0\n-1.16571\n\n\n\"01-718-1427\"\n\"Xanomeline High Dose\"\n4.55182\n4.55182\n3.71917\n8.0\n-0.83265\n\n\n\n\n\n\n\n# Assess LOCF imputation impact\nlocf_summary = (\n    gluc_data\n    .group_by([\"TRTP\", \"Last_Visit\"])\n    .agg(n_subjects=pl.len())\n    .sort([\"TRTP\", \"Last_Visit\"])\n)\n# LOCF imputation summary (last actual visit used)\nlocf_summary\n\n\nshape: (26, 3)\n\n\n\nTRTP\nLast_Visit\nn_subjects\n\n\nstr\nf64\nu32\n\n\n\n\n\"Placebo\"\n2.0\n2\n\n\n\"Placebo\"\n4.0\n2\n\n\n\"Placebo\"\n6.0\n2\n\n\n…\n…\n…\n\n\n\"Xanomeline Low Dose\"\n20.0\n5\n\n\n\"Xanomeline Low Dose\"\n24.0\n25",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ANCOVA efficacy analysis</span>"
    ]
  },
  {
    "objectID": "tlf-efficacy-ancova.html#step-4-calculate-descriptive-statistics",
    "href": "tlf-efficacy-ancova.html#step-4-calculate-descriptive-statistics",
    "title": "10  ANCOVA efficacy analysis",
    "section": "10.6 Step 4: Calculate Descriptive Statistics",
    "text": "10.6 Step 4: Calculate Descriptive Statistics\nWe compute baseline, Week 24, and change from baseline statistics by treatment group.\n\n# Calculate comprehensive descriptive statistics\ndesc_stats = []\nfor trt in treatments:\n    # Analysis data for this treatment\n    trt_data = gluc_data.filter(pl.col(\"TRTP\") == trt)\n\n    # Original baseline data (all subjects with baseline)\n    baseline_full = adlbc_eff.filter(\n        (pl.col(\"PARAMCD\") == \"GLUC\") &\n        (pl.col(\"AVISIT\") == \"Baseline\") &\n        (pl.col(\"TRTP\") == trt)\n    )\n\n    desc_stats.append({\n        \"Treatment\": trt,\n        \"N_Baseline\": baseline_full.height,\n        \"Baseline_Mean\": baseline_full[\"AVAL\"].mean() if baseline_full.height &gt; 0 else np.nan,\n        \"Baseline_SD\": baseline_full[\"AVAL\"].std() if baseline_full.height &gt; 0 else np.nan,\n        \"N_Week24\": trt_data.height,\n        \"Week24_Mean\": trt_data[\"Week_24_LOCF\"].mean() if trt_data.height &gt; 0 else np.nan,\n        \"Week24_SD\": trt_data[\"Week_24_LOCF\"].std() if trt_data.height &gt; 0 else np.nan,\n        \"N_Change\": trt_data.height,\n        \"Change_Mean\": trt_data[\"CHG\"].mean() if trt_data.height &gt; 0 else np.nan,\n        \"Change_SD\": trt_data[\"CHG\"].std() if trt_data.height &gt; 0 else np.nan\n    })\n\n# Display descriptive statistics\ndesc_df = pl.DataFrame(desc_stats)\n# Descriptive statistics by treatment\ndesc_df\n\n\nshape: (3, 10)\n\n\n\nTreatment\nN_Baseline\nBaseline_Mean\nBaseline_SD\nN_Week24\nWeek24_Mean\nWeek24_SD\nN_Change\nChange_Mean\nChange_SD\n\n\nstr\ni64\nf64\nf64\ni64\nf64\nf64\ni64\nf64\nf64\n\n\n\n\n\"Placebo\"\n79\n5.656399\n2.229324\n79\n5.639535\n1.651807\n79\n-0.016864\n2.31841\n\n\n\"Xanomeline Low Dose\"\n79\n5.419603\n0.946102\n79\n5.352148\n1.058004\n79\n-0.067455\n1.015715\n\n\n\"Xanomeline High Dose\"\n74\n5.388971\n1.374893\n74\n5.831551\n2.214668\n74\n0.44258\n1.645179",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ANCOVA efficacy analysis</span>"
    ]
  },
  {
    "objectID": "tlf-efficacy-ancova.html#step-5-perform-ancova-analysis",
    "href": "tlf-efficacy-ancova.html#step-5-perform-ancova-analysis",
    "title": "10  ANCOVA efficacy analysis",
    "section": "10.7 Step 5: Perform ANCOVA Analysis",
    "text": "10.7 Step 5: Perform ANCOVA Analysis\nWe fit the ANCOVA model with treatment and baseline glucose as covariates.\n\n# Convert to pandas for statsmodels compatibility\nancova_df = gluc_data.to_pandas()\nancova_df[\"TRTP\"] = pd.Categorical(ancova_df[\"TRTP\"], categories=treatments)\n\n# Fit ANCOVA model: Change = Treatment + Baseline\nmodel = smf.ols(\"CHG ~ TRTP + BASE\", data=ancova_df).fit()\n\n# Display model summary\n# ANCOVA Model Summary\nmodel.rsquared\nmodel.fvalue, model.f_pvalue\n# Model coefficients\nmodel.summary().tables[1]\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n2.9972\n0.392\n7.642\n0.000\n2.224\n3.770\n\n\nTRTP[T.Xanomeline Low Dose]\n-0.1768\n0.243\n-0.729\n0.467\n-0.655\n0.301\n\n\nTRTP[T.Xanomeline High Dose]\n0.3169\n0.247\n1.284\n0.200\n-0.169\n0.803\n\n\nBASE\n-0.5329\n0.062\n-8.543\n0.000\n-0.656\n-0.410\n\n\n\n\n\n\n# Calculate adjusted means (LS means) at mean baseline value\nbase_mean = ancova_df[\"BASE\"].mean()\nvar_cov = model.cov_params()\nls_means = []\n\n# LS means calculated at baseline mean\nbase_mean\n\nfor i, trt in enumerate(treatments):\n    # Create prediction vector for LS mean calculation\n    # Model: CHG = intercept + trt_effect1*(trt==1) + trt_effect2*(trt==2) + base_effect*baseline\n    x_pred = np.array([1, int(i==1), int(i==2), base_mean])\n\n    # Calculate LS mean\n    ls_mean = model.predict(pd.DataFrame({\"TRTP\": [trt], \"BASE\": [base_mean]}))[0]\n\n    # Calculate standard error for confidence interval\n    se_pred = np.sqrt(x_pred @ var_cov @ x_pred.T)\n\n    ls_means.append({\n        \"Treatment\": trt,\n        \"LS_Mean\": ls_mean,\n        \"SE\": se_pred,\n        \"CI_Lower\": ls_mean - 1.96 * se_pred,\n        \"CI_Upper\": ls_mean + 1.96 * se_pred\n    })\n\nls_means_df = pl.DataFrame(ls_means)\n# LS Means (95% CI)\nls_means_df\n\n\nshape: (3, 5)\n\n\n\nTreatment\nLS_Mean\nSE\nCI_Lower\nCI_Upper\n\n\nstr\nf64\nf64\nf64\nf64\n\n\n\n\n\"Placebo\"\n0.071554\n0.171563\n-0.264709\n0.407818\n\n\n\"Xanomeline Low Dose\"\n-0.105215\n0.171308\n-0.440977\n0.230548\n\n\n\"Xanomeline High Dose\"\n0.388498\n0.177055\n0.041471\n0.735525",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ANCOVA efficacy analysis</span>"
    ]
  },
  {
    "objectID": "tlf-efficacy-ancova.html#step-6-pairwise-treatment-comparisons",
    "href": "tlf-efficacy-ancova.html#step-6-pairwise-treatment-comparisons",
    "title": "10  ANCOVA efficacy analysis",
    "section": "10.8 Step 6: Pairwise Treatment Comparisons",
    "text": "10.8 Step 6: Pairwise Treatment Comparisons\nWe calculate treatment differences and their statistical significance.\n\n# Calculate pairwise comparisons vs. placebo\ntbl2_data = []\ncomparisons = [\n    (\"Xanomeline Low Dose vs. Placebo\", \"TRTP[T.Xanomeline Low Dose]\"),\n    (\"Xanomeline High Dose vs. Placebo\", \"TRTP[T.Xanomeline High Dose]\")\n]\n\nfor comp_name, trt_coef in comparisons:\n    # Extract coefficient estimates\n    coef = model.params[trt_coef]\n    se = model.bse[trt_coef]\n    t_stat = coef / se\n    df = model.df_resid\n    p_value = 2 * (1 - scipy_stats.t.cdf(abs(t_stat), df))\n\n    # Calculate confidence interval\n    ci_lower = coef - scipy_stats.t.ppf(0.975, df) * se\n    ci_upper = coef + scipy_stats.t.ppf(0.975, df) * se\n\n    tbl2_data.append({\n        \"Comparison\": comp_name,\n        \"Estimate\": coef,\n        \"SE\": se,\n        \"CI_Lower\": ci_lower,\n        \"CI_Upper\": ci_upper,\n        \"t_stat\": t_stat,\n        \"p_value\": p_value\n    })\n\ncomparison_df = pl.DataFrame(tbl2_data)\n# Treatment comparisons vs. placebo\ncomparison_df\n\n\nshape: (2, 7)\n\n\n\nComparison\nEstimate\nSE\nCI_Lower\nCI_Upper\nt_stat\np_value\n\n\nstr\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"Xanomeline Low Dose vs. Placeb…\n-0.176769\n0.242635\n-0.654862\n0.301324\n-0.728539\n0.467031\n\n\n\"Xanomeline High Dose vs. Place…\n0.316943\n0.246806\n-0.169369\n0.803256\n1.284179\n0.200383",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ANCOVA efficacy analysis</span>"
    ]
  },
  {
    "objectID": "tlf-efficacy-ancova.html#step-7-prepare-tables-for-rtf-output",
    "href": "tlf-efficacy-ancova.html#step-7-prepare-tables-for-rtf-output",
    "title": "10  ANCOVA efficacy analysis",
    "section": "10.9 Step 7: Prepare Tables for RTF Output",
    "text": "10.9 Step 7: Prepare Tables for RTF Output\nWe format the analysis results into publication-ready tables.\n\n# Table 1: Descriptive Statistics and LS Means\ntbl1_data = []\nfor s, ls in zip(desc_stats, ls_means):\n    tbl1_data.append([\n        s[\"Treatment\"],\n        str(s[\"N_Baseline\"]),\n        f\"{s['Baseline_Mean']:.1f} ({s['Baseline_SD']:.2f})\" if not np.isnan(s['Baseline_Mean']) else \"\",\n        str(s[\"N_Week24\"]),\n        f\"{s['Week24_Mean']:.1f} ({s['Week24_SD']:.2f})\" if not np.isnan(s['Week24_Mean']) else \"\",\n        str(s[\"N_Change\"]),\n        f\"{s['Change_Mean']:.1f} ({s['Change_SD']:.2f})\" if not np.isnan(s['Change_Mean']) else \"\",\n        f\"{ls['LS_Mean']:.2f} ({ls['CI_Lower']:.2f}, {ls['CI_Upper']:.2f})\"\n    ])\n\ntbl1 = pl.DataFrame(tbl1_data, orient=\"row\", schema=[\n    \"Treatment\", \"N_Base\", \"Mean_SD_Base\", \"N_Wk24\", \"Mean_SD_Wk24\",\n    \"N_Chg\", \"Mean_SD_Chg\", \"LS_Mean_CI\"\n])\n\n# Table 1 - Descriptive Statistics and LS Means\ntbl1\n\n\nshape: (3, 8)\n\n\n\nTreatment\nN_Base\nMean_SD_Base\nN_Wk24\nMean_SD_Wk24\nN_Chg\nMean_SD_Chg\nLS_Mean_CI\n\n\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\n\n\n\n\n\"Placebo\"\n\"79\"\n\"5.7 (2.23)\"\n\"79\"\n\"5.6 (1.65)\"\n\"79\"\n\"-0.0 (2.32)\"\n\"0.07 (-0.26, 0.41)\"\n\n\n\"Xanomeline Low Dose\"\n\"79\"\n\"5.4 (0.95)\"\n\"79\"\n\"5.4 (1.06)\"\n\"79\"\n\"-0.1 (1.02)\"\n\"-0.11 (-0.44, 0.23)\"\n\n\n\"Xanomeline High Dose\"\n\"74\"\n\"5.4 (1.37)\"\n\"74\"\n\"5.8 (2.21)\"\n\"74\"\n\"0.4 (1.65)\"\n\"0.39 (0.04, 0.74)\"\n\n\n\n\n\n\n\n# Table 2: Pairwise Comparisons (formatted for output)\ntbl2_formatted = []\nfor row in tbl2_data:\n    tbl2_formatted.append([\n        row[\"Comparison\"],\n        f\"{row['Estimate']:.2f} ({row['CI_Lower']:.2f}, {row['CI_Upper']:.2f})\",\n        f\"{row['p_value']:.4f}\" if row['p_value'] &gt;= 0.0001 else \"&lt;0.0001\"\n    ])\n\ntbl2 = pl.DataFrame(tbl2_formatted, orient=\"row\", schema=[\"Comparison\", \"Diff_CI\", \"P_Value\"])\n\n# Table 2 - Pairwise Comparisons\ntbl2\n\n\nshape: (2, 3)\n\n\n\nComparison\nDiff_CI\nP_Value\n\n\nstr\nstr\nstr\n\n\n\n\n\"Xanomeline Low Dose vs. Placeb…\n\"-0.18 (-0.65, 0.30)\"\n\"0.4670\"\n\n\n\"Xanomeline High Dose vs. Place…\n\"0.32 (-0.17, 0.80)\"\n\"0.2004\"",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ANCOVA efficacy analysis</span>"
    ]
  },
  {
    "objectID": "tlf-efficacy-ancova.html#step-8-create-regulatory-compliant-rtf-document",
    "href": "tlf-efficacy-ancova.html#step-8-create-regulatory-compliant-rtf-document",
    "title": "10  ANCOVA efficacy analysis",
    "section": "10.10 Step 8: Create Regulatory-Compliant RTF Document",
    "text": "10.10 Step 8: Create Regulatory-Compliant RTF Document\nWe generate a comprehensive efficacy table following regulatory submission standards.\n\n# Create comprehensive RTF document with multiple table sections\ndoc_ancova = rtf.RTFDocument(\n    df=[tbl1, tbl2],\n    rtf_title=rtf.RTFTitle(\n        text=[\n            \"Analysis of Covariance (ANCOVA) of Change from Baseline in\",\n            \"Fasting Glucose (mmol/L) at Week 24 (LOCF)\",\n            \"Efficacy Analysis Population\"\n        ]\n    ),\n    rtf_column_header=[\n        # Header for descriptive statistics table\n        [\n            rtf.RTFColumnHeader(\n                text=[\"\", \"Baseline\", \"Week 24 (LOCF)\", \"Change from Baseline\", \"\"],\n                col_rel_width=[3, 2, 2, 3, 2],\n                text_justification=[\"l\", \"c\", \"c\", \"c\", \"c\"],\n                text_format=\"b\"\n            ),\n            rtf.RTFColumnHeader(\n                text=[\n                    \"Treatment Group\",\n                    \"N\", \"Mean (SD)\",\n                    \"N\", \"Mean (SD)\",\n                    \"N\", \"Mean (SD)\",\n                    \"LS Mean (95% CI){^a}\"\n                ],\n                col_rel_width=[3, 0.7, 1.3, 0.7, 1.3, 0.7, 1.3, 2],\n                text_justification=[\"l\"] + [\"c\"] * 7,\n                border_bottom=\"single\",\n                text_format=\"b\"\n            )\n        ],\n        # Header for pairwise comparisons table\n        [\n            rtf.RTFColumnHeader(\n                text=[\n                    \"Pairwise Comparison\",\n                    \"Difference in LS Mean (95% CI){^a}\",\n                    \"p-Value{^b}\"\n                ],\n                col_rel_width=[5, 4, 2],\n                text_justification=[\"l\", \"c\", \"c\"],\n                text_format=\"b\",\n                border_bottom=\"single\"\n            )\n        ]\n    ],\n    rtf_body=[\n        # Body for descriptive statistics\n        rtf.RTFBody(\n            col_rel_width=[3, 0.7, 1.3, 0.7, 1.3, 0.7, 1.3, 2],\n            text_justification=[\"l\"] + [\"c\"] * 7\n        ),\n        # Body for pairwise comparisons\n        rtf.RTFBody(\n            col_rel_width=[5, 4, 2],\n            text_justification=[\"l\", \"c\", \"c\"]\n        )\n    ],\n    rtf_footnote=rtf.RTFFootnote(\n        text=[\n            \"{^a}LS means and differences in LS means are based on an ANCOVA model with treatment and baseline glucose as covariates.\",\n            f\"{{^b}}p-values are from the ANCOVA model testing treatment effects (overall F-test p-value: {model.f_pvalue:.4f}).\",\n            \"LOCF (Last Observation Carried Forward) approach is used for missing Week 24 values.\",\n            \"ANCOVA = Analysis of Covariance; CI = Confidence Interval; LS = Least Squares; SD = Standard Deviation\"\n        ]\n    ),\n    rtf_source=rtf.RTFSource(\n        text=[\n            \"Source: ADLBC Analysis Dataset\",\n            f\"Analysis conducted: {pd.Timestamp.now().strftime('%d%b%Y').upper()}\",\n            \"Statistical software: Python (statsmodels)\"\n        ]\n    )\n)\n\n# Generate RTF file\ndoc_ancova.write_rtf(\"rtf/tlf_efficacy_ancova.rtf\")\n\nrtf/tlf_efficacy_ancova.rtf\n\n\n\n\nPosixPath('pdf/tlf_efficacy_ancova.pdf')",
    "crumbs": [
      "Clinical trial project",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ANCOVA efficacy analysis</span>"
    ]
  },
  {
    "objectID": "pkg-overview.html",
    "href": "pkg-overview.html",
    "title": "11  Packaging overview",
    "section": "",
    "text": "11.1 What is an analysis package\nAn analysis package is a Python package designed specifically to organize analysis scripts and code for a clinical trial project.\nUnlike general-purpose Python packages distributed on PyPI, analysis packages serve as:\nThink of it as combining:",
    "crumbs": [
      "Analysis package",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Packaging overview</span>"
    ]
  },
  {
    "objectID": "pkg-overview.html#what-is-an-analysis-package",
    "href": "pkg-overview.html#what-is-an-analysis-package",
    "title": "11  Packaging overview",
    "section": "",
    "text": "Project containers for clinical trial deliverables\nReproducible environments for analyses\nSubmission-ready structures for regulatory review\n\n\n\nPython package structure (for code organization)\nQuarto project (for report generation)\nRegulatory requirements (for eCTD submission)",
    "crumbs": [
      "Analysis package",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Packaging overview</span>"
    ]
  },
  {
    "objectID": "pkg-overview.html#why-use-an-analysis-package",
    "href": "pkg-overview.html#why-use-an-analysis-package",
    "title": "11  Packaging overview",
    "section": "11.2 Why use an analysis package",
    "text": "11.2 Why use an analysis package\nClinical trial projects have unique needs that standard Python projects may not address:\nRegulatory compliance:\n\nFDA requires submission of analysis programs in ASCII text format\nReviewers must be able to reproduce your results\nDocumentation must explain the analysis process\n\nTeam collaboration:\n\nMultiple statisticians and programmers work on hundreds of tables\nConsistent structure reduces communication overhead\nShared functions avoid code duplication\n\nLong-term maintenance:\n\nAnalysis must be reproducible years later\nEnvironment must be reconstructable\nCode and data provenance must be clear\n\nThe Python package structure addresses these needs systematically.",
    "crumbs": [
      "Analysis package",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Packaging overview</span>"
    ]
  },
  {
    "objectID": "pkg-overview.html#analysis-package-vs-standard-package",
    "href": "pkg-overview.html#analysis-package-vs-standard-package",
    "title": "11  Packaging overview",
    "section": "11.3 Analysis package vs standard package",
    "text": "11.3 Analysis package vs standard package\nPython packages serve different purposes depending on context.\nStandard Python package (for PyPI):\n\nPurpose: Share reusable functionality\nAudience: General Python community\nScope: Generic, broadly applicable functions\nExample: polars, plotnine, rtflite\n\nAnalysis package (for submissions):\n\nPurpose: Organize trial-specific analyses\nAudience: Study team and regulators\nScope: Study-specific tables, listings, figures\nExample: demo001 (DEMO-001 study analysis)\n\n\n\nIn R terms, think of an analysis package like a project-specific R package (e.g., esubdemo) versus a CRAN package (e.g., dplyr).",
    "crumbs": [
      "Analysis package",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Packaging overview</span>"
    ]
  },
  {
    "objectID": "pkg-overview.html#key-components",
    "href": "pkg-overview.html#key-components",
    "title": "11  Packaging overview",
    "section": "11.4 Key components",
    "text": "11.4 Key components\nA typical analysis package contains:\nPython package structure:\n\npyproject.toml: Project metadata and dependencies\nsrc/studyname/: Study-specific Python functions\ntests/: Validation and testing code\nuv.lock: Exact dependency versions\n\nAnalysis content:\n\nanalysis/: Quarto documents for TLFs\ndata/: ADaM datasets (input)\noutput/: Generated tables, listings, figures (output)\n\nDocumentation:\n\nREADME.md: Project overview\n_quarto.yml: Quarto book configuration\n\nThis structure supports the complete lifecycle: development, validation, and submission.",
    "crumbs": [
      "Analysis package",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Packaging overview</span>"
    ]
  },
  {
    "objectID": "pkg-overview.html#demo-project",
    "href": "pkg-overview.html#demo-project",
    "title": "11  Packaging overview",
    "section": "11.5 Demo project",
    "text": "11.5 Demo project\nThis book uses demo-py-esub as the demonstration project.\nThe project shows how to:\n\nOrganize analysis code as a Python package\nGenerate clinical study reports with Quarto\nPrepare deliverables for eCTD submission\n\nClone the project to follow along:\ngit clone https://github.com/elong0527/demo-py-esub.git\ncd demo-py-esub\nThe project generates six TLFs:\n\nDisposition of patients\nStudy population\nBaseline characteristics\nEfficacy analysis (ANCOVA)\nAdverse events summary\nAdverse events (specific)\n\nThese cover the most common clinical reporting scenarios.",
    "crumbs": [
      "Analysis package",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Packaging overview</span>"
    ]
  },
  {
    "objectID": "pkg-overview.html#workflow-overview",
    "href": "pkg-overview.html#workflow-overview",
    "title": "11  Packaging overview",
    "section": "11.6 Workflow overview",
    "text": "11.6 Workflow overview\nThe typical workflow for an analysis package:\n1. Project setup:\n\nInitialize Python package with uv\nConfigure Quarto for report generation\nSet up version control with Git\n\n2. Development:\n\nWrite analysis functions in src/\nCreate Quarto documents in analysis/\nGenerate TLFs in output/\n\n3. Validation:\n\nWrite tests in tests/\nPerform independent review\nVerify outputs match specifications\n\n4. Submission:\n\nPack package into text files with pkglite\nPlace files in eCTD Module 5 structure\nUpdate ADRG with reproduction instructions\n\nThe following chapters detail each stage.",
    "crumbs": [
      "Analysis package",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Packaging overview</span>"
    ]
  },
  {
    "objectID": "pkg-overview.html#benefits-of-this-approach",
    "href": "pkg-overview.html#benefits-of-this-approach",
    "title": "11  Packaging overview",
    "section": "11.7 Benefits of this approach",
    "text": "11.7 Benefits of this approach\nUsing Python packages for clinical analysis provides:\nConsistency:\n\nStandard structure across all projects\nTeam members know where to find code and outputs\nReduces onboarding time for new projects\n\nAutomation:\n\nuv manages dependencies automatically\nQuarto renders all reports in batch\nTesting frameworks verify correctness\n\nReproducibility:\n\nuv.lock ensures exact dependency versions\n.python-version specifies Python version\nRepository snapshots freeze package ecosystem\n\nCompliance:\n\nBuilt-in documentation with docstrings\nTesting infrastructure for validation\nStandard structure simplifies review\n\n\n\n\n\n\n\nImportant\n\n\n\nFor regulatory submissions, reproducibility is not optional. The FDA expects to reconstruct your exact environment and verify your results.",
    "crumbs": [
      "Analysis package",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Packaging overview</span>"
    ]
  },
  {
    "objectID": "pkg-overview.html#whats-next",
    "href": "pkg-overview.html#whats-next",
    "title": "11  Packaging overview",
    "section": "11.8 What’s next",
    "text": "11.8 What’s next\nThe next chapters cover:\n\nPackage structure: Organizing code and content (Chapter 12)\nProject management: Git-centric workflows for collaboration (Chapter 13)\nSubmission overview: eCTD requirements and pkglite (Chapter 14)\nSubmission package: Packing for eCTD Module 5 (Chapter 15)\nSubmission dryrun: Verifying reproducibility (Chapter 16)\n\nWith this foundation, you’re ready to learn how to manage analysis packages effectively.",
    "crumbs": [
      "Analysis package",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Packaging overview</span>"
    ]
  },
  {
    "objectID": "pkg-structure.html",
    "href": "pkg-structure.html",
    "title": "12  Package structure",
    "section": "",
    "text": "12.1 Core principle\nOrganize clinical analysis projects as valid Python packages that leverage the entire Python packaging toolchain (especially uv).\nAdditionally, integrate essential components for clinical reporting:\nThis hybrid structure combines Python package best practices with clinical trial deliverable requirements.",
    "crumbs": [
      "Analysis package",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Package structure</span>"
    ]
  },
  {
    "objectID": "pkg-structure.html#core-principle",
    "href": "pkg-structure.html#core-principle",
    "title": "12  Package structure",
    "section": "",
    "text": "Quarto documents for reproducible analysis\nADaM datasets as inputs\nGenerated TLFs as outputs",
    "crumbs": [
      "Analysis package",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Package structure</span>"
    ]
  },
  {
    "objectID": "pkg-structure.html#complete-example-structure",
    "href": "pkg-structure.html#complete-example-structure",
    "title": "12  Package structure",
    "section": "12.2 Complete example structure",
    "text": "12.2 Complete example structure\nA clinical analysis package requires these essential components:\ndemo-py-esub/\n├── pyproject.toml\n├── .python-version\n├── uv.lock\n├── README.md\n├── .gitignore\n├── _quarto.yml\n├── index.qmd\n├── src/\n│   └── demo001/\n│       ├── __init__.py\n│       ├── utils.py\n│       ├── baseline.py\n│       ├── efficacy.py\n│       ├── population.py\n│       └── safety.py\n├── analysis/\n│   ├── tlf-01-disposition.qmd\n│   ├── tlf-02-population.qmd\n│   ├── tlf-03-baseline.qmd\n│   ├── tlf-04-efficacy-ancova.qmd\n│   ├── tlf-05-ae-summary.qmd\n│   └── tlf-06-specific.qmd\n├── data/\n│   ├── adsl.parquet\n│   ├── adae.parquet\n│   ├── adlbc.parquet\n│   ├── advs.parquet\n│   └── adtte.parquet\n├── output/\n│   ├── tlf-disposition.rtf\n│   ├── tlf-population.rtf\n│   ├── tlf-baseline.rtf\n│   ├── tlf-efficacy-ancova.rtf\n│   ├── tlf-ae-summary.rtf\n│   └── tlf-ae-specific.rtf\n└── tests/\n    ├── __init__.py\n    ├── test_utils.py\n    ├── test_baseline.py\n    └── data/\n        └── adsl_subset.parquet\nThis structure satisfies both Python packaging standards and regulatory submission requirements.\n\n\nIn R terms, this combines:\n\nR package structure (DESCRIPTION, R/, tests/)\nAnalysis project layout (vignettes/, data/, output/)",
    "crumbs": [
      "Analysis package",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Package structure</span>"
    ]
  },
  {
    "objectID": "pkg-structure.html#python-package-components",
    "href": "pkg-structure.html#python-package-components",
    "title": "12  Package structure",
    "section": "12.3 Python package components",
    "text": "12.3 Python package components\nThe Python package portion follows the Python Packaging User Guide.\n\n12.3.1 pyproject.toml\nThe single source of truth for project configuration:\n[project]\nname = \"demo001\"\nversion = \"0.1.0\"\ndescription = \"Analysis package for DEMO-001 study\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.13\"\ndependencies = [\n    \"polars&gt;=1.35.1\",\n    \"plotnine&gt;=0.15.1\",\n    \"rtflite&gt;=1.0.2\",\n]\n\n[dependency-groups]\ndev = [\n    \"pytest&gt;=8.4.2\",\n    \"ruff&gt;=0.14.3\",\n    \"mypy&gt;=1.18.2\",\n]\n\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\nKey sections:\n\n[project]: Package metadata\n[project].dependencies: Runtime dependencies for analysis\n[dependency-groups.dev]: Development tools (testing, linting)\n[build-system]: How to build the package\n\n\n\n12.3.2 .python-version\nSpecifies the exact Python version:\n3.13.9\nCreated by uv python pin 3.13.9.\n\n\n\n\n\n\nImportant\n\n\n\nUse the full MAJOR.MINOR.PATCH version (e.g., 3.13.9), not just 3.13. This prevents drift as new patch versions are released.\n\n\n\n\n12.3.3 uv.lock\nLock file with exact dependency versions:\nversion = 1\nrequires-python = \"&gt;=3.13\"\n\n[[package]]\nname = \"polars\"\nversion = \"1.35.1\"\nsource = { registry = \"https://pypi.org/simple\" }\n...\nThis file is auto-generated by uv sync and uv lock.\nNever edit manually. Commit to version control.\n\n\n12.3.4 src/demo001/\nStudy-specific Python functions go here.\nFollowing the src/ layout (recommended):\nsrc/\n└── demo001/\n    ├── __init__.py         # Package initialization\n    ├── utils.py            # Utility functions\n    ├── baseline.py         # Baseline characteristics\n    ├── efficacy.py         # Efficacy analysis\n    ├── population.py       # Population analysis\n    └── safety.py           # Safety analysis\nWhy src/ layout?\n\nPrevents accidental imports from development directory\nForces proper package installation\nIndustry best practice\n\nEach module contains related functions. For example, utils.py:\n\"\"\"Utility functions for formatting and calculations.\"\"\"\n\ndef fmt_num(x: float, digits: int = 1, width: int = 5) -&gt; str:\n    \"\"\"Format a number with specified digits and width.\n\n    Parameters\n    ----------\n    x : float\n        Number to format\n    digits : int\n        Number of decimal places\n    width : int\n        Total width of formatted string\n\n    Returns\n    -------\n    str\n        Formatted number string\n\n    Examples\n    --------\n    &gt;&gt;&gt; fmt_num(12.345, digits=2, width=6)\n    ' 12.35'\n    \"\"\"\n    return f\"{x:&gt;{width}.{digits}f}\"\nDocument all functions with docstrings following the NumPy docstring standard.\n\n\n12.3.5 tests/\nValidation tests using pytest:\ntests/\n├── __init__.py\n├── test_utils.py           # Test utility functions\n├── test_baseline.py        # Test baseline functions\n└── data/                   # Test data fixtures\n    └── adsl_subset.parquet\nExample test:\n# tests/test_utils.py\nfrom demo001.utils import fmt_num\n\ndef test_fmt_num_basic():\n    \"\"\"Test basic number formatting.\"\"\"\n    assert fmt_num(12.345, digits=2, width=6) == \" 12.35\"\n\ndef test_fmt_num_padding():\n    \"\"\"Test width padding.\"\"\"\n    assert fmt_num(1.2, digits=1, width=5) == \"  1.2\"\nRun tests:\nuv run pytest\n\n\n\n\n\n\nNote\n\n\n\nFor clinical submissions, high test coverage demonstrates code quality. Aim for &gt;80% coverage for critical functions.",
    "crumbs": [
      "Analysis package",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Package structure</span>"
    ]
  },
  {
    "objectID": "pkg-structure.html#quarto-project-components",
    "href": "pkg-structure.html#quarto-project-components",
    "title": "12  Package structure",
    "section": "12.4 Quarto project components",
    "text": "12.4 Quarto project components\nThe Quarto portion enables reproducible report generation.\n\n12.4.1 _quarto.yml\nQuarto project configuration:\nproject:\n  type: book\n\nbook:\n  title: \"DEMO-001 Analysis Results\"\n  chapters:\n    - index.qmd\n    - analysis/tlf-01-disposition.qmd\n    - analysis/tlf-02-population.qmd\n    - analysis/tlf-03-baseline.qmd\n    - analysis/tlf-04-efficacy-ancova.qmd\n    - analysis/tlf-05-ae-summary.qmd\n    - analysis/tlf-06-specific.qmd\n\nformat:\n  html:\n    theme: cosmo\nThis configures Quarto to render all analysis documents as a book.\n\n\n12.4.2 index.qmd\nLanding page for the Quarto book:\n---\ntitle: \"DEMO-001 Clinical Study Report\"\n---\n\n## Overview\n\nThis analysis package contains Tables, Listings, and Figures (TLFs)\nfor the DEMO-001 clinical trial.\n\n## Study Information\n\n- Protocol: DEMO-001\n- Phase: III\n- Indication: [Disease]\n- Primary Endpoint: [Endpoint]\n\n## Analysis Programs\n\nThe following TLFs are included:\n\n- **Disposition**: Patient disposition table\n- **Population**: Analysis population summary\n- **Baseline**: Baseline characteristics\n- **Efficacy**: Primary efficacy analysis (ANCOVA)\n- **AE Summary**: Adverse events summary\n- **AE Specific**: Specific adverse events\n\n\n12.4.3 analysis/\nAnalysis scripts as Quarto documents:\nanalysis/\n├── tlf-01-disposition.qmd\n├── tlf-02-population.qmd\n├── tlf-03-baseline.qmd\n├── tlf-04-efficacy-ancova.qmd\n├── tlf-05-ae-summary.qmd\n└── tlf-06-specific.qmd\nEach .qmd file:\n\nLoads required data\nPerforms analysis\nGenerates formatted output\nExports to RTF for submission\n\nExample structure:\n---\ntitle: \"Table 14.1.1 - Disposition of Patients\"\n---\n\n## Load Data\n\n```{python}\nimport polars as pl\nfrom demo001.utils import fmt_num\n\nadsl = pl.read_parquet(\"data/adsl.parquet\")\n```\n\n## Analysis\n\n```{python}\n# Calculate disposition counts\ndisposition = adsl.group_by(\"TRTA\").agg([\n    pl.len().alias(\"N\"),\n    pl.col(\"EOSSTT\").filter(pl.col(\"EOSSTT\") == \"COMPLETED\").count().alias(\"Completed\")\n])\n```\n\n## Output\n\n```{python}\nfrom rtflite import Table\n# Generate RTF table...\n```\n\n\n\n\n\n\nWarning\n\n\n\nFor final submissions, convert .qmd files to .py scripts. Covered in Chapter 15.",
    "crumbs": [
      "Analysis package",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Package structure</span>"
    ]
  },
  {
    "objectID": "pkg-structure.html#data-and-output-directories",
    "href": "pkg-structure.html#data-and-output-directories",
    "title": "12  Package structure",
    "section": "12.5 Data and output directories",
    "text": "12.5 Data and output directories\n\n12.5.1 data/\nInput datasets in Parquet format:\ndata/\n├── adsl.parquet            # Subject-level analysis dataset\n├── adae.parquet            # Adverse events\n├── adlbc.parquet           # Lab chemistry\n├── advs.parquet            # Vital signs\n└── adtte.parquet           # Time-to-event\nWhy Parquet?\n\nFaster than CSV for large datasets\nPreserves data types\nSmaller file size\nPython ecosystem standard\n\nFor submission, ADaM datasets are converted to SAS .xpt format per FDA requirements.\n\n\n12.5.2 output/\nGenerated TLF outputs:\noutput/\n├── tlf-disposition.rtf\n├── tlf-population.rtf\n├── tlf-baseline.rtf\n├── tlf-efficacy-ancova.rtf\n├── tlf-ae-summary.rtf\n└── tlf-ae-specific.rtf\nRTF files are submission-ready and can be converted to PDF for review.\n\n\nThe output/ directory typically goes in .gitignore since outputs are generated from source code. Commit source, not generated artifacts.",
    "crumbs": [
      "Analysis package",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Package structure</span>"
    ]
  },
  {
    "objectID": "pkg-structure.html#additional-files",
    "href": "pkg-structure.html#additional-files",
    "title": "12  Package structure",
    "section": "12.6 Additional files",
    "text": "12.6 Additional files\n\n12.6.1 .gitignore\nExclude generated files from version control:\n# Python\n__pycache__/\n*.py[cod]\n.venv/\n\n# Quarto\n_book/\n*.html\n\n# Output (generated)\noutput/\n\n# OS\n.DS_Store\nThumbs.db\n\n\n12.6.2 README.md\nProject documentation:\n# demo-py-esub\n\nAnalysis package for DEMO-001 clinical trial.\n\n## Installation\n\n```bash\ngit clone https://github.com/org/demo-py-esub.git\ncd demo-py-esub\nuv sync\n```\n\n## Usage\n\nGenerate all TLFs:\n\n```bash\nquarto render\n```\n\nRun tests:\n\n```bash\nuv run pytest tests/\n```",
    "crumbs": [
      "Analysis package",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Package structure</span>"
    ]
  },
  {
    "objectID": "pkg-structure.html#benefits-of-this-structure",
    "href": "pkg-structure.html#benefits-of-this-structure",
    "title": "12  Package structure",
    "section": "12.7 Benefits of this structure",
    "text": "12.7 Benefits of this structure\nConsistency:\n\nEvery project follows the same layout\nTeam members instantly know where files belong\nReduces cognitive load\n\nReproducibility:\n\nuv.lock pins all dependencies\n.python-version specifies Python version\nQuarto renders from source every time\n\nAutomation:\n\nuv sync restores environment\nquarto render regenerates all outputs\npytest validates all functions\n\nCompliance:\n\nStandard Python package can be built and distributed\nTests provide validation evidence\nDocumentation is built-in\n\n\n\n\n\n\n\nNote\n\n\n\nThe structure scales well. A project with 300 TLFs uses the same layout, just more files in analysis/ and src/.",
    "crumbs": [
      "Analysis package",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Package structure</span>"
    ]
  },
  {
    "objectID": "pkg-structure.html#mixed-language-projects",
    "href": "pkg-structure.html#mixed-language-projects",
    "title": "12  Package structure",
    "section": "12.8 Mixed language projects",
    "text": "12.8 Mixed language projects\nIf your organization uses both R and Python:\nSeparate projects:\nclinical-trial-001/\n├── r-package/              # R-based analyses\n│   ├── DESCRIPTION\n│   ├── R/\n│   └── vignettes/\n├── python-package/         # Python-based analyses\n│   ├── pyproject.toml\n│   ├── src/\n│   └── analysis/\n├── data/                   # Shared ADaM datasets\n└── output/                 # Shared outputs\nWhy separate?\n\nDifferent build systems (devtools vs uv)\nDifferent dependency management (renv vs uv)\nDifferent testing frameworks (testthat vs pytest)\nSimpler to maintain\n\nShare data and outputs, not source code.\n\n\n\n\n\n\nCautionMixing programming languages in a single project is often a mistake\n\n\n\nJohn Carmack noted: “It’s almost always a mistake to mix languages in a single project.”\nMixing languages in a single project increases the complexity of dependency management, testing, and build processes. It can lead to confusion about which tools to use for specific tasks and complicate collaboration among team members who may specialize in different languages.\nKeep Python projects pure Python. Keep R projects pure R. Share data and outputs, not codebases.",
    "crumbs": [
      "Analysis package",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Package structure</span>"
    ]
  },
  {
    "objectID": "pkg-structure.html#whats-next",
    "href": "pkg-structure.html#whats-next",
    "title": "12  Package structure",
    "section": "12.9 What’s next",
    "text": "12.9 What’s next\nYou’ve learned the recommended structure for analysis packages.\nThe next part covers eCTD submission:\n\nRegulatory requirements for program submission\nUsing pkglite to pack Python packages\nCreating submission packages\nVerifying reproducibility with dry runs\n\nWith this structure in place, you’re ready to prepare submission-ready deliverables.",
    "crumbs": [
      "Analysis package",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Package structure</span>"
    ]
  },
  {
    "objectID": "pkg-management.html",
    "href": "pkg-management.html",
    "title": "13  Project management",
    "section": "",
    "text": "13.1 Git-centric workflow\nClinical analysis projects require rigorous tracking and collaboration. A Git-centric workflow provides the foundation for reproducible, auditable work.\nCore principle:\nAll project assets live in version control. Work is tracked through issues, pull requests, and project boards.\nThis applies whether you use:\nThe specific platform matters less than the workflow discipline.",
    "crumbs": [
      "Analysis package",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Project management</span>"
    ]
  },
  {
    "objectID": "pkg-management.html#git-centric-workflow",
    "href": "pkg-management.html#git-centric-workflow",
    "title": "13  Project management",
    "section": "",
    "text": "GitHub Enterprise\nGitLab\nBitbucket\nAzure DevOps",
    "crumbs": [
      "Analysis package",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Project management</span>"
    ]
  },
  {
    "objectID": "pkg-management.html#plain-text-workflow",
    "href": "pkg-management.html#plain-text-workflow",
    "title": "13  Project management",
    "section": "13.2 Plain text workflow",
    "text": "13.2 Plain text workflow\nFavor plain text formats for all project artifacts:\nUse:\n\n.qmd files for analysis scripts (not Jupyter notebooks for final deliverables)\n.md files for documentation\n.toml files for configuration\n.txt files for submission packages\n\nAvoid:\n\n.xlsx files for tracking (no audit trail, merge conflicts)\nBinary formats when text alternatives exist\nProprietary formats that require special tools\n\n\n\n\n\n\n\nNote\n\n\n\nPlain text enables:\n\nClear diff views in pull requests\nMeaningful merge conflict resolution\nSearchable code history\nCommand-line friendly workflows",
    "crumbs": [
      "Analysis package",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Project management</span>"
    ]
  },
  {
    "objectID": "pkg-management.html#project-tracking",
    "href": "pkg-management.html#project-tracking",
    "title": "13  Project management",
    "section": "13.3 Project tracking",
    "text": "13.3 Project tracking\nUse properly labeled issues and pull requests to drive work. Follow Tidyteam code review principles for detailed operational advices.\n\n13.3.1 Issues for requirements\nCreate issues to capture:\n\nIndividual TLF specifications\nBug reports\nFeature requests\nValidation tasks\n\nExample issue template:\n**Title**: Implement Table 14.1.1 - Disposition of Patients\n\n**Description**:\nCreate disposition table following ICH E3 guidelines\n\n**Deliverables**:\n- [ ] Quarto document: analysis/tlf-01-disposition.qmd\n- [ ] Output table: output/tlf-disposition.rtf\n- [ ] Unit tests: tests/test_disposition.py\n\n**Validation**: Independent review required\n\n\n13.3.2 Pull requests for review\nALL code changes must go through pull requests before getting into main:\n\nDeveloper creates feature branch\nImplements changes\nOpens pull request for review\nReviewer validates code and outputs\nChanges merged after approval\n\nThis creates an audit trail of who did what and when.\n\n\n13.3.3 Project boards\nUse Kanban-style project boards to visualize work:\nColumns:\n\nBacklog: Planned work\nIn Progress: Active development\nReview: Awaiting validation\nDone: Completed and validated\n\nExample board:\n\n\n\nBacklog\nIn Progress\nReview\nDone\n\n\n\n\nTable 14.3.5\nTable 14.1.1\nTable 14.2.1\nTable 14.1.2\n\n\nFigure 14.4.1\n\nTable 14.3.1\nTable 14.2.2\n\n\n\nThis makes project status transparent to all stakeholders.\n\n\nGitHub Projects, GitLab Boards, and Jira all support this workflow. Choose based on your organization’s infrastructure.",
    "crumbs": [
      "Analysis package",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Project management</span>"
    ]
  },
  {
    "objectID": "pkg-management.html#development-lifecycle",
    "href": "pkg-management.html#development-lifecycle",
    "title": "13  Project management",
    "section": "13.4 Development lifecycle",
    "text": "13.4 Development lifecycle\nClinical analysis projects follow a structured development lifecycle similar to software development.\n\n13.4.1 Planning\nDefine scope and requirements:\n\nList all TLFs from Statistical Analysis Plan (SAP)\nCreate mock tables/shells\nAssign validation levels (independent review vs double programming)\nSet up validation tracking\n\nCreate a validation tracker (plain text format):\n| TLF | Type | Developer | Dev Status | Reviewer | Review Status |\n|-----|------|-----------|------------|----------|---------------|\n| tlf-01-disposition | Table | Alice | Complete | Bob | In Progress |\n| tlf-02-population | Table | Charlie | In Progress | Diana | Pending |\n\n\n\n\n\n\nImportant\n\n\n\nLock down the Python version and package repository snapshot during planning. Changing these mid-project breaks reproducibility.\n\n\n\n\n13.4.2 Development\nTeam members implement assigned TLFs:\n1. Create feature branch:\ngit checkout -b feature/tlf-01-disposition\n2. Implement analysis:\n\nWrite Quarto document in analysis/\nCreate helper functions in src/ if needed\nGenerate output in output/\n\n3. Self-test:\n\nVerify against mock table\nCheck calculations manually\nRun automated tests\n\n4. Commit and push:\ngit add analysis/tlf-01-disposition.qmd\ngit commit -m \"Implement disposition table (Table 14.1.1)\"\ngit push origin feature/tlf-01-disposition\n5. Open pull request:\nRequest review from assigned validator.\n\n\n13.4.3 Validation\nIndependent reviewers verify deliverables:\nFor tables:\n\nCompare output against specifications\nVerify calculations independently\nCheck formatting requirements\nReview code for errors\n\nFor analysis functions:\n\nWrite unit tests in tests/\nVerify edge cases\nCheck type annotations\nReview docstrings\n\nValidation testing example:\n# tests/test_disposition.py\nimport polars as pl\nfrom demo001.disposition import create_disposition_table\n\ndef test_disposition_counts():\n    \"\"\"Verify disposition table calculations.\"\"\"\n    # Load test data\n    df = pl.read_parquet(\"tests/data/adsl_subset.parquet\")\n\n    # Generate table\n    result = create_disposition_table(df)\n\n    # Verify counts\n    assert result[\"Screened\"][0] == 254\n    assert result[\"Randomized\"][0] == 254\n    assert result[\"Completed\"][0] == 238\nRun tests with pytest:\nuv run pytest\n\n\n13.4.4 Delivery\nProject lead ensures completion:\n1. Run compliance checks:\nuv run ruff check .\nuv run mypy src/\nuv run pytest --cov=demo001\n2. Generate all outputs in batch:\nquarto render\n3. Review validation tracker:\nEnsure all TLFs have completed validation.\n4. Prepare submission package:\nPack for eCTD (covered in Chapter 15).",
    "crumbs": [
      "Analysis package",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Project management</span>"
    ]
  },
  {
    "objectID": "pkg-management.html#agile-practices",
    "href": "pkg-management.html#agile-practices",
    "title": "13  Project management",
    "section": "13.5 Agile practices",
    "text": "13.5 Agile practices\nClinical trials benefit from agile project management:\nIterative development:\n\nWork in 2-week sprints\nDeliver subset of TLFs each sprint\nGet stakeholder feedback early\n\nContinuous integration:\n\nAutomated testing on every push\nCatch errors before review\nMaintain code quality\n\nRegular retrospectives:\n\nWhat went well?\nWhat needs improvement?\nAdjust process accordingly\n\n\n\n\n\n\n\nNote\n\n\n\nAgile doesn’t mean uncontrolled. The validation requirements remain the same. Agile provides faster feedback loops within those constraints.",
    "crumbs": [
      "Analysis package",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Project management</span>"
    ]
  },
  {
    "objectID": "pkg-management.html#automation-with-cicd",
    "href": "pkg-management.html#automation-with-cicd",
    "title": "13  Project management",
    "section": "13.6 Automation with CI/CD",
    "text": "13.6 Automation with CI/CD\nAutomate repetitive tasks using continuous integration:\nGitHub Actions example:\nname: Validation\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v5\n      - uses: astral-sh/setup-uv@v7\n      - run: uv sync\n      - run: uv run pytest tests/\n      - run: uv run ruff check .\n      - run: uv run mypy src/\nThis runs tests automatically on every pull request.\nBenefits:\n\nCatch errors before manual review\nEnsure consistent code quality\nReduce reviewer burden\nDocument test results\n\n\n\nCI/CD is optional but highly recommended for projects with multiple developers.",
    "crumbs": [
      "Analysis package",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Project management</span>"
    ]
  },
  {
    "objectID": "pkg-management.html#collaboration-best-practices",
    "href": "pkg-management.html#collaboration-best-practices",
    "title": "13  Project management",
    "section": "13.7 Collaboration best practices",
    "text": "13.7 Collaboration best practices\nWork as a team:\n\nTake project management training\nUnderstand your role in the lifecycle\nCommunicate blockers early\n\nDesign clean architecture:\n\nSeparate business logic from data processing\nWrite reusable components in src/\nKeep analysis scripts in analysis/ focused\n\nSet capability boundaries:\n\nKnow what your team can deliver\nAvoid complex integrations (e.g., mixing R and Python in same package)\nPrefer simple, robust solutions\n\nContribute to community:\n\nShare reusable components internally\nOpen source when possible\nLearn from others’ approaches",
    "crumbs": [
      "Analysis package",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Project management</span>"
    ]
  },
  {
    "objectID": "pkg-management.html#version-control-discipline",
    "href": "pkg-management.html#version-control-discipline",
    "title": "13  Project management",
    "section": "13.8 Version control discipline",
    "text": "13.8 Version control discipline\nBranch strategy:\n\nmain: Stable, validated code\nfeature/*: Development branches\nhotfix/*: Emergency fixes\n\nCommit messages:\nWrite clear, descriptive commits:\n# Good\ngit commit -m \"Add ANCOVA analysis for primary endpoint (Table 14.2.1)\"\n\n# Bad\ngit commit -m \"Update code\"\nNever commit:\n\nSensitive data\nLarge binary files\nGenerated outputs (use .gitignore)\nTemporary files\n\nAlways commit:\n\nSource code (.py, .qmd)\nConfiguration (pyproject.toml, _quarto.yml)\nDocumentation (README.md)\nTests (tests/*.py)",
    "crumbs": [
      "Analysis package",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Project management</span>"
    ]
  },
  {
    "objectID": "pkg-management.html#whats-next",
    "href": "pkg-management.html#whats-next",
    "title": "13  Project management",
    "section": "13.9 What’s next",
    "text": "13.9 What’s next\nYou’ve learned Git-centric project management workflows.\nThe next chapter covers the detailed package structure:\n\nDirectory layout for analysis packages\nOrganizing code and content\nIntegrating Quarto with Python packages\nEssential configuration files\n\nThese practices ensure consistent, reproducible, collaborative clinical analysis projects.",
    "crumbs": [
      "Analysis package",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Project management</span>"
    ]
  },
  {
    "objectID": "submission-overview.html",
    "href": "submission-overview.html",
    "title": "14  Submission overview",
    "section": "",
    "text": "14.1 Electronic Common Technical Document\nThe electronic Common Technical Document (eCTD) provides a standard format for regulatory submissions.\nThe eCTD organizes submission documents in a defined directory structure:\nFor analysis programs, we focus on Module 5 (clinical study reports).",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Submission overview</span>"
    ]
  },
  {
    "objectID": "submission-overview.html#electronic-common-technical-document",
    "href": "submission-overview.html#electronic-common-technical-document",
    "title": "14  Submission overview",
    "section": "",
    "text": "Module 1: Administrative information and prescribing information\nModule 2: Common Technical Document summaries\nModule 3: Quality (CMC)\nModule 4: Nonclinical study reports\nModule 5: Clinical study reports\n\n\n\n\nFull eCTD specifications are available from the ICH website.",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Submission overview</span>"
    ]
  },
  {
    "objectID": "submission-overview.html#fda-requirements-for-analysis-programs",
    "href": "submission-overview.html#fda-requirements-for-analysis-programs",
    "title": "14  Submission overview",
    "section": "14.2 FDA requirements for analysis programs",
    "text": "14.2 FDA requirements for analysis programs\nThe FDA Study Data Technical Conformance Guide (Section 4.1.2.10) specifies:\n\nSponsors should provide the software programs used to create all ADaM datasets and generate tables and figures associated with primary and secondary efficacy analyses. Furthermore, sponsors should submit software programs used to generate additional information included in Section 14 CLINICAL STUDIES of the Prescribing Information (PI) if applicable. The specific software utilized should be specified in the ADRG. The main purpose of requesting the submission of these programs is to understand the process by which the variables for the respective analyses were created and to confirm the analysis algorithms. Sponsors should submit software programs in ASCII text format; however, executable file extensions should not be used.\n\nKey requirements:\n\nSubmit programs for primary and secondary efficacy analyses\nSpecify software and versions in ADRG\nUse ASCII text format\nNo executable extensions",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Submission overview</span>"
    ]
  },
  {
    "objectID": "submission-overview.html#ectd-module-5-structure",
    "href": "submission-overview.html#ectd-module-5-structure",
    "title": "14  Submission overview",
    "section": "14.3 eCTD Module 5 structure",
    "text": "14.3 eCTD Module 5 structure\nAnalysis datasets and programs are organized under Module 5:\nm5/datasets/&lt;study-id&gt;/analysis/adam/\nWithin the adam/ folder, two directories are critical:\nm5/datasets/&lt;study-id&gt;/analysis/adam/\n├── datasets/\n│   ├── *.xpt                   # ADaM datasets in SAS format\n│   ├── define.xml              # Dataset definitions\n│   ├── adrg.pdf                # Analysis Data Reviewer's Guide\n│   └── analysis-results-metadata.pdf  # Analysis Results Metadata\n└── programs/\n    ├── py0pkgs.txt             # Packed Python packages\n    ├── tlf-01-disposition.txt  # Analysis program 1\n    ├── tlf-02-population.txt   # Analysis program 2\n    └── ...                     # Additional programs",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Submission overview</span>"
    ]
  },
  {
    "objectID": "submission-overview.html#the-ascii-text-requirement",
    "href": "submission-overview.html#the-ascii-text-requirement",
    "title": "14  Submission overview",
    "section": "14.4 The ASCII text requirement",
    "text": "14.4 The ASCII text requirement\nWhy ASCII text?\nPlatform independence:\n\nWorks on any operating system\nNo special software needed to view\nFuture-proof format\n\nReview process:\n\nReviewers can read code without running it\nEasy to search and navigate\nCan copy code snippets for testing\n\nCompliance verification:\n\nPlain text prevents hidden code\nNo macros or embedded executables\nTransparent to automated scanning\n\nThis creates a challenge: how to submit a Python package (which has directory structure, binary files, etc.) as ASCII text files?",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Submission overview</span>"
    ]
  },
  {
    "objectID": "submission-overview.html#the-solution-pkglite-for-python",
    "href": "submission-overview.html#the-solution-pkglite-for-python",
    "title": "14  Submission overview",
    "section": "14.5 The solution: pkglite for Python",
    "text": "14.5 The solution: pkglite for Python\npkglite for Python solves the text file requirement by packing Python projects into portable text files. Key capabilities:\n\nPack entire project directory structure into single text file\nPreserve file paths and metadata\nExclude unnecessary files with .pkgliteignore\nUnpack to restore original structure\nSupport multiple projects in one file\n\n\n\n\n\n\n\nNote\n\n\n\npkglite for Python extends the original pkglite for R with:\n\nSupport for any programming language (not just R)\nContent-based file classification (text vs binary)\nCommand-line interface for automation\n.pkgliteignore configuration support\n\n\n\n\n14.5.1 How pkglite works\nPacking:\n\nScan project directory\nClassify files (text vs binary)\nEncode file paths and contents\nWrite to single .txt file\n\nUnpacking:\n\nRead .txt file\nParse file paths and contents\nRecreate directory structure\nWrite files to disk\n\nThe packed text file follows the Debian Control File (DCF) format, similar to the R package pkglite output.",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Submission overview</span>"
    ]
  },
  {
    "objectID": "submission-overview.html#python-language-considerations",
    "href": "submission-overview.html#python-language-considerations",
    "title": "14  Submission overview",
    "section": "14.6 Python language considerations",
    "text": "14.6 Python language considerations\nAs of the August 30, 2025 FDA guidance update, the eCTD Module 5 specification explicitly allows .zip files “for delivering R packages.”\nHowever, Python (and other languages) are not explicitly mentioned.\nOur approach:\nUse pkglite to pack Python packages into portable text files. This follows the spirit of the FDA guidance:\n\nProvides ASCII text format programs\nEnables reproducibility\nDocuments software versions\nAllows reviewer verification\n\n\n\nWe developed pkglite for Python specifically to enable submission of source projects in any programming language following the same principles.",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Submission overview</span>"
    ]
  },
  {
    "objectID": "submission-overview.html#submission-workflow-overview",
    "href": "submission-overview.html#submission-workflow-overview",
    "title": "14  Submission overview",
    "section": "14.7 Submission workflow overview",
    "text": "14.7 Submission workflow overview\nThe complete submission workflow:\n1. Develop analysis package:\n\nCreate Python package with uv\nWrite analysis code in Quarto documents\nValidate outputs and functions\n\n2. Prepare submission package:\n\nPack Python package with pkglite\nConvert Quarto documents to Python scripts\nPlace files in eCTD Module 5 structure\n\n3. Update documentation:\n\nUpdate ADRG with software versions\nProvide reproduction instructions\nUpdate ARM with program metadata\n\n4. Verify reproducibility:\n\nPerform dry run test\nUnpack and install packages\nReproduce analysis results\n\nThe next chapters detail steps 2-4.",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Submission overview</span>"
    ]
  },
  {
    "objectID": "submission-overview.html#what-goes-in-the-submission",
    "href": "submission-overview.html#what-goes-in-the-submission",
    "title": "14  Submission overview",
    "section": "14.8 What goes in the submission",
    "text": "14.8 What goes in the submission\nPython packages (programs/py0pkgs.txt):\nAll study-specific Python packages. These contain helper functions used across multiple analyses.\nAnalysis programs (programs/tlf-*.txt):\nIndividual analysis scripts. Each generates one or more TLFs.\nADaM datasets (datasets/*.xpt):\nAnalysis datasets in SAS transport format. Required by CDISC standards.\nDocumentation (datasets/adrg.pdf, datasets/analysis-results-metadata.pdf):\nADRG provides:\n\nPython version and package versions\nReproduction instructions\nPlatform requirements\n\nARM provides:\n\nLinks between programs and outputs\nProgram metadata\nAnalysis descriptions",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Submission overview</span>"
    ]
  },
  {
    "objectID": "submission-overview.html#dependencies-and-package-management",
    "href": "submission-overview.html#dependencies-and-package-management",
    "title": "14  Submission overview",
    "section": "14.9 Dependencies and package management",
    "text": "14.9 Dependencies and package management\nPublic packages:\nPackages available on PyPI (e.g., polars, rtflite) do not need submission. Document versions in ADRG.\nProprietary packages:\nInternal packages (e.g., company-specific utilities) should be included if they are:\n\nHosted in private repositories (not public)\nRequired to run the analyses\nNot available to reviewers\n\nPack proprietary packages together with analysis packages using pkglite.\nPython version:\nSpecify the exact Python version in ADRG. Use uv python pin to lock the version in the project.\nPackage snapshots:\nIf your organization uses a package snapshot (similar to Posit Package Manager), provide the snapshot date in ADRG.",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Submission overview</span>"
    ]
  },
  {
    "objectID": "submission-overview.html#platform-considerations",
    "href": "submission-overview.html#platform-considerations",
    "title": "14  Submission overview",
    "section": "14.10 Platform considerations",
    "text": "14.10 Platform considerations\nCross-platform compatibility:\nPython code should work on Windows, macOS, and Linux.\nAvoid:\n\nPlatform-specific paths (C:\\ vs /usr/)\nPlatform-specific system calls\nBinary dependencies that require compilation\n\nUse:\n\npathlib for path handling\nPure Python packages when possible\nWheels for platform-independent distribution\n\nExternal dependencies:\nMinimize external dependencies beyond Python packages.\nIf required (e.g., system libraries), document clearly in ADRG.\n\n\n\n\n\n\nWarning\n\n\n\nEach external dependency increases the complexity of environment recreation. Keep it simple.",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Submission overview</span>"
    ]
  },
  {
    "objectID": "submission-overview.html#next-steps",
    "href": "submission-overview.html#next-steps",
    "title": "14  Submission overview",
    "section": "14.11 Next steps",
    "text": "14.11 Next steps\nThe following chapters provide detailed instructions for:\n\nSubmission package (Chapter 15): Using pkglite to pack Python packages and analysis programs. Converting Quarto documents to Python scripts. Organizing files in eCTD structure.\nSubmission dryrun (Chapter 16): Simulating the reviewer experience. Unpacking and installing packages. Reproducing analysis results.\n\nWith this foundation, you’re ready to prepare your first Python submission package.",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Submission overview</span>"
    ]
  },
  {
    "objectID": "submission-package.html",
    "href": "submission-package.html",
    "title": "15  Submission package",
    "section": "",
    "text": "15.1 Prerequisites\nThis chapter uses two demo repositories:\nAnalysis project: demo-py-esub\nSubmission package: demo-py-ectd\nWe assume paths demo-py-esub/ and demo-py-ectd/ below.\nInstall pkglite:\npkglite is not needed as a project dependency. Use uvx to run without installation:\nOr install globally with pipx:",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Submission package</span>"
    ]
  },
  {
    "objectID": "submission-package.html#prerequisites",
    "href": "submission-package.html#prerequisites",
    "title": "15  Submission package",
    "section": "",
    "text": "git clone https://github.com/elong0527/demo-py-esub.git\n\ngit clone https://github.com/elong0527/demo-py-ectd.git\n\n\n\nuvx pkglite --help\n\npipx install pkglite\n\n\n\n\n\n\nNote\n\n\n\nuvx and pipx both run command-line tools in isolated environments. Use uvx for one-off commands or pipx for frequently used tools.",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Submission package</span>"
    ]
  },
  {
    "objectID": "submission-package.html#the-whole-game",
    "href": "submission-package.html#the-whole-game",
    "title": "15  Submission package",
    "section": "15.2 The whole game",
    "text": "15.2 The whole game\nThe eCTD Module 5 structure for Python submissions:\ndemo-py-ectd/m5/datasets/ectddemo/analysis/adam/\n├── datasets/\n│   ├── adsl.xpt\n│   ├── adae.xpt\n│   ├── ...\n│   ├── adrg.pdf\n│   ├── analysis-results-metadata.pdf\n│   ├── define.xml\n│   └── define2-0-0.xsl\n└── programs/\n    ├── py0pkgs.txt\n    ├── tlf-01-disposition.txt\n    ├── tlf-02-population.txt\n    ├── tlf-03-baseline.txt\n    ├── tlf-04-efficacy.txt\n    ├── tlf-05-ae-summary.txt\n    └── tlf-06-ae-specific.txt\ndatasets/ folder:\n\nADaM datasets in .xpt format (SAS transport)\ndefine.xml metadata (created by Pinnacle 21 or similar tools)\nADRG and ARM documentation\n\nprograms/ folder:\n\npy0pkgs.txt: Packed Python package(s)\ntlf-*.txt: Individual analysis programs\n\nAll files in programs/ must be ASCII text files.\n\n\n\n\n\n\nImportant\n\n\n\nFile naming conventions:\n\nUse lowercase letters only\nNo underscores or special characters\nUse hyphens for separators\nNo executable extensions (.py, .exe)",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Submission package</span>"
    ]
  },
  {
    "objectID": "submission-package.html#packing-python-packages-with-py-pkglite",
    "href": "submission-package.html#packing-python-packages-with-py-pkglite",
    "title": "15  Submission package",
    "section": "15.3 Packing Python packages with py-pkglite",
    "text": "15.3 Packing Python packages with py-pkglite\n\n15.3.1 Create .pkgliteignore\nFirst, create a .pkgliteignore file to exclude unnecessary files:\nuvx pkglite use demo-py-esub/\n✓ Created .pkgliteignore in /home/user/demo-py-esub\nThis generates demo-py-esub/.pkgliteignore with default exclusions:\n# Git\n.git/\n\n# OS\n.DS_Store\nThumbs.db\n\n# Python\n__pycache__/\n*.py[cod]\n.venv/\n\n# R\n.Rproj.user/\n.Rhistory\n.RData\n.Ruserdata\n\n# Quarto\n.quarto/\nThe .pkgliteignore file follows gitignore syntax.\nWhat to exclude:\n\nGenerated files (__pycache__, .venv)\nData and outputs (submitted separately)\nVersion control metadata\nQuarto book content (if converted to scripts separately)\n\nWhat to include:\n\nSource code (src/)\nConfiguration (pyproject.toml, uv.lock, .python-version)\nTests (tests/)\nEssential documentation for package installation\n\n\n\npkglite uses content-based file classification. Text files are packed directly. Binary files trigger warnings.\n\n\n15.3.2 Pack the package\nPack the analysis package into a text file:\nuvx pkglite pack demo-py-esub/ \\\n  -o demo-py-ectd/m5/datasets/ectddemo/analysis/adam/programs/py0pkgs.txt\nPacking demo-py-esub\nReading _quarto.yml\nReading uv.lock\nReading .pkgliteignore\nReading pyproject.toml\nReading index.qmd\nReading README.md\nReading .gitignore\nReading .python-version\nReading analysis/tlf-05-ae-summary.qmd\nReading analysis/tlf-02-population.qmd\nReading analysis/tlf-03-baseline.qmd\nReading analysis/.gitignore\nReading analysis/tlf-06-specific.qmd\nReading analysis/tlf-01-disposition.qmd\nReading analysis/tlf-04-efficacy-ancova.qmd\nReading tests/test_utils.py\nReading tests/__init__.py\nReading output/tlf_ae_specific.rtf\nReading output/tlf_baseline.rtf\nReading output/tlf_population.rtf\nReading output/tlf_disposition.rtf\nReading output/tlf_ae_summary.rtf\nReading output/tlf_efficacy_ancova.rtf\nReading .github/.gitignore\nReading .github/workflows/quarto-publish.yml\nReading data/adae.parquet\nReading data/adlbhy.parquet\nReading data/adsl.parquet\nReading data/adtte.parquet\nReading data/adlbc.parquet\nReading data/adlbh.parquet\nReading data/advs.parquet\nReading src/demo001/baseline.py\nReading src/demo001/population.py\nReading src/demo001/__init__.py\nReading src/demo001/utils.py\nReading src/demo001/safety.py\nReading src/demo001/efficacy.py\n✓ Packed 1 packages into /home/user/demo-py-ectd/m5/datasets/ectddemo/analysis/adam/programs/py0pkgs.txt\nThis creates a single text file containing:\n\npyproject.toml\n.python-version\nuv.lock\nAll files in src/\nAll files in tests/\n\n\n\n15.3.3 Inspect the packed file\nView the first few lines:\nhead -n 20 demo-py-ectd/m5/datasets/ectddemo/analysis/adam/programs/py0pkgs.txt\n# Generated by py-pkglite: do not edit by hand\n# Use `pkglite unpack` to restore the packages\n\nPackage: demo-py-esub\nFile: _quarto.yml\nFormat: text\nContent:\n  project:\n    type: book\n\n  book:\n    title: \"DEMO-001 Analysis Results\"\n    chapters:\n      - index.qmd\n      - analysis/tlf-01-disposition.qmd\n      - analysis/tlf-02-population.qmd\n      - analysis/tlf-03-baseline.qmd\n      - analysis/tlf-04-efficacy-ancova.qmd\n      - analysis/tlf-05-ae-summary.qmd\n      - analysis/tlf-06-specific.qmd\n\n\n15.3.4 Packing multiple packages\nIf you have dependencies in private repositories:\nuvx pkglite pack internal-utils/ demo-py-esub/ \\\n  -o demo-py-ectd/m5/datasets/ectddemo/analysis/adam/programs/py0pkgs.txt\nPackages are packed in order specified. Always pack dependencies first (low-level before high-level).\n\n\n\n\n\n\nNote\n\n\n\nUnpacking will restore packages in the same order. Depending on how you reinstall them, the order may matter.",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Submission package</span>"
    ]
  },
  {
    "objectID": "submission-package.html#converting-quarto-to-python-scripts",
    "href": "submission-package.html#converting-quarto-to-python-scripts",
    "title": "15  Submission package",
    "section": "15.4 Converting Quarto to Python scripts",
    "text": "15.4 Converting Quarto to Python scripts\nAnalysis programs must be plain Python scripts, not Quarto documents.\n\n15.4.1 The conversion workflow\nFor each .qmd file in analysis/:\n\nRender to verify it works\nConvert .qmd to .ipynb (Jupyter notebook)\nConvert .ipynb to .py (Python script)\nClean up comments and formatting\nSave as .txt file\n\n\n\n15.4.2 Automated conversion script\nThis shell script automates the conversion:\n#!/bin/bash\n\ncd demo-py-esub/\nuv sync\nsource .venv/bin/activate\n\nconvert_analysis() {\n    local analysis_name=$1\n    local analysis_path=\"analysis/$analysis_name.qmd\"\n    local output_path=\"$HOME/demo-py-ectd/m5/datasets/ectddemo/analysis/adam/programs\"\n\n    # Render .qmd to verify it works\n    quarto render \"$analysis_path\" --quiet\n\n    # Convert .qmd to .ipynb\n    quarto convert \"$analysis_path\"\n\n    # Convert .ipynb to .py using nbconvert\n    uvx --from nbconvert jupyter-nbconvert \\\n        --to python \"analysis/$analysis_name.ipynb\" \\\n        --output \"$output_path/$analysis_name.py\"\n\n    # Remove all comments (lines starting with #)\n    awk '!/^#/' \"$output_path/$analysis_name.py\" &gt; temp && \\\n        mv temp \"$output_path/$analysis_name.py\"\n\n    # Consolidate consecutive blank lines\n    awk 'NF {p = 0} !NF {p++} p &lt; 2' \"$output_path/$analysis_name.py\" &gt; temp && \\\n        mv temp \"$output_path/$analysis_name.py\"\n\n    # Clean up intermediate files\n    rm \"analysis/$analysis_name.ipynb\"\n\n    # Format with ruff\n    uvx ruff format \"$output_path/$analysis_name.py\"\n\n    # Rename .py to .txt (no executable extension)\n    mv \"$output_path/$analysis_name.py\" \"$output_path/$analysis_name.txt\"\n}\n\n# Convert all analysis files\nfor qmd_file in analysis/*.qmd; do\n    analysis=$(basename \"$qmd_file\" .qmd)\n    convert_analysis \"$analysis\"\ndone\nSave as convert_analyses.sh and run:\nchmod +x convert_analyses.sh\n./convert_analyses.sh\n\n\n15.4.3 Add reviewer instructions\nOptionally, add a header to each .txt file:\n# Note to Reviewer\n#\n# To rerun this analysis program, please refer to the ADRG appendix.\nThis helps reviewers understand how to execute the code.\nAutomated insertion:\nheader='# Note to Reviewer\n#\n# To rerun this analysis program, please refer to the ADRG appendix.\n#\n'\n\nadd_header() {\n    local file=$1\n    echo \"$header\" | cat - \"$file\" &gt; temp && mv temp \"$file\"\n}\n\nfor txt_file in demo-py-ectd/m5/datasets/ectddemo/analysis/adam/programs/tlf-*.txt; do\n    add_header \"$txt_file\"\ndone",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Submission package</span>"
    ]
  },
  {
    "objectID": "submission-package.html#verifying-ascii-compliance",
    "href": "submission-package.html#verifying-ascii-compliance",
    "title": "15  Submission package",
    "section": "15.5 Verifying ASCII compliance",
    "text": "15.5 Verifying ASCII compliance\nNon-ASCII characters (curly quotes, em dashes, Unicode symbols) will cause submission issues. Always verify before submission.\nCurrently, there is no built-in py-pkglite utility to check for ASCII compliance, so contributions are welcome! An example verification script is available in rtflite: verify_ascii.py.",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Submission package</span>"
    ]
  },
  {
    "objectID": "submission-package.html#compliance-checklist",
    "href": "submission-package.html#compliance-checklist",
    "title": "15  Submission package",
    "section": "15.6 Compliance checklist",
    "text": "15.6 Compliance checklist\nBefore finalizing the submission package:\nFile naming:\n\n- [ ] All filenames are lowercase\n- [ ] No underscores or special characters\n- [ ] No `.py` extensions (use `.txt`)\n\nContent:\n\n- [ ] All `.txt` files are ASCII compliant\n- [ ] Python package unpacks correctly\n- [ ] Analysis programs run without errors\n\nStructure:\n\n- [ ] Files in correct eCTD Module 5 directories\n- [ ] `py0pkgs.txt` in `programs/`\n- [ ] Analysis programs in `programs/`\n- [ ] ADaM datasets in `datasets/`\n\nDocumentation:\n\n- [ ] ADRG includes Python version\n- [ ] ADRG includes package versions\n- [ ] ADRG includes reproduction instructions\n- [ ] ARM links programs to outputs",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Submission package</span>"
    ]
  },
  {
    "objectID": "submission-package.html#updating-adrg",
    "href": "submission-package.html#updating-adrg",
    "title": "15  Submission package",
    "section": "15.7 Updating ADRG",
    "text": "15.7 Updating ADRG\nThe ADRG must document the Python environment and provide reproduction instructions.\n\n15.7.1 Section: Macro Programs\nExample content:\n7.X Macro Programs\n\nSubmitted Python programs follow the naming pattern `tlf-##-*.txt`.\nAll study-specific Python functions are saved in the `py0pkgs.txt` file.\n\nThe recommended steps to unpack and use these functions are described in the Appendix.\n\nThe table below contains the software version and program metadata:\n\n**Analysis programs table:**\n\n| Program Name | Output Table | Title |\n|--------------|--------------|-------|\n| tlf-01-disposition.txt | Table 14.1.1 | Disposition of Patients |\n| tlf-02-population.txt | Table 14.1.2 | Analysis Population |\n| tlf-03-baseline.txt | Table 14.1.3 | Baseline Characteristics |\n| tlf-04-efficacy.txt | Table 14.2.1 | Efficacy Analysis (ANCOVA) |\n| tlf-05-ae-summary.txt | Table 14.3.1 | Adverse Events Summary |\n| tlf-06-ae-specific.txt | Table 14.3.2 | Specific Adverse Events |\n\n**Python environment table:**\n\n| Software | Version | Description |\n|----------|---------|-------------|\n| Python | 3.13.9 | Programming language |\n| uv | 0.5.18 | Package manager |\n\n**Python packages table:**\n\n| Package | Version | Description |\n|---------|---------|-------------|\n| polars | 1.35.1 | Data manipulation |\n| plotnine | 0.15.1 | Data visualization |\n| rtflite | 1.0.2 | RTF table generation |\n| statsmodels | 0.14.0 | Statistical models |\n\n**Proprietary packages table:**\n\n| Package | Version | Description |\n|---------|---------|-------------|\n| demo001 | 0.1.0 | DEMO-001 study analysis functions |\n\n\nPackage versions can be extracted from uv.lock or by running uv pip list.\n\n\n15.7.2 Appendix: Reproduction instructions\nProvide step-by-step instructions for reviewers:\nAppendix: Instructions to Execute Analysis Programs\n\n1. Install uv\n\nFollow instructions at https://docs.astral.sh/uv/getting-started/installation/\n\n2. Create working directory\n\nCreate a temporary directory (e.g., `C:/tempwork/` on Windows).\nCopy all files from `m5/datasets/ectddemo/analysis/adam/` to this directory.\n\n3. Unpack and install Python packages\n\nNavigate to the working directory and run:\n\n```bash\nuvx pkglite unpack programs/py0pkgs.txt -o .\n```\n\nThis restores the package structure in the current directory.\n\nInstall the package:\n\n```bash\ncd demo-py-esub\nuv sync\n```\n\nThis installs all dependencies and the demo001 package.\n\n4. Copy data to the correct location\n\nEnsure the `datasets/` folder with ADaM datasets is in the working directory.\n\n5. Execute analysis programs\n\nRun each program in order:\n\n```bash\ncd demo-py-esub\nsource .venv/bin/activate    # macOS/Linux\n# or .venv\\Scripts\\activate  # Windows\n\npython ../programs/tlf-01-disposition.txt\npython ../programs/tlf-02-population.txt\npython ../programs/tlf-03-baseline.txt\npython ../programs/tlf-04-efficacy.txt\npython ../programs/tlf-05-ae-summary.txt\npython ../programs/tlf-06-ae-specific.txt\n```\n\nEach program generates RTF output in the specified output directory.\n\n\n\n\n\n\nNote\n\n\n\nTailor the instructions to your organization’s environment. Include any special configuration (e.g., proxy settings, internal package indexes).",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Submission package</span>"
    ]
  },
  {
    "objectID": "submission-package.html#updating-arm",
    "href": "submission-package.html#updating-arm",
    "title": "15  Submission package",
    "section": "15.8 Updating ARM",
    "text": "15.8 Updating ARM\nThe Analysis Results Metadata (ARM) documents the relationship between programs and outputs.\n\n15.8.1 Section 2: Analysis Results Metadata Summary\nExample table:\n\n\n\n\n\n\n\n\n\n\nTable Reference\nTable Title\nProgramming Language\nProgram Name\nInput Files\n\n\n\n\nTable 14.1.1\nDisposition of Patients\nPython\ntlf-01-disposition.txt\nadsl.xpt\n\n\nTable 14.1.2\nAnalysis Population\nPython\ntlf-02-population.txt\nadsl.xpt\n\n\nTable 14.1.3\nBaseline Characteristics\nPython\ntlf-03-baseline.txt\nadsl.xpt\n\n\n\n\n\n15.8.2 Section 3: Analysis Results Metadata Details\nFor each table, provide:\nTable Reference: Table 14.1.1\nAnalysis Result: Disposition counts by treatment group\nAnalysis Reason: Describe study population\nAnalysis Purpose: Primary\nProgramming Statements: (Python version 3.13.9), [programs/tlf-01-disposition.txt]",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Submission package</span>"
    ]
  },
  {
    "objectID": "submission-package.html#testing-the-submission-package",
    "href": "submission-package.html#testing-the-submission-package",
    "title": "15  Submission package",
    "section": "15.9 Testing the submission package",
    "text": "15.9 Testing the submission package\nBefore finalizing, test the complete workflow:\n\nUnpack py0pkgs.txt in a clean directory\nInstall packages per ADRG instructions\nRun each analysis program\nVerify outputs match original results\n\nThis is covered in detail in Chapter 16.",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Submission package</span>"
    ]
  },
  {
    "objectID": "submission-package.html#whats-next",
    "href": "submission-package.html#whats-next",
    "title": "15  Submission package",
    "section": "15.10 What’s next",
    "text": "15.10 What’s next\nYou’ve learned how to prepare Python submission packages.\nThe next chapter covers dry run testing:\n\nSimulating the reviewer workflow\nUnpacking and installing from text files\nReproducing analysis results\nVerifying compliance\n\nDry run testing ensures your submission package works correctly.",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Submission package</span>"
    ]
  },
  {
    "objectID": "submission-dryrun.html",
    "href": "submission-dryrun.html",
    "title": "16  Submission dryrun",
    "section": "",
    "text": "16.1 Why dry run testing\nBefore submitting to regulatory agencies, you must verify that reviewers can reproduce your analysis.\nA dry run test simulates the reviewer workflow:\nThis catches issues before submission:",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Submission dryrun</span>"
    ]
  },
  {
    "objectID": "submission-dryrun.html#why-dry-run-testing",
    "href": "submission-dryrun.html#why-dry-run-testing",
    "title": "16  Submission dryrun",
    "section": "",
    "text": "Start with a clean environment\nFollow ADRG instructions exactly\nUnpack and install packages\nRun analysis programs\nVerify outputs match original results\n\n\n\nMissing dependencies\nIncorrect file paths\nPlatform-specific code\nDocumentation gaps\nVersion mismatches\n\n\n\n\n\n\n\nImportant\n\n\n\nDry run testing is not optional. It’s the only way to ensure your submission package actually works.",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Submission dryrun</span>"
    ]
  },
  {
    "objectID": "submission-dryrun.html#prerequisites",
    "href": "submission-dryrun.html#prerequisites",
    "title": "16  Submission dryrun",
    "section": "16.2 Prerequisites",
    "text": "16.2 Prerequisites\nYou need the eCTD submission package prepared in Chapter 15:\ngit clone https://github.com/elong0527/demo-py-ectd.git\ncd demo-py-ectd\nThe structure:\ndemo-py-ectd/m5/datasets/ectddemo/analysis/adam/\n├── datasets/\n│   ├── *.xpt\n│   ├── adrg.pdf\n│   └── analysis-results-metadata.pdf\n└── programs/\n    ├── py0pkgs.txt\n    └── tlf-*.txt",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Submission dryrun</span>"
    ]
  },
  {
    "objectID": "submission-dryrun.html#setting-up-the-test-environment",
    "href": "submission-dryrun.html#setting-up-the-test-environment",
    "title": "16  Submission dryrun",
    "section": "16.3 Setting up the test environment",
    "text": "16.3 Setting up the test environment\n\n16.3.1 Create a clean directory\nSimulate a reviewer’s fresh environment:\n# Create temporary directory\nmkdir ~/dryrun-test\ncd ~/dryrun-test\n\n# Copy submission materials\ncp -r ~/demo-py-ectd/m5/datasets/ectddemo/analysis/adam/* .\n\n# Verify structure\nls -R\nYou should see:\ndatasets programs\n\n./datasets:\nadadas.xpt      adlbc.xpt       adlbhpv.xpt     adrg.pdf        advs.xpt\nadae.xpt        adlbcpv.xpt     adlbhy.xpt      adsl.xpt        define.xml\nadcibc.xpt      adlbh.xpt       adnpix.xpt      adtte.xpt       define2-0-0.xsl\n\n./programs:\npy0pkgs.txt                tlf-03-baseline.txt        tlf-06-specific.txt\ntlf-01-disposition.txt     tlf-04-efficacy-ancova.txt\ntlf-02-population.txt      tlf-05-ae-summary.txt\n\n\n\n\n\n\nNote\n\n\n\nUse a completely separate directory, not your development environment. This ensures you are testing from a clean state.\n\n\n\n\n16.3.2 Install uv\nFollow the ADRG instructions exactly.\n# Verify\nuv --version\nuv 0.9.7 (0adb44480 2025-10-30)",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Submission dryrun</span>"
    ]
  },
  {
    "objectID": "submission-dryrun.html#unpacking-the-python-package",
    "href": "submission-dryrun.html#unpacking-the-python-package",
    "title": "16  Submission dryrun",
    "section": "16.4 Unpacking the Python package",
    "text": "16.4 Unpacking the Python package\n\n16.4.1 Unpack with py-pkglite\nUse pkglite to restore the package structure:\nuvx pkglite unpack programs/py0pkgs.txt -o .\nUnpacking demo-py-esub\nWriting _quarto.yml\nWriting uv.lock\nWriting .pkgliteignore\nWriting pyproject.toml\nWriting index.qmd\nWriting README.md\nWriting .gitignore\nWriting .python-version\nWriting analysis/tlf-05-ae-summary.qmd\nWriting analysis/tlf-02-population.qmd\nWriting analysis/tlf-03-baseline.qmd\nWriting analysis/.gitignore\nWriting analysis/tlf-06-specific.qmd\nWriting analysis/tlf-01-disposition.qmd\nWriting analysis/tlf-04-efficacy-ancova.qmd\nWriting tests/test_utils.py\nWriting tests/__init__.py\nWriting output/tlf_ae_specific.rtf\nWriting output/tlf_baseline.rtf\nWriting output/tlf_population.rtf\nWriting output/tlf_disposition.rtf\nWriting output/tlf_ae_summary.rtf\nWriting output/tlf_efficacy_ancova.rtf\nWriting .github/.gitignore\nWriting .github/workflows/quarto-publish.yml\nWriting data/adae.parquet\nWriting data/adlbhy.parquet\nWriting data/adsl.parquet\nWriting data/adtte.parquet\nWriting data/adlbc.parquet\nWriting data/adlbh.parquet\nWriting data/advs.parquet\nWriting src/demo001/baseline.py\nWriting src/demo001/population.py\nWriting src/demo001/__init__.py\nWriting src/demo001/utils.py\nWriting src/demo001/safety.py\nWriting src/demo001/efficacy.py\n✓ Unpacked 1 packages from programs/py0pkgs.txt into .\nThis should create:\ndemo-py-esub\n├── _quarto.yml\n├── analysis\n│   ├── tlf-01-disposition.qmd\n│   ├── tlf-02-population.qmd\n│   ├── tlf-03-baseline.qmd\n│   ├── tlf-04-efficacy-ancova.qmd\n│   ├── tlf-05-ae-summary.qmd\n│   └── tlf-06-specific.qmd\n├── data\n│   ├── adae.parquet\n│   ├── adlbc.parquet\n│   ├── adlbh.parquet\n...\nVerify the structure:\nls -la demo-py-esub/\nOr:\ntree demo-py-esub/\n\n\nThe unpacked directory name comes from the packed package. It should match the original analysis project name.",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Submission dryrun</span>"
    ]
  },
  {
    "objectID": "submission-dryrun.html#installing-dependencies",
    "href": "submission-dryrun.html#installing-dependencies",
    "title": "16  Submission dryrun",
    "section": "16.5 Installing dependencies",
    "text": "16.5 Installing dependencies\n\n16.5.1 Sync environment\nNavigate to the unpacked package and install:\ncd demo-py-esub\nuv sync\nThis creates:\n\n.venv/ virtual environment\nInstalls all dependencies from uv.lock\nInstalls the demo001 package in editable mode\n\n\n\n16.5.2 Verify installation\nCheck Python version in virtual environment:\nsource .venv/bin/activate  # macOS/Linux\n# .venv\\Scripts\\activate   # Windows\n\npython --version\nList installed packages:\nuv pip list\nVerify the demo001 package:\npython -c \"import demo001; print(demo001.__version__)\"\nThe output should be 0.1.0.\n\n\n\n\n\n\nNote\n\n\n\nIf any imports fail, check:\n\nPython version matches .python-version\nAll dependencies installed from uv.lock\nPackage installed in editable mode",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Submission dryrun</span>"
    ]
  },
  {
    "objectID": "submission-dryrun.html#running-analysis-programs",
    "href": "submission-dryrun.html#running-analysis-programs",
    "title": "16  Submission dryrun",
    "section": "16.6 Running analysis programs",
    "text": "16.6 Running analysis programs\n\n16.6.1 Execute programs\nRun each analysis program:\n# Run first program\npython ../programs/tlf-01-disposition.txt\nYou should see:\n/home/user/dryrun-test/demo-py-esub/output/tlf_disposition.rtf\nThe program should:\n\nLoad data from ../datasets/\nPerform analysis\nGenerate RTF output in output/\n\nRun remaining programs:\npython ../programs/tlf-02-population.txt\npython ../programs/tlf-03-baseline.txt\npython ../programs/tlf-04-efficacy-ancova.txt\npython ../programs/tlf-05-ae-summary.txt\npython ../programs/tlf-06-specific.txt\n\n\n16.6.2 Verify outputs\nCheck that all RTF files were created:\nls -lh output/\ntotal 456\n-rw-r--r--@ 1 user  staff   170K Nov  6 21:59 tlf_ae_specific.rtf\n-rw-r--r--@ 1 user  staff   9.6K Nov  6 21:59 tlf_ae_summary.rtf\n-rw-r--r--@ 1 user  staff   7.6K Nov  6 21:58 tlf_baseline.rtf\n-rw-r--r--@ 1 user  staff    14K Nov  6 21:57 tlf_disposition.rtf\n-rw-r--r--@ 1 user  staff   8.7K Nov  6 21:59 tlf_efficacy_ancova.rtf\n-rw-r--r--@ 1 user  staff   4.0K Nov  6 21:58 tlf_population.rtf",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Submission dryrun</span>"
    ]
  },
  {
    "objectID": "submission-dryrun.html#comparing-outputs",
    "href": "submission-dryrun.html#comparing-outputs",
    "title": "16  Submission dryrun",
    "section": "16.7 Comparing outputs",
    "text": "16.7 Comparing outputs\n\n16.7.1 Manual comparison\nOpen RTF files in a word processor and compare with original outputs, check if:\n\nTable structure matches\nNumbers are identical\nFormatting is correct\nHeaders and footers present\n\nSince RTF files are plaintext files, you can also automate this comparison with diff-based workflows.",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Submission dryrun</span>"
    ]
  },
  {
    "objectID": "submission-dryrun.html#testing-checklist",
    "href": "submission-dryrun.html#testing-checklist",
    "title": "16  Submission dryrun",
    "section": "16.8 Testing checklist",
    "text": "16.8 Testing checklist\nComplete dry run test checklist:\nEnvironment setup:\n\n- [ ] Clean directory created\n- [ ] Correct Python version installed\n- [ ] uv installed and working\n\nPackage restoration:\n\n- [ ] `py0pkgs.txt` unpacked successfully\n- [ ] Package structure restored\n- [ ] All files present\n\nDependency installation:\n\n- [ ] `uv sync` completed without errors\n- [ ] All packages installed\n- [ ] Correct package versions\n\nProgram execution:\n\n- [ ] All analysis programs run without errors\n- [ ] RTF outputs generated\n- [ ] No missing data errors\n\nOutput verification:\n\n- [ ] All expected outputs present\n- [ ] Numbers match original results\n- [ ] Tables formatted correctly\n- [ ] No corruption or errors in RTF files\n\nDocumentation:\n\n- [ ] ADRG instructions followed successfully\n- [ ] No undocumented steps required\n- [ ] Instructions are clear and complete\nFor projects with many programs, you can automate the dry run with a shell script. We will leave that as an exercise for the reader.",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Submission dryrun</span>"
    ]
  },
  {
    "objectID": "submission-dryrun.html#best-practices",
    "href": "submission-dryrun.html#best-practices",
    "title": "16  Submission dryrun",
    "section": "16.9 Best practices",
    "text": "16.9 Best practices\nTest early, test often:\nDon’t wait until the last minute. Run dry run tests throughout development.\nTest on fresh systems:\nUse virtual machines or Docker containers for truly clean environments.\nDocument everything:\nRecord commands, outputs, and any issues encountered.\nVersion control dry run scripts:\nKeep test scripts in version control alongside analysis code.\nAutomate where possible:\nAutomated tests are faster and more reliable than manual testing.\nTest on reviewer platforms:\nIf you know reviewers use Windows, test on Windows.",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Submission dryrun</span>"
    ]
  },
  {
    "objectID": "submission-dryrun.html#whats-next",
    "href": "submission-dryrun.html#whats-next",
    "title": "16  Submission dryrun",
    "section": "16.10 What’s next",
    "text": "16.10 What’s next\nCongratulations! You’ve learned the complete workflow for Python-based clinical submissions. You’re now ready to apply these practices to real clinical trial projects. The Python clinical reporting ecosystem continues to evolve. Stay engaged with the community to learn best practices and contribute improvements. Here we list the important resources to explore:\n\n\n\n\n\n\nCaution\n\n\n\nIn case you used GitHub Codespaces, remember to stop (and maybe delete) your Codespace to avoid unnecessary usage.\n\n\nRegulatory guidance:\n\nFDA Study Data Technical Conformance Guide\neCTD specifications\n\nTechnical documentation:\n\nrtflite documentation\npkglite for Python documentation\nuv documentation\nPython Packaging User Guide\n\nExample repositories:\n\ndemo-py-esub: Analysis package\ndemo-py-ectd: Submission package",
    "crumbs": [
      "eCTD submission",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Submission dryrun</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Wickham, Hadley, and Jennifer Bryan. 2023. R Packages. O’Reilly\nMedia, Inc.",
    "crumbs": [
      "References"
    ]
  }
]